[["index.html", "Clinical Trials 4H - lecture notes 1 Introduction", " Clinical Trials 4H - lecture notes Rachel Oughton 2025-01-25 1 Introduction This is just here to preserve the numbering! "],["rct-plan.html", "2 (Lecture 2) Sample size 2.1 The treatment effect 2.2 Reminder: hypothesis tests (with a focus on RCTs) 2.3 Constructing a measure of effect size", " 2 (Lecture 2) Sample size For most of this course, our trial will have two arms and our unit of randomization will be individual participants. In this section we’ll focus on continuous primary outcome variables. The topics we’ll cover fall into two categories: Before the trial - design and planning After the trial - analysis and communication but there is some interaction between these phases. The first big question asked of a trial statistician is usually how many participants does the trial need in order to be viable? Broadly speaking, there are two (opposing) ethical issues around sample size: Not enough particpants may mean not enough evidence to come to a conclusion. This is both scientifically disappointing and unethical. Too many patients means subjecting more patients than necessary to an inferior treatment. This has been quite woolly so far, but now we’ll start to think more carefully. 2.1 The treatment effect We base our sample size calculations on the primary outcome variable. Definition 2.1 Suppose our primary outcome variable is \\(X\\), which has mean \\(\\mu\\) in the control group and mean \\(\\mu + \\tau\\) in the treatment group. The variable \\(\\tau\\) is the treatment effect. The goal of our RCT is to learn about \\(\\tau\\). The larger \\(\\tau\\) is (in magnitude), the more pronounced the effect of the intervention. This problem is usually framed as a hypothesis test, where the null hypothesis is that \\(\\tau=0\\). 2.2 Reminder: hypothesis tests (with a focus on RCTs) When performing a hypothesis test, what we are aiming to find is the P-value. Definition 2.2 The P-value is the probability of obtaining a result at least as extreme (ie. further away from the null hypothesis value) than the one obtained given that the null hypothesis is true. Definition 2.3 The threshold for the p-value below which the results are considered ‘significant’ is known as the significance level of the test, and is generally written \\(\\alpha\\). 2.2.1 One-sided or two-sided? The trial clinicians will have strong beliefs about the direction of the treatment effect. Assuming that a larger value of the primary outcome variable \\(X\\) is good, they will expect \\(\\tau&gt;0\\) (or be prepared to accept \\(\\tau=0\\), no effect). Therefore should we perform a one-sided test, with \\[\\begin{align*} H_0\\,&amp;:\\, \\tau=0\\\\ H_1\\,&amp;:\\, \\tau&gt;0? \\end{align*}\\] Figure 2.1: The distribution \\(t_{31}\\), with the area corresponding to \\(t &gt; 2\\) shaded. If \\(t\\gg{0}\\), we obtain a small P-value, and reject \\(H_0\\). Conclusion: the intervention is effective (in a good way). But what if we obtain \\(t\\ll{0}\\)? In this one-sided set-up, there is no value of \\(t&lt;0\\) that would give a significant result. For this reason, we always conduct two sided hypothesis tests, with \\[\\begin{align*} H_0\\,&amp;:\\, \\tau=0\\\\ H_1\\,&amp;:\\, \\tau\\neq 0. \\end{align*}\\] Figure 2.2: The distribution \\(t_{31}\\), with the area corresponding to \\(|t| &gt; 2\\) shaded. The P-value for the two-sided test as shown in Figure 2.2 is \\[ F\\left(-2, df=31\\right) + \\left[1 - F\\left(2, df=31\\right)\\right] = 2\\times{0.0272} = 0.0543\\] and the result is no longer significant at the 0.05 level. Throughout this course, we will always use two-tailed tests. 2.2.2 Insignificant results If our P-value is large, say 0.3 or 0.5, then our result is not at all unlikely under the null hypothesis, and provides no evidence to reject \\(H_0\\). However, it is not inconsistent with the existence of a treatment effect, so we don’t say there is evidence to accept \\(H_0\\). A non-significant P-value means our results are consistent with \\(H_0:\\; \\tau=0\\), and also with some small treatment effect. Key issue: what size of treatment effect do we care about? Our sample size should be big enough to be sufficiently likely to detect a clinically meaningful treatment effect. 2.3 Constructing a measure of effect size Let’s say we are recruiting participants into two groups: group \\(T\\) will be given the new treatment (we call them the treatment group or treatment arm) and group \\(C\\) will be given the control (they are the control group or control arm). Suppose we have \\(n\\) patients in group \\(C\\), and \\(m\\) in group \\(T\\), and \\[\\begin{align*} X &amp; \\sim N\\left(\\mu, \\sigma^2\\right) \\text{ in group }C\\\\ X &amp; \\sim N\\left(\\mu + \\tau, \\sigma^2\\right) \\text{ in group }T. \\end{align*}\\] We are testing the null hypothesis \\(H_0: \\tau=0\\) against the alternative hypothesis \\(H_1: \\tau\\neq{0}\\). Using the trial data we find sample means \\(\\bar{x}_C\\) and \\(\\bar{x}_T\\) from each group, and a pooled estimate of the standard deviation \\[ s = \\sqrt{\\frac{\\left(n-1\\right)s_C^2 + (m-1)s_T^2}{n+m - 2}}, \\] where \\(s_C\\) and \\(s_T\\) are the sample standard deviations for groups \\(C\\) and \\(T\\) respectively, eg \\[ s_C = \\sqrt{\\frac{\\sum\\limits_{i=1}^n{\\left(x_i - \\bar{x}_C\\right)^2}}{n-1}}. \\] Using these values we can compute \\[D = \\frac{\\bar{x}_T - \\bar{x}_C}{s\\sqrt{\\frac{1}{n} + \\frac{1}{m}}}\\] as a standardised measure of the effect \\(\\tau\\). Theorem 2.1 Under \\(H_0\\), \\(D\\) has a \\(t\\)-distribution with \\(n+m-2\\) degrees of freedom. Proof. Under \\(H_0\\) the \\(x_i\\) are iid \\(N\\left(\\mu,\\;\\sigma^2\\right)\\), and so \\[\\begin{align*} \\bar{x}_C &amp; \\sim{N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)}\\\\ \\bar{x}_T &amp; \\sim{N\\left(\\mu, \\frac{\\sigma^2}{m}\\right)} \\end{align*}\\] and therefore \\[ \\bar{x}_T - \\bar{x}_C \\sim{N \\left(0, \\sigma^2\\left[\\frac{1}{n} + \\frac{1}{m}\\right] \\right)}\\] and \\[ \\frac{\\bar{x}_T - \\bar{x}_C}{\\sigma \\sqrt{\\frac{1}{n}+\\frac{1}{m}}} \\sim{N\\left(0,1\\right)}.\\] We know that for \\(x_1,\\ldots,x_n,\\sim N\\left(\\mu,\\sigma^2\\right)\\) for some arbitrary \\(\\mu\\) and \\(\\sigma^2\\), \\[\\frac{1}{\\sigma^2}\\sum\\limits_{i=1}^n\\left(x_i - \\bar{x}\\right)^2 \\sim{\\chi^2_{n-1}},\\] and so we have \\[\\begin{align*} \\frac{n-1}{\\sigma^2}s_C^2 &amp; \\sim \\chi^2_{n-1}\\\\ \\frac{m-1}{\\sigma^2}s_T^2 &amp; \\sim \\chi^2_{m-1}\\\\ \\text{and} &amp;\\\\ \\frac{1}{\\sigma^2}\\left[\\left(n-1\\right)s_C^2 + \\left(m-1\\right)s_T^2\\right] &amp; = \\frac{n+m-2}{\\sigma^2}s^2\\\\ &amp;\\sim \\chi^2_{n+m-2}. \\end{align*}\\] The definition of a \\(t\\)-distribution is that if \\(Z\\sim N\\left(0,1\\right)\\) and \\(Y \\sim{\\chi^2_n}\\) then \\[X = \\frac{Z} {\\sqrt{\\frac{Y}{n}}} \\sim{t_n},\\] that is \\(X\\) has a \\(t\\) distribution with \\(n\\) degrees of freedom. Plugging in our \\(N\\left(0,1\\right)\\) variable for \\(Z\\) and our \\(\\chi^2_{n+m-2}\\) variable for \\(Y\\), we have \\[\\begin{align*} \\frac{\\frac{\\bar{x}_T - \\bar{x}_C}{\\sigma\\sqrt{\\frac{1}{n} + \\frac{1}{m}}}}{\\sqrt{\\left(\\frac{n+m-2}{\\sigma^2}s^2\\right) \\bigg/ \\left(n+m-2\\right)}} &amp; = \\frac{\\bar{x}_T - \\bar{x}_C}{\\sigma\\sqrt{\\frac{1}{n} + \\frac{1}{m}}} \\bigg/ \\frac{s}{\\sigma} \\\\ &amp; = \\frac{\\bar{x}_T - \\bar{x}_C}{s\\sqrt{\\frac{1}{n} + \\frac{1}{m}}} \\\\ &amp; = D \\end{align*}\\] and therefore \\(D\\) has a \\(t\\) distribution with \\(n+m-2\\) degrees of freedom. We can therefore use \\(D\\) as our test statistic; if \\(D\\) is such that \\[ |D| &gt; t_{n+m-2}\\left(\\alpha/2\\right)\\] where \\(t_{n+m-2}\\left(\\cdot\\right)\\) is the function such that \\(P\\left(T&gt;t_{df}\\left(\\xi\\right)\\right) = \\xi\\) when \\(T \\sim{t_{df}}\\) then we can reject \\(H_0\\). Generally we approximate this with a normal distribution (since \\(n\\) and \\(m\\) are usually sufficiently large). So, if we have run a trial, and have obtained \\(n\\) values of \\(X\\) from group \\(C\\) and \\(m\\) values of \\(X\\) from group \\(T\\), we can compute \\(D\\). If \\(D\\) lies outside the interval \\(\\left[-z_{\\alpha/2}, z_{\\alpha/2}\\right]\\) then we reject \\(H_0\\). This is equivalent to \\(\\bar{x}_T - \\bar{x}_C\\) falling outside the interval \\[\\left[-z_{\\alpha/2}s\\sqrt{\\frac{1}{n} + \\frac{1}{m}}, z_{\\alpha/2}s\\sqrt{\\frac{1}{n} + \\frac{1}{m}} \\right]. \\] Brief aside on notation We’ll see a lot of the notation \\(z_{\\alpha/2}\\) and similar, so to clarify: Our argument is based on \\(H_0\\) being true - but what if it isn’t? "],["lecture-3.html", "Lecture 3 2.4 Power: If \\(H_0\\) is false 2.5 A sample size formula", " Lecture 3 Recap: We constructed a measure \\(D = \\frac{\\bar{x}_T - \\bar{x}_C}{s\\sqrt{\\frac{1}{n} + \\frac{1}{m}}}\\) that we can use to test \\(H_0:\\; \\tau=0\\), since under \\(H_0\\), \\(D\\sim{t_{n+m-2}}\\) (and approximately \\(D\\sim N\\left(0,1\\right)\\)). 2.4 Power: If \\(H_0\\) is false So far, if \\(H_0\\) is true, we have a small probability of rejecting \\(H_0\\) (type I error rate). Flip side: if \\(H_0\\) is false, and \\(\\tau\\neq{0}\\), we want a high probability of rejecting \\(H_0\\). Definition 2.4 The power of a test is the probability that we reject \\(H_0\\), given that \\(H_0\\) is false. The power function depends on the value of \\(\\tau\\) and is \\[\\Psi\\left(\\tau\\right) = \\Pr\\left(\\text{Reject } H_0\\mid{\\tau\\neq{0}}\\right) = 1 - \\beta.\\] The quantity \\(\\beta\\) therefore represents \\(\\Pr\\left(\\text{Accept } H_0\\mid{\\tau\\neq{0}}\\right)\\), which is the type II error rate. If you find the notation confusing (as I do!) then it might be helpful to remember that both \\(\\alpha\\) and \\(\\beta\\) are - probabilities of coming to the wrong conclusion. It is common to talk in terms of \\(\\alpha\\), the significance level, (which will be a low number, often 0.05) and of \\(1-\\beta\\), the power (which will be a high number, often 0.8). I’ve found though that it is not uncommon to find people refer to \\(\\beta\\) (rather than \\(1-\\beta\\)) as the power. If in doubt, keep in mind that we require \\(\\alpha,\\;\\beta \\ll 0.5\\). It is also common to use percentages: a significance level of \\(\\alpha=0.05\\) can also be referred to as “the 95% level”, and \\(\\beta=0.2\\) is the same as a “power of 80%”. When using percentages, we talk in terms of the amount of time we expect the test to come to the correct conclusion. Under \\(H_1\\), we have (approximately) \\[D \\sim{N\\left(\\frac{\\tau}{\\sigma\\lambda\\left(n,m\\right)}, 1\\right)},\\] where \\(\\lambda\\left(n,m\\right) = \\sqrt{\\frac{1}{n}+\\frac{1}{m}}\\) and \\[D = \\frac{\\bar{x}_T - \\bar{x}_C}{s\\lambda\\left(n,m\\right)}.\\] Figure 2.3 shows the distribution of \\(D\\) under \\(H_0\\) and \\(H_1\\) for some arbitrary (non-zero) effect size \\(\\tau\\). The turquoise bar shows the acceptance region of \\(H_0\\), ie. the range of observed values of \\(D\\) for which we will fail to reject \\(H_0\\). We see that this contains 95% of the area of the \\(H_0\\) distribution (we have set \\(\\alpha = 0.05\\) here), so under \\(H_0\\), we have a 0.95 probability of observing a value of \\(D\\) that is consistent with \\(H_0\\). Figure 2.3: The distribution of \\(D\\) under both \\(H_0\\) and \\(H_1\\) for some arbitrary values of treatment effect, population variance, \\(n\\) and \\(m\\), with the region in which we fail to reject \\(H_0\\) shown by the turquoise bar and the red shading. However, if \\(H_1\\) is true, and \\(\\tau\\neq{0}\\), there is a non-zero probability of observing a value of \\(D\\) that would lead us to fail to reject \\(H_0\\). This is shown by the area shaded in red, and it has area \\(\\beta\\). One minus this area (ie. the area under \\(H_1\\) that leads us to accept \\(H_1\\)) is the power, \\(1-\\beta\\). [FIGURE!!] Figure 2.4: The distribution of D under both \\(H_0\\) and \\(H_1\\) for some arbitrary values of effect size, population variance, \\(n\\) and \\(m\\), with the region in which we fail to reject \\(H_0\\) shown by the turquoise bar and the red shading. For given values of \\(\\alpha\\), \\(\\sigma\\) and \\(\\lambda\\left(n,m\\right)\\), we can calculate the power function in terms of \\(\\tau\\) by finding the area of the distribution of \\(D\\) under \\(H_1\\) for which we accept \\(H_1\\). \\[\\begin{equation} \\Psi\\left(\\tau\\right) = 1-\\beta = \\left[1 - \\operatorname{\\Phi}\\left(z_{\\frac{\\alpha}{2}} - \\frac{\\tau}{\\sigma\\lambda}\\right)\\right] + \\operatorname{\\Phi}\\left(-z_{\\frac{\\alpha}{2}} - \\frac{\\tau}{\\sigma\\lambda}\\right) \\tag{2.1} \\end{equation}\\] The first term in Equation (2.1) is the area in the direction of \\(\\tau\\). In Figures 2.3 and 2.4 this is the region to the right of the interval for which we fail to reject \\(H_0\\), ie. where \\[D &gt; z_{\\frac{\\alpha}{2}}.\\] The second term in Equation (2.1) represents the area away from the direction of \\(\\tau\\), ie. a value of \\(D\\) such that \\[ D &lt; - z_{\\frac{\\alpha}{2}},\\] assuming without loss of generality that \\(\\tau&gt;0\\). Larger sample size \\(\\rightarrow\\) greater power Equal groups \\(\\rightarrow\\) greater power In general, the probability of rejecting \\(H_0\\) increases as \\(\\tau\\) moves away from zero. Notice also that all the curves pass through the point \\(\\tau=0,\\,\\beta=0.05\\). Since \\(\\tau=0\\) corresponds to \\(H_0\\) being true, it makes sense that the probability of rejecting the \\(H_0\\) is the significance level \\(\\alpha\\). Figure 2.5: Power curves for various values of \\(n\\) and \\(m\\), with effect size in units of standard deviation, given a type I error rate of 0.05. It is common to think of the effect size in units of \\(\\sigma\\), as we have done here. 2.5 A sample size formula Equation (2.1) allows us to find any one of \\(\\tau_\\sigma,\\,\\alpha,\\,\\beta\\) and \\(\\lambda\\left(n,m\\right)\\) given values for the others. \\[\\begin{equation} \\Psi\\left(\\tau\\right) = 1-\\beta = \\left[1 - \\operatorname{\\Phi}\\left(z_{\\frac{\\alpha}{2}} - \\frac{\\tau}{\\sigma\\lambda}\\right)\\right] + \\operatorname{\\Phi}\\left(-z_{\\frac{\\alpha}{2}} - \\frac{\\tau}{\\sigma\\lambda}\\right) \\end{equation}\\] Values for \\(\\alpha\\) and \\(\\beta\\) are often specified by those planning the trial as around \\(\\alpha \\in \\left[0.01,0.05\\right],\\,1-\\beta\\in\\left[0.8,0.9\\right]\\). The remaining two variables, \\(\\tau_\\sigma\\) and \\(\\lambda\\left(n,m\\right)\\) are generally settled using one or both of the following questions: Given our budget constraints, and their implications for \\(n\\) and \\(m\\), what is the smallest value of \\(\\tau_\\sigma\\) we can achieve? What is the smallest value of \\(\\tau_\\sigma\\) that would be clinically useful to detect, and what value of \\(\\lambda\\left(n,m\\right)\\) do we need in order to achieve it? In a medical setting, an estimate of \\(\\sigma\\) is usually available, and so we will return to thinking in terms of \\(\\tau\\) and \\(\\sigma\\). In this equation, the value we use (or find) for \\(\\tau\\) is the minimum detectable effect size, which we will denote \\(\\tau_M\\). Definition 2.5 The minimum detectable effect size \\(\\tau_M\\) for a particular trial is the smallest value of effect size that is able to be detected with power \\(1-\\beta\\) and at significance level \\(\\alpha\\) (for some specified values of \\(\\alpha,\\;\\beta\\)). Therefore, Equation (2.1) becomes \\[\\begin{equation} \\Psi\\left(\\tau\\right) = 1-\\beta = \\left[1 - \\operatorname{\\Phi}\\left(z_{\\frac{\\alpha}{2}} - \\frac{\\tau_M}{\\sigma\\lambda}\\right)\\right]. \\tag{2.2} \\end{equation}\\] Because \\(\\operatorname{\\Phi}\\left(z_\\beta\\right) = 1 - \\beta\\) (by definition) and \\(\\operatorname{\\Phi}\\left(-z\\right) = 1 - \\operatorname{\\Phi}\\left(z\\right)\\) we can write this as \\[ \\operatorname{\\Phi}\\left(z_\\beta\\right) = \\operatorname{\\Phi}\\left(\\frac{\\tau_M}{\\sigma\\lambda} - z_{\\frac{\\alpha}{2}}\\right), \\] where \\(\\tau_M\\) is our minimum detectable effect size. Because of the monotonicity of \\(\\operatorname{\\Phi}\\left(\\cdot\\right)\\), we can write \\[\\begin{equation} \\begin{aligned} z_\\beta &amp; = \\frac{\\tau_M}{\\sigma\\lambda} - z_{\\frac{\\alpha}{2}} \\\\ z_\\beta + z_{\\frac{\\alpha}{2}} &amp; = \\frac{\\tau_M}{\\sigma\\lambda}. \\tag{2.3} \\end{aligned} \\end{equation}\\] Because we want to think about sample sizes, we rewrite this further. It is most common to perform trials with \\(n=m=N\\) participants in each group, in which case \\[ \\lambda\\left(n,m\\right) = \\sqrt{\\frac{2}{N}}\\] and Equation (2.3) rearranges to \\[\\begin{equation} N = \\frac{2\\sigma^2\\left(z_\\beta + z_{\\frac{\\alpha}{2}}\\right)^2}{\\tau_M^2}. \\tag{2.4} \\end{equation}\\] Example 2.1 (from Zhong 2009) A trial is being planned to test whether there is a difference in the efficacy of ACEII antagonist (a new drug) and ACE inhibitor (the standard drug) for the treatment of primary hypertension (high blood pressure). The primary outcome variable is change in sitting diastolic blood pressure (SDBP, mmHg) compared to a baseline measurement taken at the start of the trial. The trial should have a significance level of \\(\\alpha=0.05\\) and a power of \\(1-\\beta = 0.8\\), with the same number of participants in each group. The minimum clinically important difference is \\(\\tau_M = 3 \\text{ mmHg}\\) and the pooled standard deviation is \\(s = 8 \\text{ mmHg}\\). Therefore, using equation (2.4) the sample size should be at least \\[\\begin{align*} N &amp; = \\frac{2\\times{8}^2\\left(0.842 + 1.96\\right)^2}{3^2}\\\\ &amp; = 111.6, \\end{align*}\\] and therefore we need at least 112 participants in each trial arm. References Zhong, Baoliang. 2009. “How to Calculate Sample Size in Randomized Controlled Trial?” Journal of Thoracic Disease 1 (1): 51. "],["lecture-4-allocation.html", "3 (Lecture 4) Allocation 3.1 Bias 3.2 (Lecture 5) Allocation methods 3.3 (Lecture 6) Incorporating baseline measurements 3.4 Problems with allocation", " 3 (Lecture 4) Allocation [Finished with sample size for now - check out JAMAevidence: JAMA Guide to Statistics and Methods Interviews about the statistical and methodological foundations of clinical research. Esp sample size, linked from Ultra] Once we’ve decided how many participants we need in our trial, we need to determine how they will each be assigned to a trial arm. This is process is known as allocation (or sometimes as randomization). Before we think about methods for allocation, we are going to spend some time talking about bias. 3.1 Bias In statistics, bias is a systematic tendency for the results of our analysis to be different from the true value, eg. when using sample data to estimate a parameter. We will revisit what we have learned in previous courses about bias before going on to see how it affects RCTs. Definition 3.1 (Bias of an estimate) Suppose \\(T\\) is a statistic calculated to estimate a parameter \\(\\theta\\). The bias of \\(T\\) is \\[E\\left(T\\right) - \\theta.\\] If the bias of \\(T\\) is zero, we say that \\(T\\) is an unbiased estimator of \\(\\theta\\). Recall the standard deviation. If we have some data \\(x_1,\\,\\ldots,x_n\\) that are IID \\(N\\left(\\mu,\\,\\sigma^2\\right)\\), we can find the sample variance \\[ s^2 = \\frac{1}{n}\\sum\\limits_{i=1}^n\\left(x_i - \\bar{x}\\right)^2 .\\] Now, \\(E\\left(s^2\\right) \\neq {\\sigma^2}\\) (you’ve probably seen this proved so we’re not going to prove it now), and \\(s^2\\) is a biased estimator of \\(\\sigma^2\\). However, we know that \\[E \\left(\\frac{n}{n-1}s^2\\right) = \\sigma^2,\\] We can apply this correction to produce an unbiased estimate of \\(\\sigma^2\\). Now, suppose our sample \\(x_1,\\ldots,x_n\\) were drawn from \\(N\\left(\\mu,\\sigma^2\\right)\\), but were not independent of one another. Then, * neither our estimator \\(s^2\\), nor our bias-corrected estimator \\(\\frac{n}{n-1}s^2\\) would have expected value \\(\\sigma^2\\) * we cannot use our sample \\(x_1,\\ldots,x_n\\) to produce an unbiased estimator of \\(\\sigma^2\\), or even of the mean \\(\\mu\\). This is much closer to what we mean when we talk about bias in a clinical trial setting. Suppose we are testing some new treatment \\(T\\) against the standard \\(C\\). We measure some outcome \\(X\\) for each patient, and our hypothesis is that \\(X\\) behaves differently for those in the treatment group than for those in the control group. It is common practice to express this additively, \\[E\\left(X\\right) = \\mu + \\tau,\\] where \\(\\tau\\) is our treatment effect, which we can estimate using the difference in the groups’ means, \\(\\bar{X}_T - \\bar{X}_C\\). Our null hypothesis is that \\(\\tau = 0\\), and our alternative hypothesis is that \\(\\tau\\neq{0}\\), and therefore an estimate of \\(\\tau\\) from our data is very important! Clinical trials are all about estimating the treatment effect \\(\\tau\\), so it is important that there is no bias in our estimates of \\(\\bar{X}_C\\) and \\(\\bar{X}_T\\). Usually, what this comes down to is that the assumption that the data are independent, identically distributed random variables from the relevant distributions (which we have already relied on a lot for our sample size calculations) has been violated in some way. Example 3.1 Historically, women and the elderly are underrepresented in clinical trials (Cottingham and Fisher (2022)) and results are often translated from young or middle aged healthy men to these other groups (Vitale et al. (2017)). This isn’t reasonable, since women have very different hormonal activity from men, causing them to often react differently to drugs compared to men involved in the trial. The standard dose (based on trials with mostly male participants) can also be too high for many women. The complicated nature of women’s hormones is sometimes even given as a reason for not including them in the trial. Women and elderly people are also both more likely to have adverse effects to drugs in some fields. There are also ethical reasons behind the low numbers of women in trials, especially phase I and phase II trials. If a woman is possibly pregnant (and trials tend to be extremely cautious in deciding who might be pregnant!) then they are quite often excluded, in order to protect the (actual or hypothetical) fetus. Indeed, in 1977 the Food and Drug Administration (FDA) in the US recommended that women be excluded from phase I and II trials (Health (2023)) as a result of some severe cases of fetuses being harmed by drugs (especially Thalidamide) . This means that even some very mainstream drugs, for example antihistamines (Kar et al. (2012)), haven’t been tested for safety/efficacy during pregnancy, as well as some (for example HIV treatments) that would be of huge benefit to many many pregnant women. This article is an interesting read if you would like to know more. 3.1.1 Sources of bias Bias is very serious - where does it come from? Most sources of bias creep in during the selection or allocation. Selection bias Certain patients are systematically more (or less) likely be entered into the trial because of the treatment they will receive. In a properly run trial this isn’t possible, because it is only after a participant has been recruited that their treatment is chosen. If a medical professional is not comfortable with a particular patient potentially receiving one of the possible treatments, then that patient should not be entered into the trial at all. If there are many such [technically eligible] patients, then this might cause the estimated treatment effect to be worryingly far from the true population treatment effect, since the recruited group of participants would not be very representative of the true population (this is not technically selection bias, but it comes from the same problem). The doctor might know which treatment a patient would be given, eg if the allocation follows some deterministic pattern, or is fully known to the doctor in advance. Consciously or subconsciously this knowledge may influence the description they give to potential participants, and this in turn may affect which patients sign up, and the balance of the groups. In practice there should be various safeguards against this situation. Example 3.2 Suppose we run a trial comparing a surgical (S) and a non-surgical (N) treatment for some condition. Patients who are eligible are given the opportunity to join the trial by a single doctor. For each patient, disease severity is graded as 1 (less serious) or 2 (more serious). Across the full group, proportion \\(\\lambda\\) have severity 1 and proportion \\(1-\\lambda\\) have severity 2. Our primary outcome is survival time, \\(X\\), which depends on the severity of disease: \\[\\begin{align*} E\\left(X\\mid{1}\\right) &amp; = \\mu_1\\\\ E\\left(X\\mid{2}\\right) &amp; = \\mu_2 \\end{align*}\\] and we assume \\(\\mu_1&gt;\\mu_2\\). For untreated patients we have \\[ E\\left(X\\right) = \\mu = \\lambda \\mu_1 + \\left(1-\\lambda\\right)\\mu_2.\\] Suppose that for treatment group \\(N\\), the expected survival time increase by \\(\\tau_N\\), and similarly for group \\(S\\), so that we have \\[\\begin{align*} E\\left(X\\mid{N,1}\\right) &amp; = \\mu_1 + \\tau_N\\\\ E\\left(X\\mid{N,2}\\right) &amp; = \\mu_2 + \\tau_N\\\\ E\\left(X\\mid{S,1}\\right) &amp; = \\mu_1 + \\tau_S\\\\ E\\left(X\\mid{S,2}\\right) &amp; = \\mu_2 + \\tau_S. \\end{align*}\\] If all patients were admitted with equal probability to the trial (ie. independent of the severity of their disease) then the expected survival time for group \\(N\\) would be \\[\\begin{align*} E\\left(X\\mid{N}\\right) = E\\left(X\\mid{1,N}\\right)P\\left(1\\mid{N}\\right) + E\\left(X\\mid{2,N}\\right)P\\left(2\\mid{N}\\right)&amp; = \\left(\\mu_1 + \\tau_N\\right)\\lambda + \\left(\\mu_2+\\tau_N\\right)\\left(1-\\lambda\\right)\\\\ &amp; = \\mu + \\tau_N. \\end{align*}\\] Similarly, \\(E\\left(X\\mid{S}\\right) = \\mu + \\tau_S\\) and \\(\\tau = \\tau_N - \\tau_S\\) and the trial is unbiased. Suppose that although all eligible patients are willing to enter the trial, the doctor is reticent to subject patients with more severe disease (severity 2) to the surgical procedure. This is reflected in the way they explain the trial to each patient, particularly those with severity 2 whom the doctor knows will be assigned to group \\(S\\). Suppose a reduced proportion \\(q = 1-p\\) of those with severity 2 assigned to surgery enter the trial (event \\(A\\)): \\[\\begin{align*} P\\left(A\\mid{N,1}\\right) = P\\left(A\\mid{S,1}\\right) = P\\left(A\\mid{N,2}\\right) &amp; = 1 \\\\ P\\left(A\\mid{S,2}\\right) &amp; = 1-p = q. \\end{align*}\\] Since our analysis is based only on those who enter the trial, our estimated treatment effect will be \\[E\\left(X\\mid{A, N}\\right) - E\\left(X\\mid{A, S}\\right). \\] We can split these according to disease severity, so that \\[E\\left(X\\mid{A,N}\\right) = E\\left(X\\mid{A,N,1}\\right)P\\left(1\\mid{A,N}\\right) + E\\left(X\\mid{A,N,2}\\right)P\\left(2\\mid{A,N}\\right) \\] and similarly for group \\(S\\). We can calculate \\(P\\left(1\\mid{A,N}\\right)\\) using Bayes’ theorem, \\[\\begin{align*} P\\left(1\\mid{A,N}\\right) &amp; = \\frac{P\\left(A\\mid{1,N}\\right)P\\left(1\\mid{N}\\right)}{P\\left(A\\mid{N}\\right)}\\\\ &amp; = \\frac{P\\left(A\\mid{1,N}\\right)P\\left(1\\mid{N}\\right)}{P\\left(A\\mid{N,1}\\right)P\\left(1\\mid{N}\\right) + P\\left(A\\mid{N,2}\\right)P\\left(2\\mid{N}\\right)} \\\\ &amp;= \\frac{1\\times{\\lambda}}{1\\times {\\lambda} + 1 \\times{\\left(1-\\lambda\\right)}}\\\\ &amp; = \\lambda. \\end{align*}\\] Therefore we also have \\(P\\left(2\\mid{A,N}\\right) = 1 -P\\left(1\\mid{A,N}\\right) = 1-\\lambda\\). Following the same process for group \\(S\\), we arrive at \\[\\begin{align*} P\\left(1\\mid{A,S}\\right) &amp; = \\frac{P\\left(A\\mid{1,S}\\right)P\\left(1\\mid{S}\\right)}{P\\left(A\\mid{S}\\right)}\\\\ &amp; = \\frac{P\\left(A\\mid{1,S}\\right)P\\left(1\\mid{S}\\right)}{P\\left(A\\mid{S,1}\\right)P\\left(1\\mid{S}\\right) + P\\left(A\\mid{S,2}\\right)P\\left(2\\mid{S}\\right)} \\\\ &amp; = \\frac{\\lambda}{\\lambda + q\\left(1-\\lambda\\right)}, \\end{align*}\\] which we will call \\(b\\). Notice that \\(P\\left(2\\mid{S}\\right)= 1-\\lambda\\), since it is not conditional on actually participating in the trial. Therefore, \\[\\begin{align*} E\\left(X\\mid{A,N}\\right) &amp; = E \\left(X\\mid{N,1}\\right)P\\left(1\\mid{A,N}\\right) + E \\left(X\\mid{N,2}\\right)P\\left(2\\mid{A,N}\\right) \\\\ &amp; = \\left(\\mu_1 + \\tau_N\\right)\\lambda + \\left(\\mu_2 + \\tau_N\\right)\\left(1-\\lambda\\right) \\\\ &amp; = \\lambda\\mu_1 + \\left(1-\\lambda\\right)\\mu_2 + \\tau_N \\end{align*}\\] and \\[\\begin{align*} E\\left(X\\mid{A,S}\\right) &amp; = E \\left(X\\mid{S,1}\\right)P\\left(1\\mid{A,S}\\right) + E \\left(X\\mid{S,2}\\right)P\\left(2\\mid{A,S}\\right) \\\\ &amp; = \\left(\\mu_1 + \\tau_S\\right)b + \\left(\\mu_2 + \\tau_S\\right)\\left(1-b\\right) \\\\ &amp; = b\\mu_1 + \\left(1-b\\right)\\mu_2 + \\tau_S. \\end{align*}\\] From here, we can calculate the expected value of the treatment effect \\(\\tau\\) as (substituting our equation for \\(b\\) and rearranging): \\[\\begin{align*} E\\left(X\\mid{A,N}\\right) - E\\left(X\\mid{A,S}\\right) &amp; = \\tau_N - \\tau_S + \\left(\\lambda - b\\right)\\left(\\mu_1 - \\mu_2\\right) \\\\ &amp; = \\tau_N - \\tau_S - \\frac{p\\lambda\\left(1-\\lambda\\right)\\left(\\mu_1 - \\mu_2\\right)}{\\lambda + q\\left(1-\\lambda\\right)}, \\end{align*}\\] where the third term represents the bias. Notice that if \\(q=1-p = 1\\), then there is no bias. There is also no bias if \\(\\mu_1 = \\mu_2\\), ie. if there is no difference between the disease severity groups in terms of survival time. Assuming \\(\\mu_1 - \\mu_2 &gt;0\\), then the bias term is positive and \\[E\\left(X\\mid{A,N}\\right)- E\\left(X\\mid{A,S}\\right) &lt; \\tau_N - \\tau_S.\\] If \\(N\\) is the better treatment, then \\(\\tau_N - \\tau_S&gt;0\\) and the bias will cause the trial to underplay the treatment effect. Conversely, if \\(S\\) is better, then \\(\\tau_N-\\tau_S&lt;0\\) and the trial will exaggerate the treatment effect. This is because more severely ill patients have been assigned to \\(N\\) than to \\(S\\), which reduces the average survival time for those in group \\(N\\). Allocation bias Mathematically similar to selection bias, but instead of coming from human ‘error’, it arises from the random process of allocation. Suppose a trial investigates a drug that is likely to have a much stronger effect on male patients than on female patients. The cohort of recruited participants are randomised into treatment and control groups, and it happens that there is a much smaller proportion of female patients in the treatment group than in the control group. This will distort the estimated treatment effect. We will investigate various strategies for randomization designed to address this issue for known factors. Assessment bias Measurements are made on participants throughout and during the trial. Often objective: eg. weight, or concentration of blood sugar. Some measurments are subject to the individual practitioner assessing the patient. Eg, many skin conditions are assessed visually, for example estimating the proportion of the body affected. Measuring quantities such as quality of life or psychological well-being involve many subjective judgements on the part of both patient and clinician. Blood pressure used to rely on pracitioner’s hearing and judgement. Clearly it is ideal for both the patient and the clinician not to know which arm of the trial the patient was part of (this is known as a double blind trial). For treatments involving drugs, this is usually straightforward. However, for surgical interventions it is often impossible to keep a trial ‘blind’, and for interventions involving therapy (for example cognitive behavioural therapy) it is impossible for the patient to be unaware. Slight aside: publication bias In most areas of science, including clinical trials, the ultimate aim is to affect practice. This is usually done by publishing a write-up of the trial, including its design, methods, analysis and results, and publishing that in a [medical] journal. These are peer-reviewed, which means that experts from the relevant field are ask to review submitted papers, and either reject or accept them (usually conditional on some revision). These reviewers advise the editor of the journal, who ultimately decides whether or not the paper will be publised. It seems that papers reporting positive / conclusive results are more likely to be published than papers about [viable] trials that ultimately fail to reject the null hypothesis. As we know, in most cases if the null hypothesis is rejected this is indicative that there is a true treatment difference. However, sometimes by random chance a trial will detect a difference even when there isn’t one (approximately 5% of the time if \\(\\alpha=0.05\\)). If these papers are disproportionately likely to be published, the body of literature will not reflect the truth, and there may be serious implications for impact on practice. Measures are being taken to prevent this: for example, leading medical journal The Lancet insists that any clinical trial related paper is registered with them before the first participant has been recruited, with details of the design and statistical analysis plan. This is then reviewed before the trial begins. 3.1.2 Implications for allocation Clinical trials haven’t always used random allocation to assign participants to groups. Some popular alternatives: Compare groups in serial, so that \\(N_A\\) patients one year (say) form the control group, and \\(N_B\\) patients in a subsequent year, who are given treatment \\(B\\), form the intervention group. In this scenario it is impossible to control for all other changes that have occurred with time, and this leads to a systematic bias, usually in favour of treatment \\(B\\). Given the need for contemporary control participants, the question becomes how to assign participants to each group. If the clinician is able to choose who receives which treatment, or if each patient is allowed to choose or refuse certain treatments, this is almost certain to introduce bias. This is avoided by using random allocation. [ASSIGNMENT! Explain details a bit, deadline is Monday 5th Feb. ] 3.2 (Lecture 5) Allocation methods Two important aspects to the allocation being random: Every patient should have the same probability of being assigned to each treatment group. The treatment group for a particular patient should not be able to be predicted. Point 1 is important because, as we have already mentioned, the statistical theory we use to plan and analyse the trial is based on the groups being random samples from the population. Point 2 is important to avoid biases that come through the assignment of a particular patient being known either in advance or after the fact. There are some approaches that ‘pass’ the first point, but fail at the second. Eg. strict alternation (\\(ABABAB\\ldots\\)), using patient characteristics such as date of birth or first letter of surname, which is not related to the trial outcome, but which enables allocations to be predicted. We will now explore some commonly used methods of allocation. We will usually assume two equally sized groups, \\(A\\) and \\(B\\), but it is simple to generalize to three or more groups, or to unequal allocation. 3.2.1 Simple random allocation Simplest method: ‘toin coss’, where each participant has a probability 0.5 of being placed in each group. As participants arrive, assignment \\(C\\) or \\(T\\) is generated (with equal probability). Statistically ideal: generates the random sample we need All participants are allocated independently - not predictable No ‘master’ randomisation Statistically, this scheme is ideal, since it generates the random sample we need, and the assignment of each participant is statistically independent of that of all other participants. It also doesn’t require a ‘master’ randomisation; several clinicians can individually assign participants to treatment groups in parallel and the statistical properties are maintained. SRS is used effectively in many large trials, but for small trials it can be statistically problematic. The main reason for this is chance imbalance of group sizes. Suppose we have two groups, \\(T\\) of size \\(N_T\\) and \\(C\\) of size \\(N_C\\), with \\(N_T + N_C = 2n\\). Patients are allocated independently with equal probability, which means \\[N_C \\sim \\operatorname{Bi}\\left(2n,\\frac{1}{2}\\right), \\] and similar for \\(N_T\\). If the two groups are of unequal size, the larger will be of some size \\(N_{max}\\) between \\(n\\) and \\(2n\\), such that for \\(r = n+1,\\,\\ldots,\\,2n,\\) \\[\\begin{align*} P\\left(N_{max} = r\\right) &amp; = P\\left(N_C = r\\right) + P\\left(N_T = r\\right) \\\\ &amp; = 2\\binom{2n}{r}\\left(\\frac{1}{2}\\right)^{2n}. \\end{align*}\\] The probability that \\(N_C = N_T = n\\) is \\[ P\\left(N_T = N_C = n\\right)= \\binom{2n}{n}\\left(\\frac{1}{2}\\right)^{2n}. \\] These probabilities are shown in Figure 3.1. We can see that this method leads to very unequal groups relatively easily; with \\(n=15\\), \\(P\\left(N_{max}\\geq 20\\right) = 0.099\\), so there is around a one in ten chance that one group will be double or more the size of the other. Figure 3.1: The probability distribution of largest group size for n=15. For larger trials, this imbalance will be less pronounced, for example Figure 3.2 shows the same for \\(n=300\\). Figure 3.2: The probability distribution of largest group size for n=300. In this case the \\(P\\left(N_{max} \\geq 400\\right)\\approx 10^{-16}\\), so the chance of highly imbalanced groups (2:1 or worse) is much lower. However, we may want to achieve balance on some factor thought to be important, for example sex, age group or disease state, and in this case there may be small numbers even in a large trial. We saw in the sample size section that the greatest power is achieved when group sizes are equal, since this minimises the function \\[\\lambda\\left(n,m\\right) = \\sqrt{\\frac{1}{n}+\\frac{1}{m}}.\\] However, with simple random sampling we can’t guarantee equal group sizes. Example 3.3 Suppose we are designing a trial to have \\(\\alpha=0.05\\), and our minimum detectable effect size is such that \\(\\frac{\\tau_M}{\\sigma}=1\\). If 30 participants are recruited, then \\[1-\\beta = \\Phi\\left(\\sqrt{\\frac{n_T\\,n_C}{30}} - 1.96\\right). \\] The first term in the standard normal CDF comes from the fact that \\[\\left[\\lambda\\left(n,m\\right)\\right]^{-1} = \\sqrt{\\frac{nm}{n+m}} .\\] If we have equal group sizes \\(n_T=n_C=15\\), then the power achieved is 78%. If the group sizes are 10 and 20, we have a power of 73%. If the group sizes are 6 and 24, the power goes down to 59%. So, as we saw when looking at power, we don’t lose too much if the group sizes are 2:1, but a more pronounced imbalace has resulted in a much more noticeable loss. There may be other disadvantages to having such imbalance, for example increased costs, or a reduction in the amount of information gained about side effects. If this imbalance can be avoided, it should be. In this chapter, and in the computer practical on allocation, we will study the behaviour of various allocation methods by implementing them many times. This gives us an idea of how variable the results are, especially in terms of being likely to introduce bias or reduce power. In almost all real trials, patients are allocated and begin treatment as they arrive, and this may happen over the course of weeks or months. The allocation that is generated is the final allocation. 3.2.2 Random permuted blocks Very commonly used method to randomly allocate participants while avoiding too much imbalance is to use random permuted blocks (RPBs). If the blocks have size \\(2m\\), and there are two groups then there are \\[\\binom{2m}{m},\\] but this method can be adapted to more than two groups and to unequal group size. If we have two groups, \\(A\\) and \\(B\\), then there are six blocks of length 4 containing two \\(A\\)s and two \\(B\\)s \\[ \\begin{aligned} 1.&amp; AABB\\\\ 2.&amp; ABAB\\\\ 3.&amp; ABBA\\\\ 4.&amp; BAAB\\\\ 5.&amp; BABA\\\\ 6.&amp; BBAA. \\end{aligned} \\] Randomly generate a sequence from \\(\\left\\lbrace 1, 2, 3, 4, 5, 6 \\right\\rbrace\\), with equal probability. This sequence will correspond to a sequence in \\(A\\) and \\(B\\) with four times the length. For example, suppose the sequence begins \\(2,1,3,6,\\ldots\\). Replacing each number by its block, we have \\(ABAB\\;AABB\\;ABBA\\;BBAA\\;\\ldots\\). Advantages: Each patient is equally likely to receive \\(A\\) and \\(B\\) Difference between \\(N_A\\) and \\(N_B\\) never more than \\(m\\) (two in this case) Easy to implement Disadvantages: Some allocations can be predicted (last in every block, sometimes more) If the block size is fixed, and the doctors involved in the trial know which participants have received which treatments (which is unavoidable in cases such as surgery), then the allocation for some patients can be perfectly predicted. This is true for the fourth in every block, and for the third and fourth if the first two were the same. This means that selection bias may be a problem in more than 25% of participants, which is deemed unacceptable; indeed, it fails our second point about randomization. 3.2.2.1 RPBs with random block length Remove predictability (almost) by randomly varying the length of the block. Eg. there are \\[ \\binom{6}{3} = 20\\] possible 6-blocks. Instead of always using 4-blocks, we can do the following. A random number \\(X\\) is drawn from \\(\\left\\lbrace 4,6\\right\\rbrace\\) to select the block length. A second random number \\(Y\\) is drawn from 1 to 6 (\\(X=4\\)) or 1 to 20 (\\(X=6\\)). The \\(Y^{th}\\) block is chosen and participants assigned accordingly. If more participants are needed, go back to step 1. This method Ensures patients equally likely to be in \\(A\\) or \\(B\\) group sizes never differ by more than 3 reduces selection bias (only predictably if difference is 3) 3.2.3 Biased coin designs and urn schemes What if we want to retain the pure stochasticity of SRS, but with more balance? It may be that we prefer a method which achieves balance while retaining the pure stochasticity of simple random sampling. An advantage of RPBs was that once the sequence was generated, no computing power was needed. However, it is safe now to assume that any hospital pharmacy, nurse’s station, GP office or other medical facility will have a computer with access to the internet (or some internal database), and therefore more sophisticated methods are available. It is also very likely that all trial data may be stored on some central database, and so methods that rely on knowing the allocation so far (albeit in some encrypted form) should be possible even if there are multiple clinicians and sites involved. Biased coin designs and urn schemes both work by adjusting the probabilities of allocation according to balance of the design so far. such that a participant is less likely to be assigned to an over-represented group. 3.2.3.1 Biased coin designs Suppose we have groups \\(T\\) and \\(C\\), and have so far allocated \\(n\\) participants. We write \\(N_T\\left(n\\right)\\) for the number of participants allocated to treatment \\(T\\), and \\(N_C\\left(n\\right)\\) for the number of participants allocated to treatment \\(C\\). We denote the imbalance by \\[ D\\left(n\\right) = N_T\\left(n\\right) - N_C\\left(n\\right) = 2N_T\\left(n\\right) - n.\\] We use \\(D\\left(n\\right)\\) to alter the probability of allocation to each treatment in order to restore (or maintain) balance in the following way: If \\(D\\left(n\\right)=0\\), allocate patient \\(n+1\\) to treatment \\(T\\) with probability \\(\\frac{1}{2}\\). If \\(D\\left(n\\right)&lt;0\\), allocate patient \\(n+1\\) to treatment \\(T\\) with probability \\(P\\). If \\(D\\left(n\\right)&gt;0\\), allocate patient \\(n+1\\) to treatment \\(T\\) with probability \\(1-P\\). where \\(P\\in\\left(\\frac{1}{2}, 1\\right)\\). If, at some point in the trial, we have \\(\\lvert D\\left(n\\right)\\rvert = j\\), for some \\(j&gt;0\\), then we must have either \\[ \\lvert D\\left(n+1\\right)\\rvert = j+1 \\] or \\[ \\lvert D\\left(n+1\\right)\\rvert = j-1 .\\] Because of the way we have set up the scheme, \\[ p\\big(\\lvert D\\left(n+1\\right)\\rvert = j+1\\big) = 1-P \\] and \\[ p\\big(\\lvert D\\left(n+1\\right)\\rvert = j-1\\big) = P.\\] If \\(\\lvert D\\left(n\\right) \\rvert = 0\\), ie. the scheme is in exact balance after \\(n\\) allocations, then we must have \\(\\lvert D\\left(n\\right)\\rvert = 1\\). The absolute imbalances therefore form a simple random walk on the non-negative integers, with transition probabilities \\[ \\begin{aligned} P\\bigg(\\lvert D\\left(n+1\\right) \\rvert = 1 \\; \\bigg| \\;\\lvert D\\left(n\\right)\\rvert=0\\bigg)&amp; = 1\\\\ P\\bigg(\\lvert D\\left(n+1\\right) \\rvert = j+ 1 \\; \\bigg| \\; \\lvert D\\left(n\\right)\\rvert=j\\bigg)&amp; = 1 - P\\\\ P\\bigg(\\lvert D\\left(n+1\\right) \\rvert = j-1 \\; \\bigg| \\; \\lvert D\\left(n\\right)\\rvert=j\\bigg)&amp; = P \\end{aligned} \\] Figure 3.3 shows four realisations of this random walk with \\(P=0.667\\). We see that sometimes the imbalance gets quite high, but in general it isn’t too far from 0. Figure 3.3: Absolute imbalance for a biased-coin scheme with \\(P=0.667\\). Figure 3.4 shows four realisations of the random walk with \\(P=0.55\\). Here, the imbalance is able to get very high (note the change in \\(y\\)-axis); for example in the first plot, if we stopped the trial at \\(n=50\\) we would have 34 participants in one arm and only 16 in the other. Figure 3.4: Absolute imbalance for a biased-coin scheme with \\(P = 0.55\\). By contrast, with \\(P=0.9\\) as in Figure 3.5, there is much less imbalance. However, this brings with it greater predictability. Although allocation is always random, given some degree of imbalance (likely to be known about by those executing the trial), the probability of guessing the next allocation correctly is high (0.9). This invites the biases we have been trying to avoid, albeit in an imperfect form. Figure 3.5: Absolute imbalance for a biased-coin scheme with \\(P = 0.9\\). In summary \\(P\\) closer to 0.5 \\(/rightarrow\\) less predictable, more imbalance \\(P\\) closer to 1 \\(/rightarrow\\) more predictable, less imbalance A big disadvantage to the biased coin scheme is that the same probability is used regardless of the size of the imbalance (assuming it isn’t zero). In the next section, we introduce a method where the probability of allocating the next patient to the underrepresented treatment gets larger as the imbalance grows. 3.2.3.2 Urn models The urn starts off with a ball for each treatment, and a ball is added to the urn each time a participant is allocated. The ball is labelled according to the treatment allocation that participant did not receive. To allocate the next participant, a ball is drawn from the urn. If the allocations at this point are balanced, then the participant has equal probability of being allocated to each treatment. If there is imbalance, there will be more balls labelled by the underrepresented treatment, and so the participant is more likely to be allocated to that one. The greater the imbalance, the higher the probability of reducing it. The process described so far is a \\(UD\\left(1,1\\right)\\); one ball for each treatment to start with, one ball added after each allocation. More generally we assume a \\(UD\\left(r,s\\right)\\) scheme: \\(r\\) balls for each treatment to begin with \\(s\\) balls added after each allocation. Near the start of the allocation, the probabilities are likely to change a lot to address imbalance, but once a ‘reasonable number’ of allocations have been made it is likely to settle into simple random sampling (or very close). Once again, we can find the transition probabilities by considering the absolute imbalance \\(\\lvert D\\left(n\\right) \\rvert\\). Suppose that after participant \\(n\\), \\(N_T\\left(n\\right)\\) participants have been allocated to group \\(T\\), and \\(N_C\\left(n\\right) = n - N_T\\left(n\\right)\\) to group \\(C\\). The imbalance is therefore \\[D\\left(n\\right) = N_T\\left(n\\right) - N_C\\left(n\\right) = 2N_T\\left(n\\right) - n.\\] After \\(n\\) allocations there will be \\(2r + ns\\) balls in the urn: \\(r\\) for each treatment at the start, and \\(s\\) added after each allocation. Of these, \\(r + N_C\\left(n\\right)s\\) will be labelled by treatment \\(T\\) and \\(r + N_T\\left(n\\right)s\\) by treatment \\(C\\). To think about the probabilities for the absolute imbalance \\(\\lvert D\\left(n\\right)\\rvert\\), we have to be careful now about which direction it is in. If after allocation \\(n\\) there is imbalance in favour of treatment \\(C\\), then the probability that it becomes less imbalanced at the next allocation is the probability of the next allocation being to treatment \\(T\\), which is \\[ \\begin{aligned} p\\left(\\lvert D\\left(n+1\\right)\\rvert = j-1 \\mid D\\left(n\\right)=j, j&gt;0\\right) &amp; = \\frac{r + N_C\\left(n\\right)s}{2r + ns} \\\\ &amp; = \\frac{r + \\frac{1}{2}\\left(n + D\\left(n\\right)\\right)s}{2r + ns} \\\\ &amp; = \\frac{1}{2} + \\frac{D\\left(n\\right)s}{2\\left(2r + ns\\right)} \\\\ &amp; = \\frac{1}{2} + \\frac{\\lvert D\\left(n\\right)\\rvert s}{2\\left(2r + ns\\right)}. \\end{aligned} \\] Similarly, if there is currently an excess of patients allocated to treatment \\(T\\), then the imbalance will be reduced if the next allocation is to treatment \\(C\\), and so the conditional probability is \\[ \\begin{aligned} p\\left(\\lvert D\\left(n+1\\right)\\rvert = j-1 \\mid D\\left(n\\right)=j, j&lt;0\\right) &amp; = \\frac{r + N_T\\left(n\\right)s}{2r + ns} \\\\ &amp; = \\frac{r + \\frac{1}{2}\\left(n - D\\left(n\\right)\\right)s}{2r + ns} \\\\ &amp; = \\frac{1}{2} - \\frac{D\\left(n\\right)s}{2\\left(2r + ns\\right)}\\\\ &amp; = \\frac{1}{2} + \\frac{\\lvert D\\left(n\\right)\\rvert s}{2\\left(2r + ns\\right)}. \\end{aligned} \\] Because the process is symmetrical, an imbalance of a given magnitude (say \\(\\lvert D\\left(n\\right)\\rvert=j\\)) is equally likely to be in either direction. That is \\[p\\big(D\\left(n\\right) &lt; 0 \\mid \\lvert D\\left(n\\right)\\rvert =j \\big)= p\\big(D\\left(n\\right) &gt; 0 \\mid \\lvert D\\left(n\\right)\\rvert =j \\big) = \\frac{1}{2}.\\] Therefore we can use the law of total probability (or partition theorem) to find that \\[ p\\big(\\lvert D\\left(n+1\\right) \\rvert = j-1 \\mid \\lvert D\\left(n\\right) \\rvert = j \\big) = \\frac{1}{2} + \\frac{\\lvert D\\left(n\\right)\\rvert s}{2\\left(2r + ns\\right)}. \\] Since the two probabilities are equal this is trivial. Since the only other possibility is that the imbalance is increased by one, we also have \\[p\\big(\\lvert D\\left(n+1\\right) \\rvert = j+1 \\mid \\lvert D\\left(n\\right) \\rvert = j \\big) = \\frac{1}{2} - \\frac{\\lvert D\\left(n\\right)\\rvert s}{2\\left(2r + ns\\right)}. \\] As with the biased coin design, we also have the possibility that the imbalance after \\(n\\) allocations is zero, in which case the absolute imbalance after the next allocation will definitely be one. This gives us another simple random walk, with \\[ \\begin{aligned} P\\big(\\lvert D\\left(n+1\\right) \\rvert = 1 \\mid \\lvert D\\left(n\\right)=0\\big)&amp; = 1\\\\ P\\big(\\lvert D\\left(n+1\\right) \\rvert = j+ 1 \\mid \\lvert D\\left(n\\right)=j\\big)&amp; = \\frac{1}{2} - \\frac{\\lvert D\\left(n\\right)\\rvert s}{2\\left(2r + ns\\right)}\\\\ P\\big(\\lvert D\\left(n+1\\right) \\rvert = j-1 \\mid \\lvert D\\left(n\\right)=j\\big)&amp; = \\frac{1}{2} + \\frac{\\lvert D\\left(n\\right)\\rvert s}{2\\left(2r + ns\\right)} \\end{aligned} \\] Figure 3.6: Four realisations of absolute imbalance for r=1, s=1, N=50. Figure 3.7: Four realisations of absolute imbalance for r=1, s=8, N=50. Figure 3.8: Four realisations of absolute imbalance for r=8, s=1, N=50. We see that imbalance is reduced, particularly for small \\(n\\). A small \\(r\\) and large \\(s\\) enhance this, since the large number (\\(s\\)) of balls added to the urn with each allocation weight the probabilities more heavily, as in Figure 3.7. By contrast, if \\(r\\) is large and \\(s\\) is small, as in Figure 3.8, the probabilities stay closer to \\(\\left(\\frac{1}{2}, \\frac{1}{2}\\right)\\) and so more imbalance occurs early on. Particular advantages of the urn design are: Easy to implement for \\(K&gt;2\\) groups Responds to higher imbalance We’ve alluded to the idea of performing these algorithms within specific groups, to achieve balance among participants with particular characteristics. Next lecture we’ll think about allocation methods that put more focus on balance in relation to factors. 3.3 (Lecture 6) Incorporating baseline measurements At the start of the trial (ideally before allocation) various baseline measurements are usually taken. If the primary outcome variable is a continuous measurement (eg. blood pressure, weight,…) this same quantity will often be included, so that there is some measure of each participant’s condition/symptoms at the start of the trial. Factors such as age, sex, level of symptoms, things to do with treatment history and many others are included. Essentially, we include any variable we can that may lead to bias if not properly dealt with. The crucial thing is that none of these measurements (taken when they are) should be affected by the trial. Baseline measurements can be used in allocation, eg. baseline value of \\(X\\), age, sex, condition specifics,… Must not be affected by the trial 3.3.1 Stratified sampling The usual method of achieving balance with respect to prognostic factors is to divide each factor into several levels and to consider treatment assignment separately for patients having each particular combination of such factor levels. Such groups of patients are commonly referred to as randomization groups or strata. Treatment assignment is performed entirely separately for each stratum, a permuted block design of the type mentioned above often being used. In fact, using purely random treatment assignment for each stratum is equivalent to simple random assignment, so that some equalization of treatment numbers within each stratum is essential. This whole procedure is analogous to performing a factorial experiment, without being able to control the factor levels of the experimental units. Divide each prognostic factor into \\(\\geq2\\) levels, consider allocation separately within each combination of factor levels. Each such group is a ‘stratum’. Within each stratum, apply RBP, biased coin, urn model or your allocation method of choice (one that aims for balance). Example 3.4 Suppose we are planning a trial involving people over the age of 50, and we anticipate that age and sex might both play an important role in how participants respond to the treatment. For sex, we use the levels ‘male’ and ‘female’, and for age we split the range into 50-65, 66-80 and 81 or over. We therefore have six strata, and we use an allocation strategy independently in each stratum. For example, below we have used randomly permuted blocks of length four. Male Female 50-65 ABAB BBAA … ABBA BBAA … 66-80 BAAB AABB … BABA BAAB … 81 and over ABAB ABBA … ABBA BAAB … Each time a new participant arrives, we follow the randomization pattern for their stratum. We could use another allocation scheme within each stratum, for example an urn model or a biased coin. It is important that we use one that aims to conserve balance, or else the benefits of stratification are lost. A difficulty with stratified sampling is that the number of strata can quickly become large as the number of factors (or the number of levels within some factors) increases. For example, if we have four prognostic factors each with three levels, there are \\(3^4=81\\) strata. This creates a situation that is at best unwieldy, and at worst completely unworkable; in a small trial (with say 100 patients in each arm) there may be some strata with no patients in (this is actually not a problem), and probably many more with only one (this is much more problematic). Difficulty: Number of strata quickly becomes too large: eg. four factors with three levels each \\(\\rightarrow 3^4=81\\) strata. Unwieldy practically Very likely to have v small numbers in some strata 3.3.2 Minimization Minimization was first proposed by Taves (1974), then shortly after by Pocock and Simon (1975) and Freedman and White (1976). The aim of minimization is to minimize the difference between the two groups (\\(T\\) and \\(C\\)). It was developed for use with strata, as an alternative to randomly permuted blocks. Although the method was developed in the seventies, it has only gained popularity relatively recently, mainly as computers have become widely available. Aims to achieve balance between groups \\(T\\) and \\(C\\), taking prognostic factors into account. Has become more popular recently, because of computing availability. To form the strata, the people running the trial must first specify all of the factors they would like to be balanced between the two groups. These should be any variables that are thought to possibly affect the outcome. As an example, in a study comparing aspirin to a placebo preceding coronary artery surgery, Kallis et al. (1994) chose age (\\(\\leq{50}\\) or \\(&gt;50\\)), sex (M or F), operating surgeon (3 possibilities) and number of coronary arteries affected (1 or 2). When a patient enters the trial, these factors are listed. The patient is then allocated in such a way as to minimise any difference in these factors between the two groups. The minimization method has evolved since its conception, and exists in several forms. Two areas in which methods vary are Whether continuous variables have to be binned Whether there is any randomness It is generally agreed that if the risk of selection bias cannot be avoided, there should be an element of randomness. It is also usually accepted that if a variable is included in the minimization, it should also be included in the statistical analysis. 3.3.3 Minimization algorithm We have a trial in which patients are recruited sequentially and need to be allocated to arm \\(A\\) or \\(B\\). Pocock and Simon (1975) give an algorithm in the general case of \\(N\\) treatment arms, but we will not do that here. Suppose we require balance over several factors, and that these factors have \\(I, J, K, ...\\) levels. In our example above, there would be \\(I=2,\\; J=2,\\; K=3,\\; L=2\\). Note that this equates to 24 strata. We will use this example to explain minimisation. At some point in the trial, suppose we have recruited \\(n_{ijkl}\\) patients with levels \\(i,\\,j,\\,k,\\,l\\) of the factors. For example, this may be males, aged over 50, assigned to the second surgeon, with both coronary arteries affected. Within these, \\(n^A_{ijkl}\\) have been assigned to treatment arm \\(A\\), and \\(n^B_{ijkl}\\) to arm \\(B\\). So we have \\[ n^A_{ijkl} + n^B_{ijkl} = n_{ijkl} .\\] If we were to use random permuted blocks within each stratum, then we would be assured that \\[\\lvert n^A_{ijkl} - n^B_{ijkl} \\rvert \\leq{\\frac{1}{2}b},\\] where \\(b\\) is the block length. However, there are two issues with this: There may be very few patients in some strata, in which case RPBs will fail to provide adequate balance. It is unlikely that we actually need this level of balance. The first point is a pragmatic one - the method usually guaranteed to achieve good balance is likely to fail, at least for some strata. The second is more theoretical. In general, we require that groups be balanced according to each individual prognostic factor, but not to interactions. For example, it is often believed that younger patients would have generally better outcomes, but that other factors do not systematically affect this difference. Therefore, it is enough to make sure that the following are all small: \\[ \\begin{aligned} \\lvert n^A_{i+++} - n^B_{i+++} \\rvert&amp;\\text{ for each }i=1,\\ldots,I\\\\ \\lvert n^A_{+j++} - n^B_{+j++} \\rvert&amp;\\text{ for each }j=1,\\ldots,J\\\\ \\ldots&amp; \\end{aligned} \\] where \\(+\\) represents summation over the other factors, so that for example \\[n^A_{++k+} = \\sum\\limits_{i,j,l}{n^A_{ijkl}}\\] is the total number of patients with level \\(k\\) of that factor assigned to treatment arm \\(A\\). Therefore, instead of having \\(IJKL\\) constraints constraints, as we would with using randomly permuted blocks within each stratum, we have \\(I+J+K+L\\) constraints, one for each level of each factor. In our example this is 9 constraints rather than 24. In order to implement minimisation, we follow these steps: Allocate the first patient by simple randomisation. Suppose that at some point in the trial we have recruited \\(n_{ijkl}\\) patients with prognostic factors \\(i,\\,j,\\,k,\\,l\\). Of these \\(n^A_{ijkl}\\) are allocated to treatment arm \\(A\\) and \\(n^B_{ijkl}\\) to arm \\(B\\). A new patient enters the trial. They have prognostic factors at levels \\(w,\\,x,\\,y,\\,z\\). We form the sum \\[S=\\left(n^A_{w+++} - n^B_{w+++}\\right) + \\left(n^A_{+x++} - n^B_{+x++}\\right) + \\left(n^A_{++y+} - n^B_{++y+}\\right) + \\left(n^A_{+++z} - n^B_{+++z}\\right).\\] If \\(S&lt;0\\) (that is, allocation to arm \\(B\\) as dominated up to now) then we allocate the new patient to arm \\(A\\) with probability \\(P\\), with \\(P&gt;0.5\\). If the \\(S&gt;0\\), they are allocated to arm \\(B\\) with probability \\(P\\). If the \\(S=0\\), they are allocated to arm \\(A\\) with probability \\(\\frac{1}{2}\\). Some people set \\(P=1\\), whereas others would set \\(\\frac{1}{2}&lt;P&lt;1\\) to retain some randomness. Although setting \\(P=1\\) makes the system deterministic, to predict the next allocation a doctor (or whoever) would need to know \\(n^A_{i+++}\\) and so on. This is very unlikely unless they are deliberately seeking to disrupt the trial. However, generally the accepted approach is becoming to set \\(P&lt;1\\). Example 3.5 From Altman (1990) (citing Fentiman, Rubens, and Hayward (1983)). In this trial, 46 patients with breast cancer were allocated to receive either Mustine (arm A) or Talc (arm B) as treatment for pleural effusions (fluid between the walls of the lung). They used four prognostic factors: age (\\(\\leq{50}\\) or \\(&gt;50\\)), stage of disease (I or II, III or IV), time in months between diagnosis of breast cancer and diagnosis of pleural effusions (\\(\\leq{30}\\) or \\(&gt;30\\)) and menopausal status (Pre or post). Let’s suppose that 15 patients have already been allocated. The totals of patients in each treatment arm in terms of each level of each prognostic factor are shown in Table 3.1. Table 3.1: Table 3.2: Allocations of first 15 patients, divided by diagnostic factor factor level Mustine (A) Talc (B) Age 50 or younger 3 4 Age &gt;50 4 4 Stage I or II 1 2 Stage III or IV 6 6 Time interval 30 months or less 4 2 Time interval &gt;30 months 4 5 Menopausal status Pre 4 3 Menopausal status Post 5 3 Suppose our sixteenth patient is under 50, has disease at stage III, has less than 30 months between diagnoses and is pre-menopausal. Our calculation from step 4 of the minimisation algorithm is therefore \\[ \\begin{aligned} \\left(n^A_{1+++} - n^B_{1+++}\\right) + \\left(n^A_{+2++} - n^B_{+2++}\\right) + \\left(n^A_{++1+} \\right.&amp; \\left.- n^B_{++1+}\\right) + \\left(n^A_{+++1} - n^B_{+++1}\\right) \\\\ &amp; = \\left(3-4\\right) + \\left(6-6\\right) + \\left(4-2\\right) + \\left(4-3\\right) \\\\ &amp; = -1 + 0 + 2 + 1\\\\ &amp; = 2 . \\end{aligned} \\] Since our sum is greater than zero, we allocate the new patient to arm B (talc) with some probability \\(P\\in\\left(0.5,1\\right)\\) and update the table before allocating patient 17. ::: 3.4 Problems with allocation 3.4.1 Communication! 3.4.2 Interactions In clinical trials papers, the allocation groups are usually summarised in tables giving summary statistics (eg. mean and SD) of each characteristic for the control group and the intervention group. The aim of these is to show that the groups are similar enough for any difference in outcome to be attributed to the intervention itself. Figure 3.9 shows an example, taken from Ruetzler et al. (2013). Figure 3.9: Summary statistics for an RCT comparing a licorice gargle (the intervention) to a sugar-water gargle (the standard). From Ruetzler et al. (2013) The problem here is that only the marginal distributions are compared for similarity. Consider the following (somewhat extreme and minimalistic) scenario. A study aims to investigate the effect of some treatment, and to balance for gender and age in their allocation, resulting in the following summary table. Male Female Control 57.51 (7.09) 40.31 (5.83) Intervention 44.19 (5.96) 60.03 (5.27) This appears to be a reasonably balanced design. However, if we look at the joint distribution, we see that there are problems. If the intervention is particularly effective in older men, our trial will not notice. Likewise, if older women generally have a more positive outcome than older men, our trial may erroneously find the intervention to be effective. Although this example is highly manufactured and [hopefully!] unlikely to take place in real life, for clinical trials there are often many demographic variables and prognostic factors being taken into account. Achieving joint balance across all them is very difficult, and extremely unlikely to happen if it isn’t aimed for. Treasure and MacRae (1998) give an example in relation to a hypothetical study on heart disease Supposing one group has more elderly women with diabetes and symptoms of heart failure. It would then be impossible to attribute a better outcome in the other group to the beneficial effects of treatment since poor left ventricular function and age at outset are major determinants of survival in any longitudinal study of heart disease, and women with diabetes, as a group, are likely to do worse. At this point the primary objective of randomisation—exclusion of confounding factors—has failed. … If a very big trial fails, because, for example, the play of chance put more hypertensive smokers in one group than the other, the tragedy for the trialists, and all involved, is even greater. However, this issue is rarely addressed in clinical trials: a lot of faith is placed (with reasonable justification) in the likely balance achieved by random sampling, whatever method is used. We will also see in the next Chapter that we can account for some degree of imbalance at the analysis stage. References Altman, Douglas G. 1990. Practical Statistics for Medical Research. CRC press. Cottingham, Marci D, and Jill A Fisher. 2022. “Gendered Logics of Biomedical Research: Women in US Phase i Clinical Trials.” Social Problems 69 (2): 492–509. Fentiman, Ian S, Robert D Rubens, and John L Hayward. 1983. “Control of Pleural Effusions in Patients with Breast Cancer a Randomized Trial.” Cancer 52 (4): 737–39. Freedman, LS, and Susan J White. 1976. “On the Use of Pocock and Simon’s Method for Balancing Treatment Numbers over Prognostic Factors in the Controlled Clinical Trial.” Biometrics, 691–94. Health, National Institute of. 2023. “History of Women’s Participation in Clinical Research.” Office of Research on Women’s Health. https://orwh. od. nih. gov/toolkit …. https://orwh.od.nih.gov/toolkit/recruitment/history. Kallis, P, JA Tooze, S Talbot, D Cowans, DH Bevan, and T Treasure. 1994. “Pre-Operative Aspirin Decreases Platelet Aggregation and Increases Post-Operative Blood Loss–a Prospective, Randomised, Placebo Controlled, Double-Blind Clinical Trial in 100 Patients with Chronic Stable Angina.” European Journal of Cardio-Thoracic Surgery: Official Journal of the European Association for Cardio-Thoracic Surgery 8 (8): 404–9. Kar, Sumit, Ajay Krishnan, Preetha K, and Atul Mohankar. 2012. “A Review of Antihistamines Used During Pregnancy.” Journal of Pharmacology and Pharmacotherapeutics 3 (2): 105–8. Pocock, Stuart J, and Richard Simon. 1975. “Sequential Treatment Assignment with Balancing for Prognostic Factors in the Controlled Clinical Trial.” Biometrics, 103–15. Ruetzler, Kurt, Michael Fleck, Sabine Nabecker, Kristina Pinter, Gordian Landskron, Andrea Lassnigg, Jing You, and Daniel I Sessler. 2013. “A Randomized, Double-Blind Comparison of Licorice Versus Sugar-Water Gargle for Prevention of Postoperative Sore Throat and Postextubation Coughing.” Anesthesia &amp; Analgesia 117 (3): 614–21. Taves, Donald R. 1974. “Minimization: A New Method of Assigning Patients to Treatment and Control Groups.” Clinical Pharmacology &amp; Therapeutics 15 (5): 443–53. Treasure, Tom, and Kenneth D MacRae. 1998. “Minimisation: The Platinum Standard for Trials?: Randomisation Doesn’t Guarantee Similarity of Groups; Minimisation Does.” Bmj. British Medical Journal Publishing Group. Vitale, Cristiana, Massimo Fini, Ilaria Spoletini, Mitja Lainscak, Petar Seferovic, and Giuseppe MC Rosano. 2017. “Under-Representation of Elderly and Women in Clinical Trials.” International Journal of Cardiology 232: 216–21. "],["rct-analysis.html", "4 (Lecture 7) Analyzing RCT data 4.1 Confidence intervals and P-values 4.2 (Lecture 8) Using baseline values 4.3 Analysis of covariance (ANCOVA) 4.4 Some follow-up questions….", " 4 (Lecture 7) Analyzing RCT data We’re now in the post-trial stage. The trial has been run, and we have lots of data to analyze to try to assess what effect the treatment or intervention has had. In general we will use the notation \\(\\tau\\) to denote the treatment effect. In this chapter we’ll keep our focus on the scenario where the trial outcome is measured on a continuous scale, but in later weeks we’ll go on to look at other types of data. We now have trial data, and want to estimate the treatment effect \\(\\tau\\). Example 4.1 From Hommel et al. (1986). 16 diabetes patients Group \\(T\\) receive Captopril, a drug that may reduce blood pressure. Placebo given to group \\(C\\) Primary outcome \\(X\\) is systolic blood pressure (mmHg) This is important, since for those with diabetes, high blood pressure can exacerbate kidney disease (specifically diabetic nephropathy, a complication of diabetes). To participate in the trial, people had to be insulin-dependent and already affected by diabetic nephropathy. In the trial, systolic blood pressure was measured before participants were allocated to each trial arm, and then measured again after one week on treatment. A placebo was given to the control group, so that all participants were blinded. The baseline and outcome blood pressure measurements (in mmHg) are shown in Table 4.1. We see that nine participants were assigned to the treatment arm (Captopril) and the remaining seven to the placebo group. Hommel et al. (1986) say that the patients were ‘randomly allocated’ to their group. Table 4.1: Data for the Captopril trial from Hommel et al. (1986). Patient (ID) Baseline (B) Outcome at 1 week (X) Trial Arm 1 147 137 Captopril 2 129 120 Captopril 3 158 141 Captopril 4 164 137 Captopril 5 134 140 Captopril 6 155 144 Captopril 7 151 134 Captopril 8 141 123 Captopril 9 153 142 Captopril 1 133 139 Placebo 2 129 134 Placebo 3 152 136 Placebo 4 161 151 Placebo 5 154 147 Placebo 6 141 137 Placebo 7 156 149 Placebo This is very small dataset, and so in that respect it is quite unusual, but its structure is similar to many other trials. We will build up from the simplest type of analysis to some more complicated / sophisticated approaches. 4.1 Confidence intervals and P-values Because the randomization process should produce groups that are comparable, we can in principle compare \\(X\\) between the groups. Example 4.2 Summary statistics of the outcome for each group are shown below. Table 4.2: Summary statistics for each group. Sample Size Mean (mmHg) SD (mmHg) SE of mean (mmHg) Captopril 9 135.33 8.43 2.81 Placebo 7 141.86 6.94 2.62 We have \\[ \\begin{aligned} \\bar{x}_T &amp; = 135.33 \\\\ \\bar{x}_C &amp; = 141.86. \\end{aligned} \\] Difference in average of \\(X\\) between the two groups \\(141.86 - 135.33 = 6.53 \\text{mmHg}\\). Clearly overall there has been some reduction in systolic blood pressure for those in the Captopril arm, but how statistically sound is this as evidence? It could be that really (for the hypothetical population) there is no reduction, and we have just been ‘lucky’. The variances within the two groups are fairly close, so we can use the pooled estimate of standard deviation: \\[ s_p = \\sqrt{\\frac{\\sum\\limits_{i=1}^N\\left(n_i-1\\right)s_i^2}{\\sum\\limits_{i-1}^N\\left(n_i-1\\right)}}. \\] In our case \\[ \\begin{aligned} s_p&amp;= \\sqrt{\\frac{8\\times{8.43^2} + 6 \\times{6.94^2}}{8+6}}\\\\ &amp; = 7.82\\text{ mmHg.} \\end{aligned} \\] We can do an independent two-sample \\(t\\)-test, \\[ \\begin{aligned} t &amp; = \\frac{\\bar{X_C} - \\bar{X_T}}{s_p\\sqrt{\\frac{1}{n_C} + \\frac{1}{n_T}}}\\\\ &amp; = \\frac{6.53}{7.82\\sqrt{\\frac{1}{7} + \\frac{1}{9}}} \\\\ &amp; = 1.65. \\end{aligned} \\] Note that here the placebo group is group \\(C\\), and the Captopril group is group \\(T\\). Under \\(H_0\\), this value should be \\(t_{14}\\) (\\(n_i-1\\) for each group). Figure 4.1: The distribution \\(t_{14}\\), with \\(t=1.65\\) shown by the dashed line and the ‘more extreme’ areas shaded. The dashed line is at \\(t=1.65\\), and the red shaded areas show anywhere ‘at least as extreme’. We can find the area (ie. the probability of anything at least as extreme as our found value) in R by 2*(1-pt(1.65, df=14)) ## [1] 0.1211902 So \\(p=0.121\\) - not significant even at \\(\\alpha=0.1\\). 4.1.1 What do we do with this outcome? Worst case scenario: \\(\\hat{\\tau} = 6.53\\text{ mmHg}\\) is large enough to be compelling Dataset is too small for it to be statistically significant Can’t confidently conclude that Captopril has any effect on blood pressure (reject \\(H_0\\)). Can’t say that there is no effect. This is exactly the sort of scenario we hoped to avoid when planning our study. Consider the range of treatment effects that are compatible with our trial data. That is, we find the set \\[\\left\\lbrace \\tau \\mid \\frac{\\lvert \\bar{x}_C - \\bar{x}_T - \\tau \\rvert}{s\\sqrt{n_C^{-1} + n_T^{-1}}} \\leq t_{n_C+n_T-2;\\,0.975} \\right\\rbrace. \\] Suppose the true treatment effect is \\(\\tau^*\\), and we test \\(H_0:\\; \\tau = \\tau^*\\). For all \\(\\tau^*\\) inside this range, our data are not sufficiently unlikely to reject the \\(H_0\\) at the 0.05 level. For all values of \\(\\tau^*\\) outside this range, our data are sufficiently unlikely to reject that hypothesis. Rearrange to give a 95% confidence interval for \\(\\tau\\), \\[\\left\\lbrace \\tau \\mid \\bar{x}_C - \\bar{x}_T - t_{n_C+n_T-2;\\,0.975}\\,s\\sqrt{n_C^{-1} + n_T^{-1}} \\leq \\tau \\leq \\bar{x}_C - \\bar{x}_T + t_{n_C+n_T-2;\\,0.975}\\,s\\sqrt{n_C^{-1} + n_T^{-1}} \\right\\rbrace \\] Example 4.3 Continuing our example, we have \\[\\left\\lbrace \\tau \\mid \\frac{\\lvert 6.53 - \\tau \\rvert}{7.82\\sqrt{\\frac{1}{7} + \\frac{1}{9}}} \\leq t_{14;0.975} = 2.145 \\right\\rbrace \\] Here, \\(t_{14;0.975} = 2.145\\) is the \\(t\\)-value for a significance level of \\(0.05\\), so if we were working to a different significance level we would change this. Rearranging as above, this works out to be the interval \\[ -1.92 \\leq \\tau \\leq 14.98. \\] Notice that zero is in this interval, consistent with the fact that we failed to reject the null hypothesis. Some things to note We can compute this confidence interval whether or not we failed to reject the null hypothesis that \\(\\tau=0\\), and for significance levels other than 0.05. Reporting the CI is much more informative than simply reporting the \\(P\\)-value. In our Captopril example, we found that a negative treatment effect (ie. Captopril reducing blood pressure less than the placebo) of more than 2 mmHg was very unlikely, whereas a positive effective (Captopril reducing blood pressure) of up to 15 mmHg was plausible. If Captopril were inexpensive and had very limited side effects (sadly neither of which is true) it may still be an attractive drug. These confidence intervals are exactly the same as you have learned before, but we emphasise them because they are very informative in randomised controlled trials (but not so often used!). At the post trial stage, when we have data, the confidence interval is the most useful link to the concept of power, which we thought about at the planning stage. Remember that the power function is defined as \\[\\psi \\left(\\tau\\right) = P\\left(\\text{Reject }H_0\\mid \\tau\\neq 0\\right),\\] This was calculated in terms of the theoretical model of the trial, and in terms of some minimum detectable effect size \\(\\tau_M\\) that we wanted to be able to correctly detect with probability \\(1-\\beta\\) (the power). Sometimes people attempt to re-calculate the power after the trial, to detect whether the trial was underpowered. However, now we have actual data. You can’t calculate the power of a trial after it’s happened, but if we fail to reject \\(H_0\\) and \\(\\tau_M\\) is in the confidence interval for \\(\\tau\\), then that is a good indication that our trial was indeed underpowered. 4.2 (Lecture 8) Using baseline values In our example above, our primary outcome variable \\(X\\) was the systolic blood pressure of each participant at the end of the intervention period. In Table 4.1 we also have baseline measurements: measurements of systolic blood pressure for each patient from before the intervention period. We’ll denote these \\(B_T\\) and \\(B_C\\). Baseline measurements are useful primarily for two reasons: They can be used to assess the balance of the design. They can be used in the analysis. We will demonstrate these by returning to our Captopril example. Example 4.4 Balance: Placebo group: \\(\\bar{b}_C = 146.6 \\text{ mmHg}\\) and \\(SD\\left(b_C\\right) = 12.3 \\text{ mmHg}\\) Captopril group: \\(\\bar{b}_T = 148 \\text{ mmHg}\\) and \\(SD\\left(b_T\\right) = 11.4 \\text{ mmHg}\\) Not identical, but sufficiently similar not to suspect systematic imbalance. In a study this small there is likely to be some difference. Note, it’s sensible to compare things informally like this, but sometimes people suggest using a t-test or similar to test whether the two groups are similar enough. This is a flawed exercise: a formal hypothesis test is testing whether what we see is likely to have arisen by random change. We know that this arose by random chance, because we randomly allocated the patients! A formal test only makes sense if you suspect that something has systematically disrupted the randomisation. Analysis: Interested in whether Captopril has reduced BP for each individual - makes sense to compare change in BP (\\(X-B\\)), rather than final BP measurement. We can see individual data in Table 4.3 and summary statistics in Table 4.4. Table 4.3: Data for the Captopril trial from Hommel et al. (1986), with differences shown. Patient (ID) Baseline (B) Outcome at 1 week (X) Trial Arm Difference 1 147 137 Captopril -10 2 129 120 Captopril -9 3 158 141 Captopril -17 4 164 137 Captopril -27 5 134 140 Captopril 6 6 155 144 Captopril -11 7 151 134 Captopril -17 8 141 123 Captopril -18 9 153 142 Captopril -11 1 133 139 Placebo 6 2 129 134 Placebo 5 3 152 136 Placebo -16 4 161 151 Placebo -10 5 154 147 Placebo -7 6 141 137 Placebo -4 7 156 149 Placebo -7 Table 4.4: Summary statistics for each group. Sample Size Mean (mmHg) SD (mmHg) SE of mean (mmHg) Captopril 9 -12.67 8.99 3.00 Placebo 7 -4.71 7.91 2.99 Now we can perform our test as before, \\[ t = \\frac{- 12.67 - (-4.71)}{8.54\\sqrt{\\frac{1}{7}+\\frac{1}{9}}} = -1.850 \\] where 8.54 is the pooled standard deviation (as before). Under the null distribution of no difference, this has a \\(t\\)-distribution with 14 degrees of freedom, and so we have a \\(P\\)-value of 0.086. Our 0.95 confidence interval is \\[ -12.67 - (-4.71) \\pm t_{14;\\,0.975}\\times 8.54\\sqrt{\\frac{1}{7}+\\frac{1}{9}} = \\left[-17.2,\\;1.3\\right].\\] Taking into account the baseline values in this way has slightly reduced the \\(P\\)-value (though still \\(p&gt;0.05\\)) shifted the confidence interval slightly lower 4.2.1 Why did the CI and p-value change? Notation: Baseline: \\(B_C,\\,B_T\\) Outcome : \\(X_C,\\,X_T\\) All participants have been randomised from the same population, so \\[\\operatorname{E}\\left(B_C\\right) = \\operatorname{E}\\left(B_T\\right) = \\mu_B.\\] Assuming some treatment effect \\(\\tau\\) (which could still be zero) we have \\[ \\begin{aligned} \\operatorname{E}\\left(X_C\\right) &amp; = \\mu\\\\ \\operatorname{E}\\left(X_T\\right) &amp; = \\mu + \\tau. \\end{aligned} \\] Sometimes we’ll have \\(\\mu_B=\\mu\\), but this won’t always be the case. Usually we will assume that \\[\\operatorname{Var}\\left(X_C\\right) = \\operatorname{Var}\\left(X_T\\right) = \\operatorname{Var}\\left(B_C\\right) = \\operatorname{Var}\\left(B_T\\right) = \\sigma^2,\\] and this is generally fairly reasonable in practice. For our two analyses so far we have \\[ \\begin{aligned} \\operatorname{E}\\left(X_T\\right) - \\operatorname{E}\\left(X_C\\right) &amp; = \\left(\\mu + \\tau\\right) - \\mu = \\tau\\\\ \\operatorname{E}\\left(X_T - B_T\\right) - \\operatorname{E}\\left(X_C - B_C\\right) &amp; = \\left(\\mu - \\mu_B + \\tau\\right) - \\left(\\mu - \\mu_B\\right) = \\tau, \\end{aligned} \\] that is, both are unbiased estimators of \\(\\tau\\). However, whereas the first is based on data with variance \\(\\sigma^2\\), the second has \\[ \\begin{aligned} \\operatorname{Var}\\left(X_T-B_T\\right) &amp; = \\operatorname{Var}\\left(X_T\\right) + \\operatorname{Var}\\left(B_T\\right) - 2\\operatorname{cov}\\left(X_T,B_T\\right)\\\\ &amp; = \\sigma^2 + \\sigma^2 - 2\\rho\\sigma^2 \\\\ &amp; = 2\\sigma^2\\left(1-\\rho\\right), \\end{aligned} \\] where \\(\\rho\\) is the true correlation between \\(X\\) and \\(B\\), and is assumed to be the same in either group. Similarly, \\[\\operatorname{var}\\left(X_C-B_C\\right) = 2\\sigma^2\\left(1-\\rho\\right).\\] Using this to work out the variance of the estimator \\(\\hat{\\tau}\\) we find that for comparing means we have \\[\\operatorname{var}\\left(\\tau\\right) = \\operatorname{var}\\left(\\bar{x}_T - \\bar{x}_C\\right) = \\sigma^2\\left(\\frac{1}{m}+\\frac{1}{n}\\right).\\] whereas for comparing differences from baseline \\[\\operatorname{var}\\left(\\tau\\right) = \\operatorname{var}\\left[\\left(\\overline{X_T-B_T}\\right) - \\left(\\overline{X_C - B_C}\\right)\\right] = 2\\sigma^2\\left(1-\\rho\\right)\\left(\\frac{1}{m}+\\frac{1}{n}\\right).\\] Therefore, If \\(\\frac{1}{2}&lt;\\rho\\leq 1\\) there will be a smaller variance when comparing differences If \\(0\\leq\\rho&lt;\\frac{1}{2}\\)there will be a smaller variance when comparing outcome variables Intuitively, this seems reasonable: if the correlation between baseline and outcome measurements is very strong, then we can remove some of the variability between participants by taking into account their baseline measurement. However, if the correlation is weak, then by including the baseline in the analysis we are essentially just introducing noise. For our Captopril example, the sample correlation between baseline and outcome is 0.63 in the Captopril group and 0.80 in the Placebo group. This fits with the \\(P\\)-value having reduced slightly. 4.2.2 A dodgy way to use baseline variables (skip this in lectures!) Sometimes the analysis performed on a dataset is rather spurious, but it isn’t always immediately obvious why. We’ll look at one example now, because it is done sometimes. Look at each group separately and determine whether there has been a significant change in the outcome variable (assumes \\(\\mu_B = \\mu\\)). For Captopril data, we could perform a paired \\(t\\)-test on the difference between baseline \\(B\\) and outcome \\(X\\) for each patient, for each group. If we do this, we find the summary statistics in Table 4.5. Table 4.5: Summary statistics for the dodgy analysis t_stat df p_value Captopril 4.23 8 0.003 Placebo 1.58 6 0.170 We find Strong evidence for a change in blood pressure for the Captopril patients (group \\(T\\)) No such evidence for the placebo patients. Can we therefore conclude that Captopril is significantly better than the placebo? No! The analysis is flawed: The \\(p\\)-value of 0.17 in the control group doesn’t show that \\(H_0\\) is true (no treatment effect for the control group) is true, just that we can’t reject the null hypothesis. It is quite possible that there is a difference in the control group, and that numerically it could even be comparable to that in the treatment group, so although we can say that there is a significant reduction in blood pressure for the captopril group, we can’t conclude that Captopril is better than the placebo. Having set up the experiment as a randomised controlled trial, with a view to comparing the two groups, it seems strange to then deal with them separately. 4.3 Analysis of covariance (ANCOVA) Previously: based our analysis on the baseline values being statistically identical draws from the underlying distribution, and therefore having the same expectation and variance. However, in the event there will be some imbalance. We can see this in our Captopril example. Eg. we saw that difference in baseline means is \\(\\bar{b}_C - \\bar{b}_T = 1.4 \\text{ mmHg}\\). Not clinicaly significant Not large enough to make us doubt our randomisation A difference nonetheless Figure 4.2: Baseline measurements from the Captopril trial. Basic principle of ANCOVA: if there is some correlation between baseline and outcome, then baselines differing leads to outcomes differing, even if there is no treatment effect (ie. if \\(\\tau=0\\)). Indeed, how do we decide how much of the difference in outcome is down to the treatment itself, and how much is simply the difference arising from different samples? This issue arises in many trials, particularly where there is a strong correlation between baseline and outcome measurements. 4.3.1 The theory Usual notation and assumptions: Outcome \\(X\\), baseline \\(B\\) \\(E\\left(X_T\\right) = \\mu + \\tau\\) \\(E\\left(X_C\\right) = \\mu\\) \\(var\\left(X_T\\right) = var(X_C) = \\sigma^2\\) Correlation between \\(B\\) and \\(X\\) is \\(\\rho\\) within each group We want to learn about \\(\\tau\\) Baseline \\(B\\) (same measurement as \\(X\\)) is measured before start of trial. Assume \\(E(B_T) = E(B_C) = \\mu_B\\) and \\(var(B_C) = var(B_T) = \\sigma^2\\). Also assume both groups have size \\(N\\), therefore \\(2N\\) patients in all. We observe baseline measurements \\(b_1,\\,b_2,\\ldots,b_{2N}\\). 4.3.1.1 Constructing an estimator Let’s suppose that random variables \\(Z\\) and \\(Y\\) are jointly normally distributed with correlation \\(\\rho\\) \\[\\begin{equation} \\begin{pmatrix} Z\\\\ Y \\end{pmatrix} \\sim N\\left( \\begin{pmatrix} \\mu_Z\\\\ \\mu_Y \\end{pmatrix},\\; \\begin{pmatrix} \\sigma^2_Z &amp; \\rho\\sigma_Z\\sigma_Y \\\\ \\rho\\sigma_Z\\sigma_Y &amp; \\sigma^2_Y \\end{pmatrix} \\right). \\tag{4.1} \\end{equation}\\] From Equation (4.1), we know that \\(\\operatorname{E}\\left(Y\\right) = \\mu_Y\\). But, if we have observed \\(Z=z\\), this gives us some information about likely values of \\(Y\\): if \\(\\rho&gt;0\\) then a lower value of \\(z\\) should lead us to expect a lower value of \\(Y\\), for example. Figure 4.3 shows \\[\\begin{equation} \\begin{pmatrix} Z\\\\ Y \\end{pmatrix} \\sim N\\left( \\begin{pmatrix} 0\\\\ 0 \\end{pmatrix},\\; \\begin{pmatrix} 2 &amp; 1.5 \\\\ 1.5 &amp; 3 \\end{pmatrix} \\right). \\tag{4.2} \\end{equation}\\] Figure 4.3: A bivariate normal density. The higher the value of \\(\\rho\\) (in magnitude), the more the conditional distribution of \\(Y\\) given an observed value of \\(z\\) deviates from the marginal distribution of \\(Y\\) (in our example, \\(N\\left(0,\\;3\\right)\\)). In particular, \\[\\operatorname{E}\\left(Y\\mid{Z=z}\\right)\\neq{\\operatorname{E}\\left(Y\\right)}.\\] If we have another random variable, \\(W\\), that is independent of \\(Y\\) (and note that if two normally distributed variables are uncorrelated, they are also independent), then observing \\(W=w\\) doesn’t give us any information about the distribution of \\(Y\\), so we have \\[ \\operatorname{E}\\left(Y\\mid{W=w}\\right)={\\operatorname{E}\\left(Y\\right)}. \\] We can combine this information to work out \\(\\operatorname{E}\\left(Y\\mid{Z=z}\\right)\\). Firstly, we’ll calculate the covariance of \\(Z\\) and \\(Y-kZ\\), for some constant \\(k\\). We can find this by \\[ \\begin{aligned} \\operatorname{cov}\\left(Z,\\,Y-kZ\\right) &amp;= \\operatorname{E}\\big[\\left(Z-\\mu_Z\\right)\\left(Y-kZ - \\mu_Y + k\\mu_Z\\right)\\big]\\\\ &amp;\\text{ (using that }\\operatorname{cov\\left(Z,Y\\right)=\\operatorname{E}\\left[\\left(Z-\\operatorname{E}\\left(Z\\right)\\right)\\left(Y-\\operatorname{E}\\left(Y\\right)\\right)\\right]}\\\\ &amp; = \\operatorname{E}\\left[\\left(Z-\\mu_Z\\right)\\left(Y-\\mu_Y\\right) - k\\left(Z-\\mu_Z\\right)^2\\right]\\\\ &amp; = \\rho \\sigma_Z\\sigma_Y - k\\sigma_Z^2. \\end{aligned} \\] If we set \\[k = \\beta = \\frac{\\rho\\sigma_Y}{\\sigma_Z} \\] then \\(\\operatorname{cov}\\left(Z,\\,Y-\\beta Z\\right)=0\\), and since \\(Y-\\beta Z\\) is also normally distributed, this means that \\(Z\\) and \\(Y-\\beta Z\\) are independent. Therefore we have \\[\\operatorname{E}\\left(Y-\\beta Z \\mid Z=z\\right) = \\operatorname{E}\\left(Y-\\beta Z\\right) = \\mu_Y - \\beta \\mu_Z.\\] However, since we’re conditioning on an observed value of \\(Z=z\\) we can take \\(Z\\) to be fixed at this value, and so \\(\\operatorname{E}\\left(\\beta Z\\mid{Z=z}\\right) = \\beta z\\). Finally, this allows us to calculate \\[ \\begin{aligned} \\operatorname{E}\\left(Y\\mid{Z=z}\\right) &amp; = \\operatorname{E}\\left(\\beta Z\\mid{Z=z}\\right) + \\mu_Y - \\beta\\mu_Z\\\\ &amp; = \\mu_Y + \\beta\\left(z - \\mu_Z\\right). \\end{aligned} \\] Applying this to our clinical trials setup: \\[ \\begin{aligned} \\operatorname{E}\\left(X_i\\mid{b_i}\\right) &amp;= \\mu + \\rho\\left(b_i - \\mu_B\\right)\\text{ in the control group}\\\\ \\operatorname{E}\\left(X_i\\mid{b_i}\\right) &amp;= \\mu +\\tau + \\rho\\left(b_i - \\mu_B\\right)\\text{ in the test group.} \\end{aligned} \\] From this, we find that \\[\\begin{equation} \\operatorname{E}\\left(\\bar{X}_T - \\bar{X}_C\\mid{\\bar{b}_T,\\,\\bar{b}_C}\\right) = \\tau + \\rho\\left(\\bar{b}_T - \\bar{b}_C\\right). \\tag{4.3} \\end{equation}\\] If there is a difference in the baseline mean between groups, then the difference in outcome means is not an unbiased estimator of the treatment effect \\(\\tau\\). Assuming \\(\\rho&gt;0\\) (which is almost always the case) then if \\(\\bar{b}_T&gt;\\bar{b}_C\\) the difference in outcome means overestimates \\(\\tau\\) if \\(\\bar{b}_T&lt;\\bar{b}_C\\), the difference in outcome means underestimates \\(\\tau\\). The only situation in which the difference in outcome means is an unbiased estimator is when \\(\\rho=0\\), however this is not common in practice. Comparing the difference between outcome and baseline, as we did in 4.2, does not solve this problem, since we have \\[\\operatorname{E}\\left[\\left(\\bar{X}_T - \\bar{b}_T\\right) - \\left(\\bar{X}_C - \\bar{b}_C\\right)\\mid{\\bar{b}_T,\\,\\bar{b}_C}\\right] = \\tau + \\left(\\rho-1\\right)\\left(\\bar{b}_T - \\bar{b}_C\\right),\\] which is similarly biased (unless \\(\\rho=1\\), which is never the case). Notice, however, that if we use as our estimator \\[\\begin{equation} \\hat{\\tau} = \\left(\\bar{X}_T - \\bar{X}_C\\right) - \\rho \\left(\\bar{b}_T - \\bar{b}_C\\right) \\tag{4.4} \\end{equation}\\] then, following from Equation (4.3) we have \\[\\operatorname{E}\\left[\\left(\\bar{X}_T - \\bar{X}_C\\right) - \\rho \\left(\\bar{b}_T - \\bar{b}_C\\right)\\mid{\\bar{b}_T,\\,\\bar{b}_C}\\right] = \\tau + \\rho\\left(\\bar{b}_T - \\bar{b}_C\\right)- \\rho\\left(\\bar{b}_T - \\bar{b}_C\\right) = \\tau. \\] 4.3.1.2 What’s the variance of this estimator? To work out the variance of \\(\\hat{\\tau}\\) in Equation (4.4) we need to go back to our bivariate normal variables. Recall that \\(\\operatorname{var}\\left(Y\\right) = \\operatorname{E}\\left[Y^2\\right] - \\left[\\operatorname{E}\\left(Y\\right)\\right]^2\\), and so \\[\\begin{equation} \\operatorname{var}\\left(Y\\mid{Z=z}\\right) = \\operatorname{E}\\left(Y^2\\mid{Z=z}\\right) - \\left[\\operatorname{E}\\left(Y\\mid{Z=z}\\right)\\right]^2. \\tag{4.5} \\end{equation}\\] We already know the second term, and we can find the first term using the same idea as before, this time noting that \\(Z\\) and \\(\\left(Y-\\beta Z\\right)^2\\) are independent. From this, and using the fact that (for example) \\[ \\begin{aligned} \\operatorname{var}\\left(Z\\right) &amp; = \\operatorname{E}\\left(Z^2\\right) - \\left[\\operatorname{E}\\left(Z\\right)\\right]^2\\\\ \\text{and therefore} &amp; \\\\ \\operatorname{E}\\left(Z^2\\right) &amp;= \\sigma_Z^2 + \\mu_Z^2, \\end{aligned} \\] we find that \\[\\begin{equation} \\operatorname{E}\\left[\\left(Y-\\beta Z\\right)^2 \\mid Z=z\\right] = \\operatorname{E}\\left[\\left(Y-\\beta Z\\right)^2\\right] =S^2 + \\left(\\mu_Y - \\beta \\mu_Z\\right)^2, \\tag{4.6} \\end{equation}\\] where \\(S^2 = \\sigma^2_Y + \\beta^2\\sigma^2_Z - 2\\beta\\rho\\sigma_Z\\sigma_Y = \\sigma^2_Y\\left(1-\\rho^2\\right)\\) (by plugging in \\(\\beta = \\frac{\\rho\\sigma_Y}{\\sigma_Z}\\)). If we multiply out the left-hand side of Equation (4.6), we find that this is the same as \\[\\operatorname{E}\\left[Y^2\\mid{Z=z}\\right] - 2\\beta\\operatorname{E}\\left(Y\\mid{Z=z}\\right) + \\beta^2 z^2 = \\operatorname{E}\\left[Y^2\\mid{Z=z}\\right] - 2\\beta z\\left(\\mu_Y - \\beta\\mu_Z\\right) - \\beta^2 z^2.\\] Equating this with Equation (4.6) and rearranging, we find \\[\\operatorname{E}\\left[Y^2\\mid{Z=z}\\right] = S^2 + \\left(\\mu_Y - \\beta\\mu_Z\\right)^2 + 2\\beta z\\left(\\mu_Y - \\beta \\mu_Z\\right) + \\beta^2z^2.\\] Now we can expand out \\[\\operatorname{E}\\left(Y\\mid{Z=z}\\right) = \\mu_Y + \\beta\\left(z-\\mu_Z\\right) = \\left(\\mu_Y - \\beta\\mu_Z\\right) +\\beta z\\] to find \\[\\left[\\operatorname{E}\\left(Y\\mid{Z=z}\\right) \\right]^2 = \\left(\\mu_Y - \\beta\\mu_Z\\right)^2 + 2\\beta z \\left(\\mu_Y - \\beta\\mu_Z\\right) + \\beta^2 z^2.\\] Finally (!) we can use these two expressions to find \\[ \\begin{aligned} \\operatorname{var}\\left(Y\\mid{Z=z}\\right) &amp; = \\operatorname{E}\\left[Y^2\\mid{Z=z}\\right] - \\left[\\operatorname{E}\\left(Y\\mid{Z=z}\\right)\\right]^2\\\\ &amp; = S^2 \\\\ &amp; = \\sigma_Y^2\\left(1-\\rho^2\\right). \\end{aligned} \\] One thing to notice is that this conditional variance of \\(Y\\) doesn’t depend on the observed value of \\(Z=z\\). It can also never exceed \\(\\sigma^2_Y\\), and is only equal to \\(\\sigma^2_Y\\) if \\(Z\\) and \\(Y\\) are uncorrelated. Back to our estimator! Recall that in ANCOVA our estimator of the treatment effect \\(\\tau\\) is \\[ \\hat{\\tau} = \\left(\\bar{X}_T - \\bar{X}_C\\right) - \\rho\\left(\\bar{b}_T - \\bar{b}_C\\right)\\] and that we have \\[\\operatorname{cor}\\left(\\bar{X}_T - \\bar{X}_C,\\; \\bar{b}_T - \\bar{b}_C\\right) = \\rho.\\] Therefore, using the result we just found, \\[ \\begin{aligned} \\operatorname{var}\\left(\\hat{\\tau}\\right) = \\operatorname{var}\\left[\\left(\\bar{X}_T - \\bar{X}_C\\right) - \\rho\\left(\\bar{b}_T - \\bar{b}_C\\right)\\mid{\\bar{b}_T,\\bar{b}_C}\\right] &amp;= \\operatorname{var}\\left[\\left(\\bar{X}_T - \\bar{X}_C\\right) \\mid{\\bar{b}_T,\\bar{b}_C}\\right]\\\\ &amp; = \\operatorname{var}\\left(\\bar{X}_T - \\bar{X}_C\\right)\\left(1-\\rho^2\\right)\\\\ &amp; = \\frac{2\\sigma^2}{N}\\left(1-\\rho^2\\right). \\end{aligned} \\] Notice that unlike our first estimator that used baseline values, in Section 4.2, the variance of the ANCOVA estimate can never exceed \\(\\frac{2\\sigma^2}{N}\\); if the baseline and outcome are uncorrelated, ANCOVA will perform as well as a \\(t\\)-test. 4.3.2 (Lecture 9) The practice In the previous section we established an unbiased estimate of the treatment effect that takes into account the baseline measurements. We can’t use this estimator as a model, because: \\(\\hat\\tau\\) relies on the \\(\\rho\\), which is unknown In real life, the groups are unlikely to have equal size and variance, so ideally we’d lose these constraints We can solve both of these by fitting the following statistical model to the observed outcomes \\(x_i\\): \\[ \\begin{aligned} x_i &amp; = \\mu + \\gamma b_i + \\epsilon_i &amp; \\text{ in group C}\\\\ x_i &amp; = \\mu + \\tau + \\gamma b_i + \\epsilon_i &amp; \\text{ in group T}&amp;. \\end{aligned} \\] Where \\(\\epsilon_i\\) are independent errors with distribution \\(N\\left(0,\\,\\sigma^2\\right)\\) \\(b_i\\) are the baseline measurements for \\(i=1,\\ldots,N_T+N_C\\), for groups \\(T\\) and \\(C\\) with sizes \\(N_T\\) and \\(N_C\\) respectively. Sometimes this is written instead in the form \\[ x_i = \\mu + \\tau G_i+ \\gamma b_i + \\epsilon_i \\] where \\[ G_i = \\begin{cases} 1 \\;\\text{ if participant }i\\text{ is in Group }T\\\\ 0 \\;\\text{ if participant }i\\text{ is in Group }C \\end{cases} \\] This is a factor variable, which you may remember from Stats Modelling II (if you took it). If \\(G_i=1\\) (ie. participant \\(i\\) is in group \\(T\\)) then \\(\\tau\\) is added. If \\(G_i=0\\) (ie. participant \\(i\\) is in group \\(C\\)) then it isn’t. We have four parameters to estimate: \\(\\mu,\\,\\tau,\\,\\gamma\\) and \\(\\sigma^2\\). For the first three we can use least squares (as you have probably seen for linear regression). Our aim is to minimise the sum of squares \\[S\\left(\\mu,\\, \\tau,\\,\\gamma\\right) = \\sum\\limits_{i\\text{ in }T} \\left(x_i - \\mu - \\tau - \\gamma b_i\\right)^2 + \\sum\\limits_{i\\text{ in }C} \\left(x_i - \\mu - \\gamma b_i\\right)^2.\\] This leads to estimates \\(\\hat{\\mu},\\, \\hat{\\tau}\\) and \\(\\hat{\\gamma}\\). We won’t worry about how this sum is minimised, since we’ll always be using pre-written R functions. The estimates \\(\\hat{\\mu},\\, \\hat{\\tau}\\) and \\(\\hat{\\gamma}\\) are then used to estimate \\(\\sigma^2\\), using \\[\\hat{\\sigma}^2 = \\frac{S\\left(\\hat{\\mu},\\hat{\\tau}, \\hat{\\gamma}\\right)}{N_T + N_C -3}.\\] The general form for this is \\[ \\hat{\\sigma}^2 = \\frac{SSE}{n-p},\\] where \\(SSE\\) is the residual sum of squares \\(n\\) is the number of data points \\(p\\) the number of parameters (apart from \\(\\sigma^2\\)) being estimated. If you want to know why that is, you can find out here (look particularly at page 62), but we will just take it as given! As well as generating a fitted value \\(\\hat{\\tau}\\), we (or rather R!) will also find the standard error of \\(\\hat\\tau\\), and we can use this to generate a confidence interval for the treatment effect \\(\\tau\\). The technique is known as ANCOVA (short for the Analysis of Covariance) Can be implemented in R and many other statistical software packages. Notice that it is really just a linear model (the like of which you have seen many times) with at least one factor variable, and with a particular focus (application-wise) on the coefficient of the treatment group variable. Example 4.5 Let’s now implement ANCOVA on our Captopril data in R. We do this by first fitting a linear model using ‘lm’, with baseline measurement and arm as predictor variables and outcome as the predictand. lm_capt = lm(outcome ~ baseline + arm, data = df_hommel) summary(lm_capt) ## ## Call: ## lm(formula = outcome ~ baseline + arm, data = df_hommel) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.129 -3.445 1.415 2.959 11.076 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 67.5731 19.7577 3.420 0.00456 ** ## baseline 0.4578 0.1328 3.446 0.00434 ** ## armPlacebo 7.1779 2.9636 2.422 0.03079 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.869 on 13 degrees of freedom ## Multiple R-squared: 0.5629, Adjusted R-squared: 0.4957 ## F-statistic: 8.372 on 2 and 13 DF, p-value: 0.004608 The variable ‘arm’ here is being included as a factor variable, so it behaves like \\[ \\text{arm}_i = \\begin{cases} 0 &amp; \\text{ if participant }i\\text{ is assigned Captopril}\\\\ 1 &amp; \\text{ if participant }i\\text{ is assigned Placebo}. \\end{cases} \\] Therefore, for a patient assigned Placebo, a value of 7.1779 is added, as well as the intercept and baseline term. This results in a model with two parallel fitted lines. For our previous methods we have calculated a confidence interval for the treatment effect \\(\\tau\\), and we will do that here too. The second column of the linear model summary (above) gives the standard errors of each estimated parameter, and we see that the standard error of \\(\\hat{\\tau}\\) is 2.9636. Therefore, to construct a 95/% confidence interval for \\(\\hat{\\tau}\\), we use (to 3 decimal places) \\(7.178\\; \\pm\\; t_{0.975;13}\\times{2.964} = \\left(0.775,\\; 13.580\\right).\\) The model has \\(n-p=13\\) degrees of freedom because there are \\(n=16\\) data points and we are estimating \\(p=3\\) parameters. Notice that unlike our previous confidence intervals, this doesn’t contain zero, and so our analysis has enabled us to conclude that there is a significant reduction in blood pressure with Captopril. Also \\(p&lt;0.05\\). However, you can tell from the width of the interval (and the fact that \\(p\\) is still quite close to 0.05) that there is still a lot of uncertainty about \\(\\tau\\). The ‘Residual standard error’ term near the bottom of the linear model summary is the estimate of \\(\\hat{\\sigma}\\), so here we have \\(\\hat{\\sigma}^2 = 5.869^2 = 34.44.\\) As with any fitted model, we should check the residuals. resid_capt = resid(lm_capt) df_hommel$resid= resid_capt ggplot(data = df_hommel, aes(x=baseline, y=resid, col=arm)) + geom_point() + geom_hline(yintercept=0)+ xlab(&quot;Baseline&quot;)+ ylab(&quot;Residual&quot;)+theme_bw() Residuals look OK: No clear patterns Distribution appears to be similar for each treatment group Though, with such a small sample it’s difficult really to assess the fit of the model. 4.4 Some follow-up questions…. This might have raised a few questions, so we will address those now. 4.4.1 Didn’t we say that \\(X_T - X_C\\) was an unbiased estimator of \\(\\tau\\)? In Sections 4.1 and 4.2 we used both \\(\\bar{X}_T - \\bar{X}_C\\) and \\(\\left(\\bar{X}_T - \\bar{B}_T\\right) - \\left(\\bar{X}_C - \\bar{B}_C\\right)\\) as unbiased estimators of \\(\\tau\\). Then, in Section 4.3.1 we showed that \\[ \\begin{aligned} \\operatorname{E}\\left(\\bar{X}_T - \\bar{X}_C\\mid{\\bar{b}_T,\\;\\bar{b}_C}\\right)&amp; = \\tau + \\rho\\left(\\bar{b}_T - \\bar{b}_C\\right)\\\\ \\operatorname{E}\\left[\\left(\\bar{X}_T - \\bar{b}_T\\right) - \\left(\\bar{X}_C - \\bar{b}_C\\right)\\mid{\\bar{b}_T,\\,\\bar{b}_C}\\right] &amp;= \\tau + \\left(\\rho-1\\right)\\left(\\bar{b}_T - \\bar{b}_C\\right), \\end{aligned} \\] that is, neither of these quantities are unbiased estimators of \\(\\tau\\) (except in very specific circumstances). Is this a contradiction? No! The first two estimators were blind to the values of \\(B_T\\) and \\(B_C\\), and used their a priori statistical properties. ANCOVA uses the observed values, which will have exactly those properties. Because of the randomisation procedure, a priori they can be treated the same. However, once we have observed values for the baseline, \\(b_T\\) and \\(b_C\\), they are very unlikely to be exactly the same. They are also (along with all other baseline measurements, often things like age, sex, height etc.) definitely not affected by the trial, since they are taken before any placebo or treatment has been administered, and often even before allocation. However, conditioning on their observed values can reduce the variance of our estimate of \\(\\tau\\), as we have seen. In this sense, the observed baseline means \\(\\bar{b}_T\\) and \\(\\bar{b}_C\\) are known as ancillary statistics: they contain no direct information about the parameter we are interested in (in this case \\(\\tau\\)) *our inferences can be improved by conditioning on their observed values. 4.4.2 What if the lines shouldn’t be parallel? The unequal slopes model In the analysis above, we assumed that the coefficient \\(\\gamma\\) of baseline (the estimate of the correlation between outcome and baseline) is the same in both groups; we have fitted an equal slopes model. It isn’t obvious that this should be the case, and indeed we can test for it. Allowing each group to have a different slope means including an interaction term between baseline and treatment group, \\[ x_i = \\mu + \\tau G_i+ \\gamma b_i + \\lambda b_i G_i + \\epsilon_i . \\] The term \\(\\lambda b_i G_i\\) is 0 if participant \\(i\\) is in group \\(C\\) and \\(\\lambda b_i\\) if participant \\(i\\) is in group \\(T\\). Therefore, for participants in group \\(C\\), the gradient is still \\(\\gamma\\), but for participants in group \\(T\\) it is now \\(\\gamma + \\lambda\\). We can test whether this interaction term should be included (that is, whether we should fit an unequal slopes model) by including it in a model and analysing the results. Example 4.6 Continuing once again with the Captopril dataset, we now fit the model lm_capt_int = lm(outcome ~ arm + baseline + baseline:arm, data = df_hommel) summary(lm_capt_int) ## ## Call: ## lm(formula = outcome ~ arm + baseline + baseline:arm, data = df_hommel) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.094 -3.475 1.412 2.979 11.145 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 66.85150 28.02488 2.385 0.0344 * ## armPlacebo 8.72484 40.93465 0.213 0.8348 ## baseline 0.46272 0.18886 2.450 0.0306 * ## armPlacebo:baseline -0.01051 0.27723 -0.038 0.9704 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.108 on 12 degrees of freedom ## Multiple R-squared: 0.563, Adjusted R-squared: 0.4537 ## F-statistic: 5.153 on 3 and 12 DF, p-value: 0.01614 We see that the \\(p\\)-value for the coefficient \\(\\lambda\\) (seen in the arm:baseline row) is not at all significant (0.97). Therefore we can be confident that there is no need to fit unequal slopes for this dataset. This fits with our earlier conclusion (from inspecting the residuals) that just including first order terms is fine. 4.4.3 Can we include any other baseline covariates? In Section 4.2 when our estimated treatment effect was \\(\\hat\\tau = \\left(\\bar{x}_T - \\bar{b}_T\\right) - \\left(\\bar{x}_C - \\bar{b}_C\\right)\\), the only other variable we could take into account was the baseline measurement, because it is on the same scale as the outcome \\(X\\). However, in ANCOVA, our treatment effect is \\[ \\hat\\tau = \\left(\\bar{x}_T - \\bar{x}_C\\right) - \\hat\\gamma\\left(\\bar{b}_T - \\bar{b}_C\\right), \\] and the inclusion of the coefficient \\(\\gamma\\) means that we can include other covariates on different scales too. Must be baseline measurements Any covariate used in allocation should be included in analysis Not to be confused with ‘the baseline’, which would generally mean the same measurement as the primary outcome, but before treatment). This is because they cannot, at that point, have been affected by the treatment, or have had an influence on the post-trial outcome measurement. Example 4.7 In this study, 60 patients take part in a trial investigating the effect of a new treatment and exercise on their stress score, after adjusting for age. Two treatment levels: yes or no Three exercise levels: low, moderate and high 10 participants for each combination of treatment and exercise levels. Because in ANCOVA we fit a coefficient to every covariate, we can include exercise (another factor variable) and age (a continuous variable) in this analysis. The table below shows the mean and standard deviation of age for each combination of treatment and exercise level. If we were being picky / thorough, we might note that (perhaps unsurprisingly!) the mean and standard deviation of age are both lower in the high exercise groups. This might well affect our analysis, but we won’t go into this now. treatment exercise mean sd yes low 61.7 4.691600 yes moderate 59.6 2.590581 yes high 57.0 2.211083 no low 62.1 4.332051 no moderate 61.4 5.947922 no high 57.9 3.381321 Fitting a linear model, we see that treatment, high levels of exercise and age have an effect on stress. lm_stresslin = lm(score ~ treatment + exercise + age, data = stress) summary(lm_stresslin) ## ## Call: ## lm(formula = score ~ treatment + exercise + age, data = stress) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.0261 -3.7497 -0.4285 3.0943 13.3696 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 55.72934 10.91888 5.104 4.27e-06 *** ## treatmentno 4.32529 1.37744 3.140 0.00272 ** ## exercisemoderate 0.08735 1.69032 0.052 0.95897 ## exercisehigh -9.61841 1.84741 -5.206 2.96e-06 *** ## age 0.49811 0.17648 2.822 0.00662 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.288 on 55 degrees of freedom ## Multiple R-squared: 0.6045, Adjusted R-squared: 0.5757 ## F-statistic: 21.01 on 4 and 55 DF, p-value: 1.473e-10 In particular, taking a high level of exercise reduced participants’ stress scores by around 9.6, and the treatment reduced stress scores by around 4.3. Participants’ stress scores increased slightly with age (just under half a point per year!). We can plot the residuals to check that the model is a reasonable fit And these look reasonably OK. We could also test for interactions, firstly across all factors: ## ## Call: ## lm(formula = score ~ (treatment + exercise + age):(treatment + ## exercise + age), data = stress) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.5637 -3.3982 0.4173 2.3827 10.3907 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 61.25416 19.86949 3.083 0.00333 ** ## treatmentno 3.89781 24.16324 0.161 0.87250 ## exercisemoderate -14.60897 24.31690 -0.601 0.55070 ## exercisehigh -12.03441 29.53812 -0.407 0.68544 ## age 0.43121 0.32097 1.343 0.18518 ## treatmentno:exercisemoderate -0.20723 3.35949 -0.062 0.95106 ## treatmentno:exercisehigh 8.12783 3.72077 2.184 0.03365 * ## treatmentno:age -0.03769 0.38851 -0.097 0.92311 ## exercisemoderate:age 0.24286 0.40215 0.604 0.54864 ## exercisehigh:age -0.03524 0.50722 -0.069 0.94488 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.106 on 50 degrees of freedom ## Multiple R-squared: 0.6647, Adjusted R-squared: 0.6043 ## F-statistic: 11.01 on 9 and 50 DF, p-value: 3.181e-09 and then restricted to the interactions that seem important: ## ## Call: ## lm(formula = score ~ treatment + exercise + age + treatment:exercise, ## data = stress) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.3250 -3.0192 0.2745 2.4650 10.6667 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 56.79090 10.41383 5.453 1.32e-06 *** ## treatmentno 1.52858 2.23026 0.685 0.4961 ## exercisemoderate 0.01746 2.25662 0.008 0.9939 ## exercisehigh -13.70331 2.36314 -5.799 3.78e-07 *** ## age 0.50355 0.16684 3.018 0.0039 ** ## treatmentno:exercisemoderate 0.15503 3.16129 0.049 0.9611 ## treatmentno:exercisehigh 8.21822 3.15375 2.606 0.0119 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.985 on 53 degrees of freedom ## Multiple R-squared: 0.6613, Adjusted R-squared: 0.623 ## F-statistic: 17.25 on 6 and 53 DF, p-value: 6.167e-11 Notice that now, the effect of the treatment on its own is not significant. Also notice that for both the linear exercise terms and the interactions between the exercise and treatment, the effects of moderate and low exercise are very similar. Combining the coefficients, someone who does a high level of exercise: is likely to reduce their stress score by 13.7 if they receive the treatment is likely to reduce their stress score by \\(13.7 - 8.2 = 5.5\\) if they don’t receive the treatment Returning to our initial look at the dataset, the fact that age is a factor, and high levels of exercise are clearly very important should worry us slightly, since there are very few older people doing high levels of exercise. This may mean our model is inaccurate. An important caution! As you’ll have seen if you read Kendall (2003) (for formative assignment 1), we should have everything in place, including a statistical analysis plan, before the trial. We should already know which covariates we plan to include in our model, and how. ‘Trawling’ for the best possible model by trying lots of different things (and inevitably settling on the one that leads to the most significant conclusion) is poor practice, and can increase the type I error rate (\\(\\alpha\\)). I realise that is sort of what we’ve done in this Section on Analysis, but that was to demonstrate and compare the different methods. Proceeding in the way we have, trying lots of different models, when analysing and writing up a trial would be very poor practice! There’s another excellent episode of the JAMA Evidence podcast, with a focus on adjusting for covariates, that talks about this issue (you can find it here and linked from Ultra). That draws to a close our work with continuous outcome variables. In the next lecture, we’ll start thinking about binary outcome variables. References Hommel, EHEBMJ, Hans-Henrik Parving, Elisabeth Mathiesen, Berit Edsberg, M Damkjaer Nielsen, and Jørn Giese. 1986. “Effect of Captopril on Kidney Function in Insulin-Dependent Diabetic Patients with Nephropathy.” Br Med J (Clin Res Ed) 293 (6545): 467–70. Kendall, John. 2003. “Designing a Research Project: Randomised Controlled Trials and Their Principles.” Emergency Medicine Journal: EMJ 20 (2): 164. "],["ss-bin.html", "5 (Lecture 10) Sample size for a binary variable 5.1 The Delta Method 5.2 A sample size formula", " 5 (Lecture 10) Sample size for a binary variable So far almost everything we’ve covered has related to continuous outcome variables, which we assumed to be normally distributed. This allowed us to use familiar techniques such as the \\(t\\)-test, and to take baseline information into account in an accessible way (the linear model / ANCOVA). However, very often clinical trials do not have a continuous, normally distributed output, and in the next two sections we will look at two other common possibilities: binary data (this section) and survival data (next section). Binary outcome: ‘the patient was alive 2 years after the procedure’ or not ‘the patient was clear of eczema within a month’ or not. Often coded as ‘success’ or ‘failure’, or 1 or 0. For a trial whose primary outcome variables are binary, the sample size calculations we derived in Chapter 2 will not work, so in this section we’ll work through a similar method developed for binary variables. Suppose we conduct a trial with a binary primary outcome variable and two groups, \\(T\\) and \\(C\\), containing \\(n_T\\) and \\(n_C\\) participants respectively. The number of successes in each group, \\(R_T\\) and \\(R_C\\), will be Binomially distributed, \\[\\begin{align*} R_T &amp;\\sim{Bi\\left(n_T,\\, \\pi_t\\right)} \\\\ R_C &amp;\\sim{Bi\\left(n_C,\\,\\pi_C\\right)}. \\end{align*}\\] Now we have \\[ \\begin{aligned} H_0:&amp;\\;\\pi_T = \\pi_C\\\\ H_1:&amp;\\;\\pi_T\\neq{\\pi_C}. \\end{aligned} \\] We will need enough participants to test this hypothesis with sufficient power. With the trial data we find estimates \\[\\begin{align*} p_T &amp; = \\frac{R_T}{n_T} \\\\ p_C &amp; = \\frac{R_C}{n_C}. \\end{align*}\\] Recall that \\[\\operatorname{var}\\left(p_X\\right)=\\pi_X\\left(1-\\pi_X\\right),\\] where \\(X\\) is \\(T\\) or \\(C\\). Notice, the variance depends on the mean \\(\\rightarrow\\) no free parameter. This means there is no free parameter equivalent to \\(\\sigma\\) in the binary situation, and the number of participants required will depend on the approximate value of \\(\\pi_T\\) and \\(\\pi_C\\). This makes the derivation of a sample size formula somewhat more complicated, and so we first of all make a transformation to remove the dependence of mean and variance. To do this we use an approximation technique called the delta method. 5.1 The Delta Method Suppose a random variable \\(X\\) has mean \\(\\mu\\) and variance \\(\\sigma^2 = \\sigma^2\\left(\\mu\\right)\\), ie. its variance depends on its mean. If we have a ‘well-behaved’ (infinitely differentiable etc.) function \\(f\\left(X\\right)\\), what are its mean and variance? To find this exactly requires us to evaluate a sum or integral, and this may be analytically intractable, so we use instead a crude approximation. First, we expand \\(f\\left(X\\right)\\) in a first-order Taylor series about \\(\\mu\\), which gives us \\[\\begin{equation} f\\left(X\\right) \\approx f\\left(\\mu\\right) + \\left(X-\\mu\\right)f&#39;\\left(\\mu\\right) \\tag{5.1} \\end{equation}\\] and therefore \\[\\begin{equation} \\left(f\\left(X\\right) - f\\left(\\mu\\right)\\right)^2 \\approx \\left(X-\\mu\\right)^2\\left[f&#39;\\left(\\mu\\right)\\right]^2. \\tag{5.2} \\end{equation}\\] Taking expectations of Equation (5.1) we find \\(E\\left(f\\left(X\\right)\\right) \\approx f\\left(\\mu\\right)\\). Use this in the LHS of Equation (5.2) so that when we take expectations of Equation (5.2) we find \\[\\begin{equation} \\operatorname{var}\\left(f\\left(X\\right)\\right) = \\sigma^2\\left(\\mu\\right)\\left[f&#39;\\left(\\mu\\right)\\right]^2, \\tag{5.3} \\end{equation}\\] where both sides come from \\[\\operatorname{var}\\left(X\\right) = \\operatorname{E}\\left[\\left(X - \\mu\\right)^2\\right].\\] This series of approximations, which generally works well, is the Delta method. We use the Delta method to find a transformation \\(f\\left(X\\right)\\) for which (at least approximately) the variance is unrelated to the mean. To do this, we solve the differential equation \\[ \\operatorname{var}\\left[f\\left(X\\right)\\right] = \\sigma^2\\left(\\mu\\right) \\left[f&#39;\\left(\\mu\\right)\\right]^2 = \\text{constant}. \\] In the case of proportions for a binary variable, this becomes \\[ \\frac{\\pi\\left(1-\\pi\\right)}{n} \\left[f&#39;\\left(\\pi\\right)\\right]^2 = K\\] for some constant \\(K\\). We can rearrange this to \\[f&#39;\\left(\\pi\\right) = \\sqrt{\\frac{Kn}{\\pi\\left(1-\\pi\\right)} } \\propto \\sqrt{\\frac{1}{\\pi\\left(1-\\pi\\right)}}.\\] So we need \\[\\int^\\pi \\sqrt{\\frac{1}{u\\left(1-u\\right)}}du, \\] where the notation indicates that we want the anti-derivative, evaluated at \\(\\pi\\). By substituting \\(u = w^2\\) we find \\[\\begin{align*} f\\left(\\pi\\right) &amp; \\propto \\int^\\pi{\\frac{1}{\\sqrt{w^2\\left(1-w^2\\right)}}2w\\,dw}\\\\ &amp;\\propto \\int{\\frac{1}{\\sqrt{1 - w^2}}}dw\\\\ &amp; \\propto \\arcsin{\\left(\\sqrt{\\pi}\\right)}. \\end{align*}\\] Setting \\(f\\left(\\pi\\right) = \\arcsin\\left(\\sqrt{\\pi}\\right)\\) and using the chain rule, we find \\[\\left[f&#39;\\left(\\pi\\right)\\right]^2 = \\frac{1}{4\\pi\\left(1-\\pi\\right)} .\\] Finally, we can substitute this into Equation \\@ref(eq:delta3), with $f\\left(X\\right) = \\arcsin\\left(\\sqrt{X}\\right)$ to find \\[\\begin{align*} \\operatorname{var}\\left[f\\left(X\\right)\\right] &amp; \\approx \\sigma^2\\left(\\pi\\right)\\left[f&#39;\\left(\\pi\\right)\\right]^2 \\\\ &amp; \\approx{\\frac{\\pi\\left(1-\\pi\\right)}{n}\\cdot\\frac{1}{4\\pi\\left(1-\\pi\\right)}}\\\\ &amp; \\approx{\\frac{1}{4n}}, \\end{align*}\\] and we have achieved our aim of finding a transformation of \\(X\\) whose variance is not related to the mean. This is sometimes called the angular transformation. 5.2 A sample size formula Our estimate \\(p_X\\) (the proportion of successes in group \\(X\\)) is approximately normally distributed (by CLT). Not true for \\(n\\leq{30}\\) ish (very small for a clinical trial) or for \\(\\pi\\) close to 0 or 1, say \\(\\pi&lt;0.15\\) or \\(\\pi&gt;0.85\\) (this is more likely to be an issue for some trials). The linear approximation in Equation (5.1) shows us that if \\(p_X\\) is normally distributed then \\(f\\left(p_X\\right) = \\arcsin\\left(\\sqrt{p_X}\\right)\\) will be [approximately] normally distributed too. In fact, \\(\\arcsin\\left(\\sqrt{p_X}\\right)\\) is approximately normally distributed with mean \\(\\arcsin{\\left(\\sqrt{\\pi_X}\\right)}\\) and variance \\(1/\\left(4\\pi_X\\right)\\). Using this information, we can test \\(H_0:\\,\\pi_T =\\pi_C\\) at the 100\\(\\alpha\\)% confidence level by using the variable \\[ D = \\frac{\\arcsin{\\left(\\sqrt{p_T}\\right)} - \\arcsin{\\left(\\sqrt{p_C}\\right)}}{\\sqrt{\\frac{1}{4n_T} + \\frac{1}{4n_C}}}= \\frac{\\arcsin{\\left(\\sqrt{p_T}\\right)} - \\arcsin{\\left(\\sqrt{p_C}\\right)}}{\\frac{1}{2}\\lambda\\left(n_T,n_C\\right)}, \\] which is analogous to the variable \\(D\\) constructed in Section 2.3; the difference in \\(f\\left(p_T\\right)\\) and \\(f\\left(p_C\\right)\\) divided by the standard error of the difference. Using the same logic as in Sections 2.4 and 2.5, the starting place for a sample size formula to achieve significance level \\(\\alpha\\) and power \\(\\beta\\) is \\[ \\frac{2\\left(\\arcsin{\\left(\\sqrt{\\pi_T}\\right)} - \\arcsin{\\left(\\sqrt{\\pi_C}\\right)}\\right)}{\\lambda\\left(n_T,n_C\\right)} = z_\\beta + z_{\\frac{\\alpha}{2}}. \\] For two groups of equal size \\(N\\), this leads us to \\[\\begin{equation} N = \\frac{\\left(z_\\beta + z_{\\frac{\\alpha}{2}}\\right)^2}{2\\left(\\arcsin{\\left(\\sqrt{\\pi_T}\\right)} - \\arcsin{\\left(\\sqrt{\\pi_C}\\right)}\\right)^2}. \\tag{5.4} \\end{equation}\\] Because \\(\\arcsin{\\left(\\sqrt{\\pi_T}\\right)} - \\arcsin{\\left(\\sqrt{\\pi_C}\\right)}\\) is not a function of \\(\\pi_T - \\pi_C\\), we cannot express this in terms of the difference itself, but instead need to specify the expected probabilities of success in each group. In practice, it is likely that \\(\\left(\\pi_C\\right)\\) is well understood, and \\(\\left(\\pi_T\\right)\\) can be specified by using the nearest clinically important value of \\(\\pi_T\\). Example 5.1 (From Smith et al. 1994) Two approaches to managing malignent low bile duct obstruction: surgical biliary bypass endoscopic insertion of a stent. Primary outcome: ‘Did the patient die within 30d of the procedure?’ Trial designed to have \\(\\alpha=0.05,\\,\\beta=0.95\\), which gives \\(z_{\\frac{\\alpha}{2}}=1.96,\\,z_{\\beta} = 1.65\\). The trial wanted to be able to determine a change in 30 day mortality rate from 0.2 to at most 0.05. Equation (5.4)) gives \\[ N = \\frac{\\left(1.65 + 1.96\\right)^2}{2\\left(\\arcsin{\\left(\\sqrt{0.2}\\right)} - \\arcsin{\\left(\\sqrt{0.05}\\right)}\\right)^2} = 114.9, \\] and so each group in our trial should contain 115 patients. If instead our aim had been to detect a change from around 0.5 to 0.35 (the same in terms of \\(\\pi_T - \\pi_C\\)) then \\[ N = \\frac{\\left(1.65 + 1.96\\right)^2}{2\\left(\\arcsin{\\left(\\sqrt{0.5}\\right)} - \\arcsin{\\left(\\sqrt{0.35}\\right)}\\right)^2} = 280.8 ,\\] that is, 281 patients per trial arm. Later in the course we’ll look at another way of estimating sample size. References Smith, AC, JF Dowsett, RCG Russell, ARW Hatfield, and PB Cotton. 1994. “Randomised Trial of Endoscopic Steriting Versus Surgical Bypass in Malignant Low Bileduct Obstruction.” The Lancet 344 (8938): 1655–60. "],["lecture-11-analysis-for-binary-outcomes.html", "6 (Lecture 11) Analysis for binary outcomes 6.1 Point estimates and Hypothesis tests 6.2 (Lecture 12) Measures of difference for binary data 6.3 Accounting for baseline observations: logistic regression 6.4 Diagnostics for logistic regression", " 6 (Lecture 11) Analysis for binary outcomes For a group of \\(n\\) participants, we will have allocated \\(n_C\\) to the control group (group \\(C\\)), and \\(n_T\\) to the treatment group (group \\(T\\)). Natural to model number of ‘successes’ \\(R_C\\) by \\[R_C \\sim \\operatorname{Bi}\\left(n_C,\\,\\pi_C\\right).\\] Similarly the number of successes in the treatment group can be modelled as \\[R_T \\sim\\operatorname{Bi}\\left(n_T,\\,\\pi_T\\right),\\] and the focus of our analysis is on comparing \\(\\pi_C\\) and \\(\\pi_T\\). To do this we need point estimates of \\(\\pi_C\\) and \\(\\pi_T\\) interval estimates for some measure of the discrepancy between them ways to test \\(H_0:\\;\\pi_C = \\pi_T.\\) 6.1 Point estimates and Hypothesis tests First of all, we can tabulate the results of a trial with a binary outcome like this: Successes Failures Total Treatment \\(r_T\\) \\(n_T-r_T\\) \\(n_T\\) Control \\(r_C\\) \\(n_C-r_C\\) \\(n_C\\) Total \\(r\\) \\(n - r\\) \\(n\\) Note that because this is a table of observed values, they are now all in lower case. We can estimate \\(\\pi_C\\) and \\(\\pi_T\\) by the sample proportions \\[ \\begin{aligned} p_C &amp;= \\frac{r_C}{n_C}\\\\ p_T &amp;= \\frac{r_T}{n_T} \\end{aligned}. \\] We know that \\[\\operatorname{E}\\left(p_C\\right) = \\pi_C\\] and \\[\\operatorname{Var}\\left(p_C\\right) = \\frac{\\pi_C\\left(1-\\pi_C\\right)}{n_C},\\] and similarly for \\(\\operatorname{E}\\left(p_T\\right)\\) and \\(\\operatorname{Var}\\left(p_T\\right)\\). If \\(Y_{iC}\\) is the outcome of the \\(i\\)-th patient in group \\(C\\), with \\(Y_{iC}=1\\) if the participant’s outcome is ‘success’ \\(Y_{iC}=0\\) otherwise. Then we have \\[r_C = \\sum\\limits_{i=1}^{n_C} y_{iC},\\] and similarly for group \\(T\\). Since \\(p_C\\) and \\(p_T\\) are therefore sample means, the Central Limit Theorem \\(\\implies\\) \\(p_C\\) and \\(p_P\\) can be approximated by normal distributions: \\[ \\begin{aligned} p_C &amp; \\sim N\\left(\\pi_C,\\, \\frac{\\pi_C\\left(1-\\pi_c\\right)}{n_C}\\right)\\\\ p_T &amp; \\sim N\\left(\\pi_T,\\, \\frac{\\pi_T\\left(1-\\pi_T\\right)}{n_T}\\right). \\end{aligned} \\] This means we can test the null hypothesis that \\(\\pi_C = \\pi_T\\) by referring our observed value of \\(p_T - p_C\\) to a normal distribution with mean 0 and variance \\[ \\frac{\\pi_T\\left(1-\\pi_T\\right)}{n_T} + \\frac{\\pi_C\\left(1-\\pi_c\\right)}{n_C},\\] which we can approximate by substituting in \\(p_C\\) and \\(p_T\\). Under \\(H_0\\), \\(\\pi_C = \\pi_T = \\pi\\), so it would be more appropriate to use this as the common variance. The variance of \\(p_T - p_C\\) becomes \\[\\pi\\left(1-\\pi\\right)\\left(\\frac{1}{n_C} + \\frac{1}{n_T}\\right), \\] and in calculations we replace \\(\\pi\\) with \\(p = r/n\\). Putting all this together, our test statistic is \\[Z = \\frac{p_T - p_C}{\\sqrt{p\\left(1-p\\right)\\left(\\frac{1}{n_T} + \\frac{1}{n_C}\\right)}}.\\] Example 6.1 From al (1948). 109 patients with tuberculosis Assigned to either receive Streptomycin (group \\(T\\)), or placebo (group \\(C\\)) Primary outcome variable is whether or not the patient was improved after the treatment period. The data include several other covariates, including gender, baseline condition (good, fair or poor) and whether the patient had developed resistance to streptomycin after 6 months. ## improved ## arm FALSE TRUE ## Streptomycin 17 38 ## Control 35 17 We therefore have \\[ \\begin{aligned} n_C &amp; = 52 \\\\ n_T &amp; = 55 \\\\ p_C &amp; = \\frac{17}{17+35} &amp; = 0.327\\\\ p_T &amp; = \\frac{38}{38+17} &amp; = 0.691\\\\ p &amp; = \\frac{38+17}{107} &amp;= 0.514. \\end{aligned} \\] and can calculate our \\(Z\\) statistic to be \\[ \\begin{aligned} Z &amp; = \\frac{0.691 - 0.327}{\\sqrt{0.514\\left(1-0.514\\right)\\left(\\frac{1}{52} + \\frac{1}{55}\\right)}}\\\\ &amp; = 3.765. \\end{aligned} \\] Finally, we can find the \\(p\\)-value of this test statistic (making sure to have two tails!) 2*(1-pnorm(3.765, mean=0, sd=1)) ## [1] 0.0001665491 So we can reject the hypothesis that streptomycin has no effect on tuberculosis at the \\(\\alpha=0.05\\) level (and indeed many lower levels). 6.1.1 An alternative approach: chi-squared Another way to approach this would be to conduct a chi-squared test. Calculate the expected values \\(\\left(E_i\\right)\\) for each box of the summary table Compare them to the observed values \\(\\left(O_i\\right)\\) by finding the summary statistic \\[ X^2 = \\sum \\frac{\\left(o_i - e_i\\right)^2}{e_i}.\\] Under \\(H_0:\\; \\pi_C = \\pi_T\\), the test statistic \\(X^2 \\sim \\chi^2_1\\). We see that the larger the differences between the observed and expected values, relative to the expected values, the larger the test statistic, and therefore the less probably under the \\(\\chi^2_1\\) distribution. Example 6.2 Continuing our streptomycin example, we can calculate a table of expected values by observing that proportion \\(p=0.514\\) of the total number of patients were improved. There are 52 in the control group, therefore we expect \\(0.514\\times 52 = 26.73\\) improved patients in the control group, and by the same logic \\(0.514\\times 55 = 28.27\\) in the treatment group. Our expected table is therefore ## improved ## arm FALSE TRUE ## Streptomycin 26.730 28.270 ## Control 25.272 26.728 We can therefore calculate the \\(\\chi^2\\) statistic by looping through the elements of the tables: sum_chi_sq = 0 # set a running total going # in the following, tab_obs is the table of observed values and # tab_exp is the table of expected values for (i in 1:2){ for (j in 1:2){ tmp = ((tab_obs[i,j] - tab_exp[i,j])^2)/tab_exp[i,j] sum_chi_sq = sum_chi_sq + tmp } } sum_chi_sq ## [1] 14.17595 1-pchisq(sum_chi_sq, df=1) ## [1] 0.0001664847 and again we have a very significant result. In fact, these two tests are almost equivalent, and we have that \\(\\sqrt{X^2} = Z\\): sqrt(sum_chi_sq) ## [1] 3.765097 6.1.2 (Lecture 11) Likelihood: A more rigorous way Our method above was quite informal, and also made heavy use of the central limit theorem. We can use maximum likelhood to derive a more formally justified test for binary outcomes. This also lays a good foundation for more complex situations. Using same notation \\(y_{iC}\\) to be outcome variable (0 or 1, in this case) of the \\(i\\)-th participant in the control group (and so on). The contribution of the \\(i\\)-th patient in group \\(C\\) to the likelihood is \\[\\pi_C^{y_{iC}}\\left(1 - \\pi_C\\right)^{y_{iC}} \\] (remember we can ignore multiplicative constant terms). Combining all \\(n_C\\) patients in group \\(C\\), their contribution will be \\[ \\pi_C^{r_C}\\left(1-\\pi_C\\right)^{n_C - r_C},\\] where \\(r_C\\) is the number of ‘successes’ in group \\(C\\). Similarly for the treatment group we will have \\[ \\pi_T^{r_T}\\left(1-\\pi_T\\right)^{n_T - r_T}.\\] Wwe can find the complete likelihood function \\[ \\begin{aligned} L\\left(\\pi_C,\\pi_T \\mid \\left\\lbrace y_{iC}\\right\\rbrace, \\left\\lbrace y_{iT}\\right\\rbrace \\right) &amp; L\\left( \\pi_C,\\pi_T \\mid {n_C,n_T, r_C, r_T}\\right)\\\\ &amp; = \\pi_C^{r_C}\\left(1-\\pi_C\\right)^{n_C - r_C}\\pi_T^{r_T}\\left(1-\\pi_T\\right)^{n_T - r_T}. \\end{aligned} \\] The log-likelihood is therefore \\[ l\\left( \\pi_C,\\pi_T \\mid {n_C,n_T, r_C, r_T}\\right) = r_C\\log\\pi_C + \\left(n_C-r_C\\right)\\log\\left(1-\\pi_C\\right) + r_T\\log\\pi_T + \\left(n_T-r_T\\right)\\log\\left(1-\\pi_T\\right).\\] If we differentiate with respect to \\(\\pi_C\\), we find \\[\\frac{\\mathrm{d} l\\left( \\pi_C,\\pi_T \\mid {n_C,n_T, r_C, r_T}\\right)}{\\mathrm{d}\\pi_C} = \\frac{r_C}{\\pi_C} - \\frac{n_C-r_C}{1-\\pi_C}.\\] Setting this to zero we find (reassuringly!) that \\(\\hat\\pi_C = \\frac{r_C}{n_C}\\). We can repeat this exercise for \\(\\pi_T\\). If we assume that there is one common probability \\(\\pi\\) of success, we can find \\(\\hat\\pi\\) by maximising \\(l\\left(\\pi,\\pi \\mid {n_C,n_T, r_C, r_T}\\right)\\) with respect to \\(\\pi\\), and again this works out to be \\(\\frac{r_{C} + r_T}{n}\\) as before. We can use these to construct a likelihood ratio test, by calculating \\[ \\begin{aligned} \\lambda_{LR} = &amp; -2\\left[l\\left( \\hat\\pi,\\hat\\pi \\mid {n_C,n_T, r_C, r_T}\\right) - l\\left( \\hat\\pi_C,\\hat\\pi_T \\mid {n_C,n_T, r_C, r_T}\\right)\\right]\\\\ = &amp; 2\\left[\\underbrace{r_C\\log\\frac{r_C}{n_C} + \\left(n_C-r_C\\right)\\log\\left(1-\\frac{r_C}{n_C}\\right) + r_T\\log\\frac{r_T}{n_T} + \\left(n_T-r_T\\right)\\log\\left(1-\\frac{r_T}{n_T}\\right) }_{l\\left( \\hat\\pi_C,\\hat\\pi_T \\mid {n_C,n_T, r_C, r_T}\\right)} \\right. \\\\ &amp;\\;\\;\\;\\;\\;\\; \\left. - \\underbrace{\\Big(r\\log\\left(p\\right) + \\left(n-r\\right)\\log\\left(1-p\\right)\\Big)}_{l\\left( \\hat\\pi,\\hat\\pi \\mid {n_C,n_T, r_C, r_T}\\right)}\\right]\\\\ =&amp; 2\\left[\\underbrace{r_C \\log\\left(\\frac{r_C}{n_C p}\\right)}_{\\text{Group }C\\text{ success}} + \\underbrace{\\left(n_C - r_C\\right)\\log\\left(\\frac{n_C - r_C}{n_C\\left(1-p\\right)}\\right)}_{\\text{Group }C\\text{ fail}} \\right.\\\\ &amp; \\;\\;\\;\\;\\;\\; \\left.+ \\underbrace{r_T \\log\\left(\\frac{r_T}{n_T p}\\right)}_{\\text{Group }T\\text{ success}} + \\underbrace{\\left(n_T - r_T\\right)\\log\\left(\\frac{n_T - r_T}{n_T\\left(1-p\\right)}\\right)}_{\\text{Group }T\\text{ fail}}\\right] \\end{aligned} \\] where we use \\(p,\\, r,\\, n\\) to denote the pooled values (\\(n = n_C + n_T\\) etc.). Each term in the final line corresponds to a subgroup of the participants, as labelled, and if we rearrange them slightly we see that This can be re-written as \\[\\lambda_{LR} = 2 \\sum\\limits_{i\\in G} o_i \\log\\left(\\frac{o_i}{e_i}\\right),\\] where \\(G\\) is the set of subgroups (group \\(C\\) success etc.). Under the null hypothesis that \\(\\pi_C = \\pi_T = \\pi\\), and for sufficiently large \\(n_C,\\;n_T\\), \\(\\lambda_{LR}\\) has a \\(\\chi^2\\) distribution with one degree of freedom. Example 6.3 Continuing with the streptomycin example, we can calculate this new test statistic in R by looping through the subgroups. sum_LR = 0 # set a running total going # in the following, tab_obs is the table of observed values and # tab_exp is the table of expected values for (i in 1:2){ for (j in 1:2){ tmp = tab_obs[i,j] * log(tab_obs[i,j]/tab_exp[i,j]) sum_LR = sum_LR + tmp } } teststat_LR = 2*sum_LR teststat_LR ## [1] 14.5028 1-pchisq(teststat_LR, df=1) ## [1] 0.0001399516 Not surprisingly, this value is quite close to the one we obtained earlier! Having thought about tests for one proportion, we now move on to thinking how we might compare proportions. Brief overview of the different measure’s we’ll be looking at: Absolute risk difference (ARD): \\(\\pi_T - \\pi_C\\) Number needed to treat (NNT): \\(\\frac{1}{\\text{ARD}} = \\frac{1}{\\pi_T - \\pi_C}\\) Risk ratio (RR): \\(\\frac{\\pi_T}{\\pi_C}\\) Odds Ratio (OR): \\(\\frac{\\pi_T/ \\left(1-\\pi_T\\right)}{\\pi_C/ \\left(1-\\pi_C\\right)}\\) 6.2 (Lecture 12) Measures of difference for binary data Important note: treating \\(\\pi_T&gt;\\pi_C\\) as good Our question in the last lecture was ‘is what we’ve observed statistically significant?’ For streptomycin example the answer was a resounding ‘Yes!’. However, for questions like ‘How big is the difference between the effects of each treatment?’ ‘What is the treatment effect?’ things are a bit less clear. For continuous \\(X\\), it made sense to think about the treatment effect as \\(\\mu_T - \\mu_C\\). In the binary case there are several ways we can think of the difference between two proportions \\(\\pi_C\\) and \\(\\pi_T\\). Each requires a different approach, so we will work our way through them in the next couple of lectures. 6.2.1 Absolute risk difference and Number Needed to Treat The absolute risk difference is \\[\\text{ARD} = \\pi_T - \\pi_C,\\] and is sometimes used. Loses a lot of information that we’d like to keep in: a change from \\(\\pi_C=0.03\\) to \\(\\pi_T=0.01\\) is very different from \\(\\pi_C=0.57\\) to \\(\\pi_T = 0.55\\). Eg., suppose a treatment reduces the incidence of some terrible symptom from \\(\\pi_C=0.03\\) to \\(\\pi_T=0.01\\). The absolute risk difference is \\(0.02\\) here. For some other treatment that results in a reduction from \\(\\pi_C=0.57\\) to \\(\\pi_T = 0.55\\) we have the same absolute risk difference, even though it feels (and is!) a much less significant reduction. BUT these numbers are (usually) about people. If the outcome is ‘cured’ or ‘not cured’, then for \\(N\\) patients, \\(N\\times\\text{ARD}\\) is the number of extra patients you would expect to cure if you used treatment \\(T\\) instead of treatment \\(C\\) (which may be nothing or may some usual course of treatment). Linked to this is the number needed to treat (NNT), which is defined as \\[ \\text{NNT} = \\frac{1}{\\pi_T - \\pi_C} = \\frac{1}{\\text{ARD}}. \\] The NNT is the number of patients you’d need to treat (with treatment \\(T\\) rather than \\(C\\)) before you would bring benefit to one extra patient. The website TheNNT collects together results from many clinical trials and uses the NNT as a summary. Some of the results are quite surprising, compared to how effective we think medicines are! Popular as a clinical benchmark Provides useful intuition in terms of the number of people it will help Eg. \\(\\pi_T = 0.25,\\,\\pi_C=0.2\\), then \\(\\text{ARD} = 0.05\\) and \\(\\text{NNT} = 20.\\) After treating 20 patients with treatment \\(C\\) we expect to cure (say) 4, whereas treating 20 patients with treatment \\(T\\) it is expected that we will cure 5. For very small proportions, the NNT can be large even for what appears to be an important difference. For example, if \\(\\pi_C=0.005\\) and \\(\\pi_T = 0.015\\) then \\(\\text{ARD}=0.01\\) and \\(\\text{NNT}=100\\). It might be decided that the necessary changes and costs are not worth it for such a small difference. That said, the NNT is not the easiest statistic to work with, as we shall see! 6.2.1.1 Confidence intervals for ARD and NNT Let’s make a confidence interval for the treatment difference \\(\\tau_{ARD} = \\pi_T - \\pi_C\\). Using the same normal approximation as before, we can estimate \\(\\tau_{ARD}\\) by \\(p_T - p_C\\), and \\(\\operatorname{var}\\left(p_T - p_C\\right)\\) by \\[ \\frac{p_T\\left(1-p_T\\right)}{n_T} + \\frac{p_C\\left(1-p_C\\right)}{n_C}.\\] Our \\(100\\left(1-\\alpha\\right)\\)% confidence interval is therefore given by \\[\\left(p_T - p_C - z_{\\frac{\\alpha}{2}}\\sqrt{\\frac{p_T\\left(1-p_T\\right)}{n_T} + \\frac{p_C\\left(1-p_C\\right)}{n_C}},\\; p_T - p_C + z_{\\frac{\\alpha}{2}}\\sqrt{\\frac{p_T\\left(1-p_T\\right)}{n_T} + \\frac{p_C\\left(1-p_C\\right)}{n_C}}\\right) \\] Example 6.4 Back to our streptomycin example, we can now construct a \\(100\\left(1-\\alpha\\right)\\)% confidence interval for the ARD. Our estimated treatment effect is (to 3 decimal places) \\[\\hat\\tau=p_T - p_C = \\frac{38}{55} - \\frac{17}{52} = 0.364.\\] Our estimate of the standard error of \\(\\hat\\tau\\) is \\[ \\begin{aligned} \\frac{p_T\\left(1-p_T\\right)}{n_T} + \\frac{p_C\\left(1-p_C\\right)}{n_C} &amp; = \\frac{\\frac{38}{55}\\times \\frac{17}{55}}{55} + \\frac{\\frac{17}{52}\\times \\frac{35}{52}}{52}\\\\ &amp; = 0.0811 \\end{aligned} \\] and therefore a 95% confidence interval for \\(\\tau_{ARD}\\) is \\[\\left(0.364 - z_{0.975}\\sqrt{0.0811},\\; 0.364 + z_{0.975}\\sqrt{0.0811}\\right) = \\left(0.187,\\; 0.541\\right). \\] As we should expect from the very low \\(p\\)-value we saw, the 95% confidence interval does not contain zero. Our expected value of \\(\\tau_{NNT}\\) is \\[ \\text{NNT} = \\frac{1}{\\tau_{ARD}} = \\frac{1}{0.364} = 2.75.\\] That is, we would expect to treat three patients before one is improved (in terms of their tuberculosis symptoms). We can use the limits of the 95% CI for \\(\\tau_{ARD}\\) to form a 95% CI for NNT: simply by taking the reciprocals of the limits to get \\[\\left(\\frac{1}{0.541},\\; \\frac{1}{0.178}\\right) = \\left(1.85,\\; 5.34 \\right).\\] Because the NNT is the reciprocal of something approximately normally distributed, it has a distribution with a long tail, and we see that the confidence interval is therefore skewed. 6.2.1.2 What if the difference is not significant? In the above section you might have already wondered what happens if the confidence interval for the absolute risk difference (ARD) contains zero. To illustrate this, we will make up some data for a small trial: Successes Failures Total Treatment 9 5 14 Control 4 8 12 Total 13 13 26 The ARD is now \\[\\frac{9}{14} - \\frac{4}{12} = \\frac{3}{13} \\approx 0.310 \\] and our 95% confidence interval for \\(\\tau_{ARD}\\) is \\(\\left(-0.0567,\\;0.676\\right)\\). Our CI is very wide (this is not a very good trial!), and now contains zero. It looks very likely that the treatment is effective (the interval only just contains zero) but how many patients might we need to treat before we expect to see an extra success? The expected value of NNT is \\[ \\frac{1}{0.310} = 3.23,\\] which is fine. However, our CI contains the possibility that \\(\\tau_{ARD}=0\\), in which case the NNT is in some sense infinite: no matter how many patients we treat, we don’t expect to see any extra improvements. Therefore, it feels appropriate that our CI for \\(\\tau_{NNT}\\) should contain infinity. When thinking about a confidence interval for the NNT, we need to think about signs, and what negative and positive values mean. If both the lower and upper limits of the confidence interval for ARD are positive, there is no issue - the treatment is effective, and our NNT confidence interval is another entirely positive interval. If the confidence interval for ARD is entirely negative, we have an entirely negative interval for NNT. A negative value of NNT can be thought of as the ‘number needed to treat to harm one extra person’. The tricky situation is when the CI for \\(\\tau_{ARD}\\) is \\(\\left(-L, U\\right)\\) with \\(L,U&gt;0\\), ie. an interval containing zero. As we approach zero from \\(U\\), the upper limit of the CI for \\(\\pi_T - \\pi_C\\), the number of patients we need to treat increases, since the treatment effect is getting smaller, until at \\(\\pi_T - \\pi_C=0\\) the NNT is infinite. Therefore, the part of the CI for NNT corresponding to the positive part of the CI for ARD is \\[\\left(\\frac{1}{U},\\; \\infty\\right)\\] As we approach zero from the left in the interval (ie. from \\(-L\\)), we need to treat more and more patients to harm one more. In this region the NNT is negative, since if we deny some patients the treatment we will benefit a few. Therefore the CI for \\(\\tau_{NNT}\\) corresponding to the negative part of the CI for \\(\\tau_{ARD}\\) is \\[\\left(-\\infty,\\;-\\frac{1}{L}\\right), \\] and altogether the confidence interval for the number needed to treat (NNT) is the union of these two intervals, \\[\\left(-\\infty,\\;-\\frac{1}{L}\\right) \\cup \\left(\\frac{1}{U},\\; \\infty\\right).\\] The plot below shows relationship between ARD and NNT, with the intervals for our toy example shown in bold on the respective axis (the NNT interval should continue infinitely in both directions so for obvious reasons this is not all shown!). Altman (1998) (available here) makes a compelling push for the use of confidence intervals for the number needed to treat. You can decide for yourself whether what you think of it! Problems with the confidence interval for the ARD The ‘standard’ method is not so reliable if the proportion is close to zero or one. The coverage probability of a 95% CI like in Section 6.2.1.1 often turns out to be more like 90% or even 85%. Also, the limits of the ‘standard’ CI aren’t forced to be in \\(\\left[-1,1\\right]\\). Newcombe Method Step 1: find an interval estimate for a single proportion \\(\\pi\\). As before, this can be written \\[\\left\\lbrace \\pi \\mid \\frac{\\lvert p - \\pi \\rvert}{\\sqrt{\\pi\\left(1-\\pi\\right)/n}} \\leq z_{\\frac{\\alpha}{2}} \\right\\rbrace = \\left\\lbrace \\pi \\mid \\left(p - \\pi\\right)^2 \\leq z^2_{\\frac{\\alpha}{2}}\\frac{\\pi\\left(1-\\pi\\right)}{n} \\right\\rbrace. \\] We find the limits of the \\(100\\left(1-\\alpha\\right)\\)% level CI by changing the right hand side to an equality: \\[\\left(p - \\pi\\right)^2 = z^2_{\\frac{\\alpha}{2}}\\frac{\\pi\\left(1-\\pi\\right)}{n}.\\] ‘standard’ method: Substitute \\(p\\) (the estimated value of \\(\\pi\\) from our sample) into the right hand side of Equation (6.1) for \\(\\pi\\), to get \\[\\begin{equation} \\left(p - \\pi\\right)^2 = z^2_{\\frac{\\alpha}{2}}\\frac{\\pi\\left(1-\\pi\\right)}{n}. \\tag{6.1} \\end{equation}\\] which we solve to get the limits \\[ \\pi = p \\pm z_{\\frac{\\alpha}{2}}\\sqrt{\\frac{p\\left(1-p\\right)}{n}}.\\] Newcombe’s method Keep \\(\\pi\\) in the right hand side and solve the quadratic in Equation (6.1) in terms of \\(\\pi\\). The benefit of this new method will be most obvious for a probability that is close to 0 or 1. Eg. Suppose we have 1 success out of 50 patients, so \\(p=0.02,\\;n=50\\). The limits of a standard 95% confidence interval will be \\[\\left(0.02 - z_{0.975}\\sqrt{\\frac{0.02\\times{0.98}}{50}},\\; 0.02 + z_{0.975}\\sqrt{\\frac{0.02\\times{0.98}}{50}}\\right) = \\left(-0.0188,\\;0.0588\\right),\\] whereas the limits to the Newcombe 95% CI will be the roots of \\[\\left(0.02-\\pi\\right)^2 = z^2_{\\alpha/2}\\frac{\\pi\\left(1-\\pi\\right)}{50}\\] which work out to be ## [1] 0.003539259 0.104954436 Visually, we can represent this as below by plotting the LHS (solid) and RHS (dashed for new method, dotted for standard method). The thick solid red line shows \\(p_T\\), the estimated proportion, the thinner dashed red lines show the Newcombe 95% CI and the dotted red lines show the standard 95% CI. Notice that the limits of each confidence interval are formed by the points at which the solid line (LHS) crosses the dashed / dotted lines (RHS). Example 6.5 Returning to our streptomycin example, our estimate of the probability of success for the treatment group is \\(p_T = \\frac{38}{55},\\;n_T = 55\\), and therefore our equation becomes \\[\\left(\\frac{38}{55} - \\pi\\right)^2 = z^2_{\\frac{\\alpha}{2}}\\frac{\\pi\\left(1-\\pi\\right)}{55}.\\] Solving this equation in the usual way (using the quadratic formula) we find the limits ## [1] 0.5597141 0.7971771 By contrast, in our standard method we have \\[\\left(\\frac{38}{55} - \\pi\\right)^2 = z^2_{\\frac{\\alpha}{2}}\\frac{\\frac{38}{55}\\left(1-\\frac{38}{55}\\right)}{55}\\] which is ## [1] 0.5687797 0.8130385 We can see this graphically Figure 6.1: As before, dashed for Newcombe, dotted for standard Notice that the interval with the new method is now asymmetrical, which is more realistic. Similarly for the control proportion \\(\\pi_C\\), we have \\(p_C = \\frac{17}{52},\\; n_C=52\\), and our Newcombe interval is ## [1] 0.2152207 0.4624381 compared to the standard confidence interval ## [1] 0.1994256 0.4544205 Again, we can see this graphically. Figure 6.2: As before, dashed for Newcombe, dotted for standard 6.2.1.3 Extending this to \\(\\pi_T - \\pi_C\\) What the Newcombe interval has given us is a superior method for creating confidence intervals for proportions. But, what we would like is a method for calculating a confidence interval for the difference in two proportions. You’ll be relieved to hear that there is such a method, and we’ll give a sketch here of how it works. The limits of the ‘standard method’ confidence interval at significance level \\(\\alpha\\) are given by \\[\\begin{equation} \\left(p_T - p_C - z_{\\frac{\\alpha}{2}}\\sqrt{\\frac{p_T\\left(1-p_T\\right)}{n_T} + \\frac{p_C\\left(1-p_C\\right)}{n_C}},\\; p_T - p_C + z_{\\frac{\\alpha}{2}}\\sqrt{\\frac{p_T\\left(1-p_T\\right)}{n_T} + \\frac{p_C\\left(1-p_C\\right)}{n_C}}\\right). \\tag{6.2} \\end{equation}\\] We can rewrite this as \\[\\begin{equation} \\left(p_T - p_C - \\sqrt{\\omega^2_T + \\omega^2_C},\\; p_T - p_C + \\sqrt{\\omega^2_T + \\omega^2_C}\\right) \\end{equation}\\] where \\(\\omega_T\\) and \\(\\omega_C\\) are the widths of the separate single-sample ‘standard’ confidence intervals for \\(p_T\\) and \\(p_C\\). Newcombe’s method: proceed in the same way, but instead use the widths of the Newcombe confidence intervals for the individual probabilities \\(p_T\\) and \\(p_C\\). A bit more complicated, since for a Newcombe \\(\\left(p_X - l_X\\right)\\neq \\left(u_X - p_X\\right)\\) (for \\(X=T\\) or \\(C\\)) So, we have \\[ \\left(p_T - p_C - \\sqrt{\\left(p_T-l_T\\right)^2 + \\left(u_C - p_C\\right)^2},\\; p_T - p_C + \\sqrt{\\left(u_T - p_T\\right)^2 + \\left(p_C - l_C\\right)^2}\\right). \\] These differences must be calculated using the individual sample confidence interval method. Example 6.6 Applying this Newcombe method to our Streptomycin example, recall that we have \\[ \\begin{aligned} p_T &amp; = \\frac{38}{55}\\\\ p_T - l_T &amp; = \\frac{38}{55} - 0.5597 = 0.1312\\\\ u_T - p_T &amp; = 0.7972 - \\frac{38}{55} = 0.1064\\\\ p_C &amp; = \\frac{17}{52} \\\\ p_C - l_C &amp; = \\frac{17}{52} - 0.2152 = 0.1117\\\\ u_C - p_C &amp; = 0.4624 - \\frac{17}{52} = 0.1355. \\end{aligned} \\] Our \\(95\\%\\) confidence interval is therefore \\[ \\begin{aligned} \\left(p_T - p_C - \\sqrt{\\left(p_T-l_T\\right)^2 + \\left(u_C - p_C\\right)^2}\\right.&amp;,\\left. p_T - p_C + \\sqrt{\\left(u_T - p_T\\right)^2 + \\left(p_C - l_C\\right)^2}\\right)\\\\ \\left(\\frac{38}{55}-\\frac{17}{52} - \\sqrt{0.1312^2 + 0.1355^2}\\right.&amp;,\\left.\\frac{38}{55}-\\frac{17}{52} + \\sqrt{0.1064^2 + 0.1117^2}\\right)\\\\ \\left(0.3640 - 0.1886 \\right.&amp;,\\left. 0.3640+ 0.1543\\right)\\\\ \\left(0.157 \\right.&amp;,\\left.0.500\\right). \\end{aligned} \\] This is skewed somewhat lower than our standard CI of \\(\\left(0.187,\\;0.541\\right).\\) 6.2.2 (Lecture 13) Risk Ratio (RR) and Odds ratio (OR) Measure so far, esp ARD, quite analagous to the continuous normally distributed case. However, there are yet more commonly used measures of difference for proportions, which need to be dealt with differently, but also afford more opportunities for modelling. The risk ratio is defined as \\[\\text{RR} = \\frac{\\pi_T}{\\pi_C}\\] The odds ratio is defined as \\[\\text{OR} = \\frac{\\pi_T/\\left(1-\\pi_T\\right)}{\\pi_C/\\left(1-\\pi_C\\right)}\\] Note that for both RR and OR: the null value is one, not zero. Always positive (assuming \\(\\pi_C,\\pi_T\\neq{0,1}\\)) We think about things multiplicatively, so for example if \\(RR=3\\) we can say that the event is “3 times more likely” in group \\(T\\) than in group \\(C\\). Odds Reminder: The odds of some event \\(A\\) are \\[\\frac{p(A)}{1-p(A)} \\] So, if (for some event \\(A\\)), \\(p\\left(A\\right)=0.2\\), the odds of \\(A\\) are \\[\\frac{p\\left(A\\right)}{p\\left(A&#39;\\right)} = \\frac{0.2}{0.8} = \\frac{1}{4}, \\] which we say as “1 to 4” or 1:4. For every one time \\(A\\) occurs, we expect it not to occur four times. The odds ratio compares the odds of the outcome of interest in the Treament group with the odds of that event in the Control group. It tells us how the odds of the event are affected by the treatment (vs control). For probbabilities near zero, RR and OR are quite similar. Example 6.7 For our Streptomycin example, we estimated the ARD by \\[\\hat\\tau_{ARD}=p_T - p_C = \\frac{38}{55} - \\frac{17}{52} = 0.364,\\] or could have alternatively had \\[\\hat\\tau_{ARD}=p_C - p_T = \\frac{17}{52} - \\frac{38}{55} = - 0.364.\\] For the risk ratio, we have \\[\\hat{\\tau}_{RR} = \\frac{p_T}{p_C} = \\frac{38/55}{17/52} = 2.113,\\] or could alternatively have \\[\\hat{\\tau}_{RR} = \\frac{p_C}{p_T} = \\frac{17/52}{38/55} = 0.473 = \\frac{1}{2.113}.\\] We could say that a patient is “more than twice as likely to be cured with streptomycin than by the control”. For the odds ratio, we have \\[\\hat{\\tau}_{OR} = \\frac{p_T/\\left(1-p_T\\right)}{p_C/\\left(1-p_C\\right)} = \\frac{(38/55)/(17/55)}{(17/52)/(35/52)} = 4.602, \\] and therefore the odds of recovery are around 4.6 greater for Streptomycin than for the control. Similarly, we could reframe this as \\[\\hat{\\tau}_{OR} = \\frac{p_C/\\left(1-p_C\\right)}{p_T/\\left(1-p_T\\right)} = \\frac{(17/52)/(35/52)}{(38/55)/(17/55)} = 0.217 = \\frac{1}{4.602}.\\] 6.2.2.1 Confidence intervals for RR and OR Symmetry works differently on the RR and OR scale from on the ARD scale. There is an equivalence between an interval \\(\\left(l,\\,u\\right)\\) (with \\(l,u&gt;1\\)) and \\(\\left(\\frac{1}{u},\\frac{1}{l}\\right)\\), since these intervals would equate to comparing the same two treatments in different directions Similarly, on this scale the interval \\[\\left(\\frac{1}{k},\\,k\\right) \\text{ for some }k&gt;1 \\] can be thought of as symmetric, in that one treatment may be up to \\(k\\) times more effective than the other, in either direction. Therefore, to build a confidence interval for OR or RR, we will not be following the usual formula \\[\\text{point estimate } \\pm{z\\times{SE}}.\\] You may have already been thinking that a log transformation would be useful here, and you’d be correct! The sort-of symmetric intervals we’ve been discussing here actually are symmetric (about zero) on the log scale. With a log transformation, the above interval becomes \\(\\left(-\\log k, \\; \\log k\\right)\\). Firstly we’ll consider the risk ratio. Let’s define \\[ \\phi = \\log\\left(\\frac{\\pi_T}{\\pi_C}\\right).\\] The natural way to estimate this is with the sample proportions \\[\\log\\left(\\frac{p_T}{p_C}\\right) = \\log\\left(p_T\\right) - \\log\\left(p_C\\right).\\] These estimated proportions should be approximately normal and independent of one another, and so \\(\\log\\left(\\frac{p_T}{p_C}\\right)\\) is approximately normal with mean \\(\\phi\\) (the true value) and variance \\[\\operatorname{var}\\left(\\log\\left(p_T\\right)\\right) + \\operatorname{var}\\left(\\log\\left(p_C\\right)\\right). \\] We can now apply the Delta method. Reminder: If RV \\(X\\) has mean \\(\\mu\\) and variance \\(\\sigma^2(\\mu)\\) then \\[var\\left[f(X)\\right]\\approx \\sigma^2(\\mu) \\left[f&#39;(\\mu)\\right]^2.\\] We find \\[\\operatorname{var}\\left[\\log\\left(p_T\\right)\\right] = \\operatorname{var}\\left[\\log\\left(\\frac{r_T}{n_T}\\right)\\right] \\approx \\frac{\\pi_T\\left(1-\\pi_T\\right)}{n_T}\\times{\\left(\\frac{1}{\\pi_T}\\right)^2} = \\frac{1}{n_T\\pi_T} - \\frac{1}{n_T}. \\] Since we estimate \\(\\pi_T\\) by \\(r_T/n_T\\) this can be estimated by \\[\\frac{1}{r_T} - \\frac{1}{n_T}.\\] Notice that we are relying on the derivative of \\(\\log\\left(x\\right)\\) being \\(x^{-1}\\), so we must always use natural logarithms. This leads us to the result that, approximately \\[\\log\\left(\\frac{p_T}{p_C}\\right) \\sim N\\bigg(\\phi,\\,\\left(\\frac{1}{r_T} - \\frac{1}{n_T}\\right) + \\left(\\frac{1}{r_C} - \\frac{1}{n_C}\\right) \\bigg) \\] and so we can generate \\(100\\left(1-\\alpha\\right)\\)% confidence intervals for \\(\\phi\\) as \\(\\left(l_{RR},\\;u_{RR}\\right)\\), where the limits are \\[ \\log\\left(\\frac{p_T}{p_C}\\right) \\pm z_{\\frac{\\alpha}{2}}\\sqrt{\\left(r_T^{-1} - n_T^{-1}\\right) + \\left(r_C^{-1} - n_C^{-1}\\right)}. \\] This then translates to an interval for the risk ratio itself of \\(\\left(e^{l_{RR}},e^{u_{RR}}\\right)\\). Example 6.8 Returning once again to our streptomycin example, recall that we have \\[ \\begin{aligned} r_T &amp; = 38\\\\ n_T &amp; = 55 \\\\ r_C &amp; = 17 \\\\ n_C &amp; = 52 \\end{aligned} \\] and so the limits of the confidence interval (with \\(\\alpha=0.05\\)) on the log scale are \\[\\log\\left(\\frac{38/55}{17/52}\\right) \\pm 1.96\\sqrt{\\frac{1}{38} - \\frac{1}{55} + \\frac{1}{17} - \\frac{1}{52}} = \\log(2.11) \\pm 1.96 \\times 0.218\\] which gives us \\(\\left(0.320,\\,1.176\\right)\\) on the log scale, and a 95% CI for the risk ratio of \\(\\left(1.377,\\,3.243\\right)\\). So, we’ve seen that we can find confidence intervals for each of our four measures of difference. But we probably want to also be able to incorporate baseline measurements, as we did for continuous outcome variables. 6.3 Accounting for baseline observations: logistic regression We saw with the continuous outcomes that it is often advantageous to include baseline measurements of the outcome (if they are known) in our analysis, and this is the same for binary outcomes. In this section we use the term ‘baseline observations’ to mean any measurement that was known before the trial started. Unlike with continuous measurements, with a binary outcome, there is not usually a pre-trial value of the primary outcome. A binary outcome is often already relative to pre-trial (for example ‘Have the patient’s symptoms improved?’) or refers to an event that definitely wouldn’t have happened pre-trial (for example ‘Did the patient die within the next 6 months?’ or ‘Was the patient cured?’). However, as we saw with ANCOVA, we can include other sorts of covariates in a linear model, so this is fine. The general form of model that we would like for patient \\(i\\) is \\[\\text{outcome}_i = \\mu + \\tau G_i + \\beta_1\\times{\\text{baseline}_{1i}} + \\ldots + \\beta_p\\times{\\text{baseline}_{pi}} + \\text{error}_i,\\] where \\(G_i\\) is an indicator function taking values 1 if patient \\(i\\) was in group \\(T\\) and 0 if they were in group \\(C\\), \\(\\text{baseline}_1,\\;\\ldots,\\;\\text{baseline}_p\\) are \\(p\\) baseline measurements Problems with binary variables. The outcome for patient \\(i\\) will be either 0 or 1, but the terms in the model above do not guarantee this at all. Adding a normally distributed error term doesn’t really make sense in this context, so we will remove it. We can also make the LHS continuous by modelling mean outcome rather than a single outcome. This makes sense, since if several patients were identical to patient \\(i\\) (in the sense of having the same baseline covariate values and being allocated to the same treatment), we probably wouldn’t expect them all to have exactly the same outcome. In which case our model becomes \\[\\text{mean outcome}_i = \\mu + \\tau G_i + \\beta_1\\times{\\text{baseline}_{1i}} + \\ldots + \\beta_p\\times{\\text{baseline}_{pi}}.\\] However now, our LHS is in \\(\\left[0,1\\right]\\) but the RHS could take any real value. To address this we use the logit transformation, which takes the mean outcome from \\(\\left[0,1\\right]\\) to \\(\\mathbb{R}\\). The logit function is the log of the odds, \\[\\operatorname{logit}\\left(\\pi\\right) = \\log\\frac{\\pi}{1-\\pi}.\\] As \\(\\pi\\) tends to zero, \\(\\operatorname{logit}\\left(\\pi\\right)\\) tends to \\(-\\infty\\), and as \\(\\pi\\) tends to one, \\(\\operatorname{logit}\\left(\\pi\\right)\\) tends to \\(\\infty\\). The derivative of the \\(\\operatorname{logit}\\) function is \\[ \\frac{d\\operatorname{logit}\\left(\\pi\\right)}{d\\pi} = \\frac{1}{\\pi\\left(1-\\pi\\right)}\\] which is always positive for \\(\\pi\\in\\left[0,1\\right]\\). This means that we can use it to transform our mean outcome (which we will now call \\(\\pi\\), since the mean outcome is the estimate of the probability of success) in the model \\[\\begin{equation} \\operatorname{logit}\\left(\\pi\\right) = \\mu + \\tau G + \\beta_1\\times{\\text{baseline}_{1}} + \\ldots + \\beta_p\\times{\\text{baseline}_{p}} \\tag{6.3} \\end{equation}\\] and any value in \\(\\mathbb{R}\\) is allowed on both sides. This model is known as logistic regression, and belongs to a class of models called Generalized Linear Models. If you did Advanced Statistical Modelling III you’ll have seen these before. If you haven’t seen them, and want to know more, this article gives a nice introduction (and some useful R tips!). 6.3.1 What does this model tell us? We now have an equation for a model that makes sense, but what is it actually modelling? And what does it tell us about the effect of the treatment? Consider the difference between two patients who are the same in every respect except one is assigned to group \\(C\\) (so \\(G=0\\)) and the other to group \\(T\\) (so \\(G=1\\)). The model gives: \\[ \\begin{aligned} \\operatorname{logit}\\left(\\pi\\right) = \\log\\left(\\frac{\\pi}{1-\\pi}\\right) = \\log\\left(\\text{Odds of success group T}\\right) &amp; = \\mu + \\tau + \\beta_1x_1 + \\ldots + \\beta_px_p &amp; \\text{ (group T)}\\\\ \\operatorname{logit}\\left(\\pi\\right) = \\log\\left(\\frac{\\pi}{1-\\pi}\\right) = \\log\\left(\\text{Odds of success group C}\\right) &amp; = \\mu + \\beta_1x_1 + \\ldots + \\beta_px_p &amp; \\text{ (group C)} \\end{aligned} \\] Subtracting one from the other, we find \\[ \\begin{aligned} \\log(\\text{Odds of success for group T}) - &amp; \\log(\\text{Odds of success for group C})\\\\ &amp;= \\log\\left(\\frac{\\text{Odds of success for group T}}{\\text{Odds of success for group C}}\\right) = \\log\\left(OR\\right) \\\\ &amp;= \\tau. \\end{aligned} \\] That is, \\(\\tau\\) is the log of the OR, or \\(e^\\tau\\) is the OR adjusted for variables \\(x_1,\\;\\ldots,\\;x_p\\). While the baseline covariates \\(x_1,\\ldots,x_p\\) affect the probability of ‘success’, \\(\\tau\\) is a measure of the effect of the treatment compared to control given some set of baseline covariate values. A logistic regression model can also be used to predict the odds of ‘success’ for a patient with particular characteristics. Next Lecture: Fitting a logistic regression model Diagnostics for logistic regression 6.3.2 (Lecture 14) Fitting a logistic regression model Logistic regression models are generally fitted using maximum likelihood. In the notation of Equation (6.3) \\[\\operatorname{logit}\\left(\\pi\\right) = \\mu + \\tau G + \\beta_1\\times{\\text{baseline}_{1}} + \\ldots + \\beta_p\\times{\\text{baseline}_{p}},\\] the parameters we need to fit are \\(\\mu,\\;\\tau\\) and \\(\\beta_1,\\ldots,\\beta_p\\). To ease notation, we will collect these into a vector \\(\\boldsymbol\\beta\\), with \\(\\beta_0=\\mu\\), \\(\\beta_1=\\tau\\) and \\(\\beta_2,\\ldots,\\beta_{p+1}\\) the original \\(\\beta_1,\\ldots,\\beta_p\\). Sorry this is confusing - we won’t really use the vector \\(\\boldsymbol\\beta\\) after this, or think about the parameters individually (apart from \\(\\tau\\)). Now we write the RHS of Equation (6.3) for participant \\(i\\) as \\[x_i^T\\boldsymbol\\beta = \\sum\\limits_{j=0}^{q} x_{ij}\\beta_j, \\] where \\(x_{i0}=1\\) (so that \\(\\beta_0\\) is the intercept \\(\\mu\\)) \\(x_{i1}= \\begin{cases} 0\\text{ if participant }i\\text{ is in group }C\\\\ 1\\text{ if participant }i\\text{ is in group }T \\end{cases}\\) \\(x_{i2},\\ldots,x_{iq}\\) are the baseline covariates. If \\(\\pi_i\\) is \\(p\\left(Y_i=1\\right)\\) \\(i\\) for patients \\(i=1,\\ldots,n\\), then the logistic model specifies these \\(n\\) parameters through the \\(q+1\\) parameters \\(\\beta_j\\), via the \\(n\\) expressions \\[\\begin{equation} \\operatorname{logit}\\left(\\pi_i\\right) = x_i^T\\boldsymbol\\beta. \\tag{6.4} \\end{equation}\\] Using the Bernoulli distribution The likelihood given data \\(y_1,\\ldots,y_n\\) (remember these are all zero or one) is \\[L\\left(\\left\\lbrace\\pi_i\\right\\rbrace\\mid \\left\\lbrace y_i \\right\\rbrace \\right) = \\prod\\limits_{i=1}^n \\pi_i^{y_i}\\left(1-\\pi_i\\right)^{1-y_i}.\\] The log-likelihood is therefore \\[\\begin{align*} \\ell\\left(\\left\\lbrace\\pi_i \\right\\rbrace \\mid\\left\\lbrace y_i\\right\\rbrace\\right) &amp; = \\sum\\limits_{i=1}^n\\left[y_i\\log(\\pi_i) + \\left(1-y_i\\right)\\log\\left(1-\\pi_i\\right)\\right]\\\\ &amp; = \\sum\\limits_{i=1}^n\\left[y_i\\log\\left(\\frac{\\pi_i}{1-\\pi_i}\\right) + \\log\\left(1-\\pi_i\\right)\\right], \\end{align*}\\] where \\(y_i=0\\) or 1 is the outcome for participant \\(i\\). Using Equation (6.4) we can rewrite this in terms of \\(\\boldsymbol\\beta\\) as \\[\\ell\\left(\\left\\lbrace\\beta_j \\right\\rbrace\\mid{\\text{data}}\\right) = \\sum\\limits_{i=1}^n \\left[y_i x_i^T\\boldsymbol\\beta - \\log\\left(1+e^{x_i^T\\boldsymbol\\beta}\\right)\\right].\\] The fitted model is then the one with the values \\(\\beta_j\\), \\(j=0,\\dots,q\\), that maximise this expression (and hence maximise the likelihood itself), which we will label the \\(\\left\\lbrace \\hat{\\beta}_j\\right\\rbrace\\). This is generally done some via some numerical method, and we won’t go into that here. The method used by R will generate the MLE \\(\\hat\\beta_j\\) for each \\(\\beta_j\\), and also an estimate of the standard error of each \\(\\hat\\beta_j\\). In particular there will be an estimate of the standard error of \\(\\hat\\beta_1\\), better known as \\(\\hat\\tau\\), the estimate of the treatment effect. This is important, because it means we can test the hypothesis that \\(\\tau=0\\), and can form a confidence interval for the adjusted log odds ratio. Some cautions As with any linear model, we need to ensure that it is appropriate for our dataset. Two key things we need to check for are: Collinearity: we should make sure that none of the independent variables are highly correlated. This is not uncommon in clinical datasets, since measurements are sometimes strongly related. Sometimes therefore, this can mean choosing only one out of a collection of two or more strongly related variables. linear effect across the range of the dataset: a linear model is based on the assumption that the effect of the independent variables is the same across the whole range of the data. This is not always the case. For example, the rate of deterioration with age can be more at older ages. This can be dealt with either by binning age into categories, or by using a transformation, eg. age\\(^2\\). Note that this would still be a linear model, because it is linear in the coefficients. Example 6.9 Testing whether use of anti-inflammatory NSAID therapies at time of ERCP (an invasive medical procedure) reduce rate of panreatitis. Occurance of post-ERCP pancreatitis around 16%. The study had 602 participants. A procedure performed by threading an endoscope through the mouth to the opening in the duodenum where bile and pancreatic digestive juices are released into the intestine. ERCP is helpful for treating blockages of flow of bile (gallstones, cancer), or diagnosing cancers of the pancreas, but has a high rate of complications (15-25%). The occurrence of post-ERCP pancreatitis is a common and feared complication, as pancreatitis can result in multisystem organ failure and death, and can occur in ~ 16% of ERCP procedures. This study tests whether the use of anti-inflammatory NSAID therapies at the time of ERCP reduce the rate of this complication. The dataset contains 33 variables, but we will focus on a small number: \\(X\\): (primary outcome) - incidence of post-ercp pancreatitis 0 (no), 1 (yes). Treatment arm rx: 0 (placebo), 1 (treatment) Site: 1, 2, 3, 4 Risk: Risk score (1 to 5). Should be factor but treated as continuous. Age: from 19 to 90, mean 45.27, SD 13.30. The correlation between risk and age is -0.216, suggesting no problems of collinearity between those two variables. Note: an obvious one to include would be gender, but I tried it and it is not at all significant, so I have pre-whittled it down for [even more] simplicity. data(&quot;indo_rct&quot;) summary(indo_rct[ ,c(1,2,3,4,6,32)]) ## id site age risk outcome ## Min. :1001 1_UM :164 Min. :19.00 Min. :1.000 0_no :523 ## 1st Qu.:1152 2_IU :413 1st Qu.:35.00 1st Qu.:1.500 1_yes: 79 ## Median :2138 3_UK : 22 Median :45.00 Median :2.500 ## Mean :1939 4_Case: 3 Mean :45.27 Mean :2.381 ## 3rd Qu.:2289 3rd Qu.:54.00 3rd Qu.:3.000 ## Max. :4003 Max. :90.00 Max. :5.500 ## rx ## 0_placebo :307 ## 1_indomethacin:295 ## ## ## ## ## Some things to note: # There are very few patients in group 4, and not many in group 3 # The age range goes from 19 to 90 # &#39;rx&#39; is the group variable ## Checking for collinearity with factor variables # No consistent patterns between age and site or risk and site indo_rct%&gt;% group_by(site) %&gt;% summarise( meanage=mean(age), sdage=sd(age), meanrisk = mean(risk), sdrisk=sd(risk) ) ## # A tibble: 4 × 5 ## site meanage sdage meanrisk sdrisk ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1_UM 47.2 14.2 2.06 0.888 ## 2 2_IU 44.4 12.9 2.52 0.846 ## 3 3_UK 45.9 11.6 2.23 0.896 ## 4 4_Case 47.3 22.9 1.67 0.289 ## We will try models with age and age^2 glm_indo_agelin = glm(outcome ~ age + site + risk + rx, data=indo_rct, family = binomial(link = &quot;logit&quot;)) glm_indo_agesq = glm(outcome ~ I(age^2) + site + risk + rx, data=indo_rct, family = binomial(link = &quot;logit&quot;)) summary(glm_indo_agelin) ## ## Call: ## glm(formula = outcome ~ age + site + risk + rx, family = binomial(link = &quot;logit&quot;), ## data = indo_rct) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.786293 0.641354 -2.785 0.00535 ** ## age -0.008458 0.009921 -0.853 0.39390 ## site2_IU -1.229290 0.269258 -4.565 4.98e-06 *** ## site3_UK -1.127935 0.775917 -1.454 0.14603 ## site4_Case -13.864394 827.921132 -0.017 0.98664 ## risk 0.561880 0.142342 3.947 7.90e-05 *** ## rx1_indomethacin -0.763269 0.261538 -2.918 0.00352 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 468.01 on 601 degrees of freedom ## Residual deviance: 427.07 on 595 degrees of freedom ## AIC: 441.07 ## ## Number of Fisher Scoring iterations: 14 summary(glm_indo_agesq) ## ## Call: ## glm(formula = outcome ~ I(age^2) + site + risk + rx, family = binomial(link = &quot;logit&quot;), ## data = indo_rct) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.954e+00 4.930e-01 -3.963 7.39e-05 *** ## I(age^2) -9.388e-05 1.081e-04 -0.869 0.38498 ## site2_IU -1.231e+00 2.693e-01 -4.571 4.87e-06 *** ## site3_UK -1.135e+00 7.759e-01 -1.463 0.14355 ## site4_Case -1.385e+01 8.275e+02 -0.017 0.98664 ## risk 5.593e-01 1.427e-01 3.919 8.88e-05 *** ## rx1_indomethacin -7.617e-01 2.614e-01 -2.914 0.00357 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 468.01 on 601 degrees of freedom ## Residual deviance: 427.03 on 595 degrees of freedom ## AIC: 441.03 ## ## Number of Fisher Scoring iterations: 14 Since neither age nor age^2 appear influential, we’ll remove it and keep the other covariates. glm_indo = glm(outcome ~ site + risk + rx, data=indo_rct, family = binomial(link = &quot;logit&quot;)) summary(glm_indo) ## ## Call: ## glm(formula = outcome ~ site + risk + rx, family = binomial(link = &quot;logit&quot;), ## data = indo_rct) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.2307 0.3814 -5.848 4.97e-09 *** ## site2_IU -1.2204 0.2689 -4.539 5.66e-06 *** ## site3_UK -1.1289 0.7755 -1.456 0.14546 ## site4_Case -13.8400 833.2426 -0.017 0.98675 ## risk 0.5846 0.1395 4.191 2.78e-05 *** ## rx1_indomethacin -0.7523 0.2610 -2.883 0.00395 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 468.01 on 601 degrees of freedom ## Residual deviance: 427.81 on 596 degrees of freedom ## AIC: 439.81 ## ## Number of Fisher Scoring iterations: 14 From the summary we see that \\(\\hat\\tau = -0.752\\), with a standard error of 0.261. A 95% CI for \\(\\tau\\) is therefore \\[-0.752 \\pm 1.96\\times 0.261 = \\left(-1.26,\\;-0.240\\right),\\] and we can use this to find a 95% CI for the OR: \\[\\left(e^{-1.26},\\,e^{-0.240}\\right) = \\left(0.284,\\; 0.787\\right).\\] Through this model we reject the null hypothesis that \\(\\pi_T = \\pi_C\\), or that \\(OR=1\\). We do see however from the Null deviance and the Residual deviance that the model isn’t explaining a huge proportion of the variation. We can also use the model to estimate the odds of ‘success’ (the outcome \\(Y_i=1\\)) for different groups of patients. If \\[\\log\\left(\\frac{p(Y_i=1)}{1-p(Y_i=1)}\\right)=x_i^T\\hat{\\boldsymbol\\beta},\\] where \\(Y_i\\) here is the primary outcome for patient \\(i\\). Rearrange to find the probability, \\[p\\left(Y_i=1\\right) = \\frac{\\exp\\left(x_i^T\\boldsymbol\\beta\\right)}{1+\\exp(x_i^T\\boldsymbol\\beta)}. \\] This will be the probability, according to the model, that a patient with this particular combination of baseline characteristics will have outcome 1. Example 6.10 Continuing with Example 6.9, we can find estimates of the log odds (and therefore the probability) of post-ECRP pancreatitis for various categories of patient. For this we will make heavy use of the summary table summary(glm_indo) ## ## Call: ## glm(formula = outcome ~ site + risk + rx, family = binomial(link = &quot;logit&quot;), ## data = indo_rct) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.2307 0.3814 -5.848 4.97e-09 *** ## site2_IU -1.2204 0.2689 -4.539 5.66e-06 *** ## site3_UK -1.1289 0.7755 -1.456 0.14546 ## site4_Case -13.8400 833.2426 -0.017 0.98675 ## risk 0.5846 0.1395 4.191 2.78e-05 *** ## rx1_indomethacin -0.7523 0.2610 -2.883 0.00395 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 468.01 on 601 degrees of freedom ## Residual deviance: 427.81 on 596 degrees of freedom ## AIC: 439.81 ## ## Number of Fisher Scoring iterations: 14 For example, a patient from site 1, with risk level 3, in the control group would have odds \\[\\exp\\left(-2.2307 + 3\\times 0.5846\\right) = 0.6207, \\] which translates to a probability of post-ECRP pancreatitis of \\[\\frac{0.6207}{1+0.6207} = 0.383. \\] By contrast, a patient in group \\(T\\), from site 2, at risk level 1, would have odds \\[\\exp\\left(-2.2307 - 1.2204 + 1\\times 0.5846 - 0.7523\\right) = 0.0268, \\] which is equivalent to a probability of post-ECRP pancreatitis of \\[\\frac{0.0268}{1+0.0268} = 0.0261.\\] Being more methodical we can collect these into a table. Since the site 3 and 4 coefficents are not significant (mainly due to a lack of data), we will treat them as zero and lump them in with the site 1 participants These are fitted values, around which there is some uncertainty! ## site risk rx logodds odds prob ## 1 2_IU 1 0_placebo -2.87 0.06 0.05 ## 2 2_IU 2 0_placebo -2.28 0.10 0.09 ## 3 2_IU 3 0_placebo -1.70 0.18 0.15 ## 4 2_IU 4 0_placebo -1.11 0.33 0.25 ## 5 2_IU 5 0_placebo -0.53 0.59 0.37 ## 6 1_UM 1 0_placebo -1.65 0.19 0.16 ## 7 1_UM 2 0_placebo -1.06 0.35 0.26 ## 8 1_UM 3 0_placebo -0.48 0.62 0.38 ## 9 1_UM 4 0_placebo 0.11 1.11 0.53 ## 10 1_UM 5 0_placebo 0.69 2.00 0.67 ## 11 2_IU 1 1_indomethacin -3.62 0.03 0.03 ## 12 2_IU 2 1_indomethacin -3.03 0.05 0.05 ## 13 2_IU 3 1_indomethacin -2.45 0.09 0.08 ## 14 2_IU 4 1_indomethacin -1.86 0.15 0.13 ## 15 2_IU 5 1_indomethacin -1.28 0.28 0.22 ## 16 1_UM 1 1_indomethacin -2.40 0.09 0.08 ## 17 1_UM 2 1_indomethacin -1.81 0.16 0.14 ## 18 1_UM 3 1_indomethacin -1.23 0.29 0.23 ## 19 1_UM 4 1_indomethacin -0.64 0.52 0.34 ## 20 1_UM 5 1_indomethacin -0.06 0.94 0.49 6.4 Diagnostics for logistic regression There are many diagnostic techniques for binomial data (see eg. Collett (2003a)) but we will only touch on a small number. Unlike with a linear regression model, we don’t have residuals to analyse, because our model output is fundamentally different from our data: our model outputs are probabilities, but our data is all either 0 or 1. Just because a particular patient had an outcome of 1, we can’t conclude that their probability should have been high. If the ‘true’ probability of \\(X=1\\) for some group of similar (in the baseline covariates sense) patients is 0.9, this means we should expect 1 in 10 of these patients to have \\(X=0\\). This makes diagnostics somewhat trickier. Diagnostics for logistic regression fall into two categories: discrimination and calibration. We will look at each of these in turn, though by no means exhaustively. 6.4.1 Discrimination Treating logistic regression model as a classifier: for each participant the model outputs some value, on the \\(\\operatorname{logit}\\left(p\\right)\\) scale. If that value is below some threshold, we classify that participant as 0 If the value is above the threshold, we classify them as 1. Here, we are slightly abandoning the notion that the model is predicting probabilities, and instead testing whether the model can successfully order the patients correctly. Can we set some threshold on the model output that (almost) separates the cohort into its ones and zeros? A classic way to asses this is by using Receiver Operating Characteric (ROC) analysis. ROC analysis was developed during the second world war, as radar operators analysed their classification accuracy in distinguishing signal (eg. an enemy plane) from noise. It is still widely used in the field of statistical classification, including in medical diagnostics. ROC analysis can be applied to any binary classifier, not just logistic regression. 6.4.1.1 ROC analysis To understand ROC analysis, we need to revisit two concepts relating to tests or classifiers that you might not have seen since Stats I, and we will introduce (or remind ourselves of) some notation to do this: \\(\\hat{p}_i\\in\\left(0,1\\right)\\) is the fitted value of the logistic regression model for patient \\(i\\) \\(Y_i=0\\) or \\(1\\) is the true outcome for patient \\(i\\) \\(t\\in\\left(0,1\\right)\\) is the threshold value. If \\(\\hat{p}_i&lt;t\\) we classify patient \\(i\\) as 0 \\(\\hat{p}\\geq t\\) we classify them as 1. The language of ROC analysis is so entrenched in diagnostic/screening tests that I have kept it here for consistency. A ‘positive’ result for us is \\(Y=1\\), and a ‘negative’ result is \\(Y=0\\). Definition 6.1 The sensitivity of a test (or classifier) is the probability that it will output positive (or 1) if the true value is positive (or 1): \\[p\\left(\\hat{p}_i \\geq t \\mid{Y_i=1}\\right).\\] Definition 6.2 The specificity of a test (or classifier) is the probability that it will output negative (or 0) if the true value is negative (or 0): \\[p\\left(\\hat{p}_i &lt; t \\mid{Y_i=0}\\right) \\] We estimate these by the proportions within the dataset. These are very commonly used for thinking about diagnostic tests and screening tests, and in these contexts a ‘success’ or ‘positive’ is almost always the presence of some condition or disease. In our context, we need to be mindful that a 1 could be good or bad, depending on the trial. The core part of a ROC analysis is to plot sensitivity against 1-specificity for every possible value of the threshold. In a logistic regression context, the lowest the threshold can be is zero. If we set the \\(t=0\\), the model will predict everyone to have an outcome of 1. The sensitivity will be 1 and the specificity will be 0. If we set \\(t=0\\), we will classify everyone as a 0, and have sensitivity 0 and specificity 1. If we vary the threshold from 0 to 1 the number of people classified in each group will change, and so will the sensitivity and specificity. This forms a ROC curve. DASHBOARD!!! Example 6.11 Let’s look at the model we fitted in Example 6.9. To draw the ROC curve of this data, we will use the R package pROC. fit_indo = fitted(glm_indo) # Fitted values from glm_indo out_indo = indo_rct$outcome # outcome values (0 or 1) roc_indo_df = data.frame(fit = fit_indo, out = out_indo) The main function in the package pROC is roc, which creates a roc object. and ggroc that sort and plot the data for us: roc_indo = roc(data=roc_indo_df, response = out, predictor=fit) With that object we can do various things, such as plot the ROC curve: ggroc(roc_indo, legacy.axes=T) + geom_abline(slope=1, intercept=0, type=2)+theme_bw() Figure 6.3: ROC curve for our logistic regression model of the indo RCT data (solid line). The dotted line shows the ROC curve we’d expect with random guessing. and find the area under the curve for the model auc(roc_indo) ## Area under the curve: 0.7 So we see that our model is better than random guessing, but really not all that good! Wherever we put a threshold (if we use the model that way), many people will be mis-classified. It’s also worth noting that here we’re performing the diagnostics on the data we used to fit the model: if we were to use the model on a new set of patients, the fit would likely be slightly worse. 6.4.2 Calibration Here we are thinking of the model as actually modelling probabilities, and therefore we want to determine whether these probabilities are, in some sense, ‘correct’ or ‘accurate’. Work through different ‘types’ of patient (by which we mean different combinations of baseline covariate values) and see whether the proportions of ones in the data broadly match the probability given by the model. Quite an ad-hoc method! If the explanatory variables are factors, and we have repeated observations for the different combinations of factor levels, then for each combination we can estimate the probability of success (or whatever our outcome variable is) using the data, and compare this to the fitted model value. For continuous explanatory variables we will need to bin the data into ranges. Example 6.12 This example uses the model fitted in Example 6.9. The trial has 602 participants and there are many fewer than 602 combinations of the above factor variables, so for many such combinations we will have estimates. Since we are in three dimensions, plotting the data is moderately problematic. We will have a plot for each site (or for the two main ones), use risk score for the \\(x\\) axis and colour points by treatment group. The circles show the proportions of ones in the data, and are sized by the number of observations used to calculate that estimate, and the crosses and lines show the mean and 95% CI of the fitted value. These plots are not the easiest to interpret, but there seems to be no evidence of systematic trends away from the model. We will look some more at this in the upcoming practical class, as well as some further principles of model validation. For now, we’re done with Binary data, and in our next few lectures we’ll think about survival, or time-to-event data. References al, Geoffrey Marshall et. 1948. “STREPTOMYCIN TREATMENT OF PULMONARY TUBERCULOSIS a MEDICAL RESEARCH COUNCIL INVESTIGATION.” British Medical Journal. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2091872/pdf/brmedj03701-0007.pdf. ———. 1998. “Confidence Intervals for the Number Needed to Treat.” Bmj 317 (7168): 1309–12. Collett, David. 2003a. Modelling Binary Data. 2nd ed. Texts in Statistical Science. Chapman &amp; Hall. "],["lecture-15-working-with-time-to-event-data.html", "7 (lecture 15) Working with time-to-event data 7.1 Censored times 7.2 The Survival Curve and the Hazard function", " 7 (lecture 15) Working with time-to-event data time to event data / survival data - the amount of time that elapses before a particular event happens. As a sub-field of statistics, survival analysis has been around for a long time, as people have thought about and worked data like mortality records (most notably John Graunt, who used the ‘Bills of Mortality’ during the 1600s to better understand the plague and other causes of death). However, it developed rapidly during the many cancer related clinical trials of the 1960s and 1970s. IN clinical trials, the event is very often death, and so the language of ‘death’ and ‘survival’ pervade the field. However, the event can be other things, and can indeed be positive. *why this branch of statistics came to be known as **survival analysis*. Time-to-event data also appears in other applications, such as engineering (eg. monitoring the reliability of a machine) and marketing (eg. thinking of the time-to-purchase). As well as the books already mentioned, this chapter makes use of Collett (2003b). Usually, survival data is given in terms of time, but it can also be the number of times something happens (for example, the number of clinic appointments attended) before the event in question occurs. Time to event data is tricky because: skewed - can’t use normal dist censored - we don’t usually observe the full dataset 7.1 Censored times For some participants, we won’t observe the event: it may not happen to them in the lifetime of the trial they may exit the trial early for some reason they may be lost to follow-up We know these people hadn’t experienced the event up to some time \\(t\\), but we don’t know what happened next. All we know is that their time-to-event or survival time is greater than that time \\(t\\). These partial observations are known as censored times, and in particular as right-censored times, because the event happens after the censored time. It is possible (but less common) to have left-censored or interval-censored data, but in this course we will deal only with right-censoring. Figure 7.1: An example of some censored data. Possible options: Treat censored times as observations, ie. as though the event had happened at time \\(t\\) - bias the results of the trial. The survival times reported would be systematically shorter than the true ones. For example, in the dataset shown in Figure 7.1, we would estimate the survival probability at time 10 as 0.2, since only two of the 10 participants were still in the trial after time 10. But it may well be that some of the participants whose observations were censored before \\(t=10\\) were still alive at \\(t=10\\). Remove the censored times, and only analyse complete observations - losing data and therefore valuable information. This approach may well also lead to bias, for example if some subset of patients died quite soon into the trial, but the remainder lived a long time (past the end of the trial). If our analysis ignores the survivors, we are likely to underestimate the general survival time. In the dataset in Figure 7.1 there are five participants (3,6,7,8,10) whom we are no longer able to observe at time 10, but of whom none had experienced the event by the point at which they were censored. So we know that we need to somehow include these censored times in our analysis. How we do so will depend on our approach. 7.2 The Survival Curve and the Hazard function The field of survival analysis is relatively unusual in statistics, in that it isn’t treated predominantly parametrically. For most continuous data, it is overwhelmingly common to work with the normal distribution and its friends (eg. the student’s t distribution). Similarly binary data is dominated by the binomial distribution. Inference is therefore often focussed on the parameters \\(\\mu,\\;\\sigma\\) or \\(p\\), as an adequate summary of the truth given whatever parameteric assumption has been made. However, in survival analysis, it is often the case that we focus on the whole shape of the data; there isn’t an accepted dominating approach. We need to introduce some key ways of working with such data. The survival time (or time-to-event) \\(t\\) for a particular individual can be thought of as the value of a random variable \\(T\\), which can take any non-negative value. We can think in terms of a probability distribution over the range of \\(T\\). If \\(T\\) has a probability distribution with underlying probability density function \\(f\\left(t\\right)\\), then the cumulative distribution function is given by \\[F\\left(t\\right) = P\\left(T&lt;t\\right) = \\int\\limits_0^t f\\left(u\\right)du, \\] and this gives us the probability that the survival time is less than \\(t\\). Definition 7.1 The survival function, \\(\\operatorname{S}\\left(t\\right)\\), is the probability that some individual (in our context a participant) survives longer than time \\(t\\). Therefore \\(\\operatorname{S}\\left(t\\right) = 1 - F(t)\\). Conventionally we plot \\(\\operatorname{S}\\left(t\\right)\\) against \\(t\\) and this gives us a survival curve. We can immediately say two things about survival curves: All participants must be alive at the start of the trial \\(\\implies\\operatorname{S}\\left(0\\right)=1\\). It’s impossible to survive past \\(t_2&gt;t_1\\) but not past time \\(t_1\\) \\(\\implies\\frac{d\\operatorname{S}\\left(t\\right)}{dt}\\leq{0}\\), ie. \\(\\operatorname{S}\\left(t\\right)\\) is non-increasing. Figure 7.2 shows two survival curves, comparing different therapies. We see that the hormonal therapy reduces the survival time slightly compared to no hormonal therapy. Figure 7.2: An example of two survival curves, taken from Syriopoulou et al. (2022). Following on from the survival function, we have the Hazard function \\(h\\left(t\\right)\\). Definition 7.2 The Hazard function \\(h(t)\\) is the probability that an individual who has survived up to time \\(t\\) fails just after time \\(t\\); in other words, the instantaneous probability of death (or experiencing the event) at time \\(t\\). If we use \\(T\\) to denote the random variable of survival time (or time-to-event) then \\(\\operatorname{S}\\left(t\\right)\\) and \\(h(t)\\) are defined by \\[ \\begin{aligned} \\operatorname{S}\\left(t\\right)&amp;= \\operatorname{Pr}\\left(T&gt;t\\right)\\\\ h\\left(t\\right) &amp; = \\lim\\limits_{s\\rightarrow{0+}}\\frac{\\operatorname{Pr}\\left(t&lt;T&lt;t+s\\mid{T&gt;t}\\right)}{s}. \\end{aligned} \\] Using the definition of conditional probability, we can rewrite \\(h(t)\\) as \\[ \\begin{aligned} h\\left(t\\right) &amp; = \\lim\\limits_{s\\rightarrow{0+}}\\frac{\\operatorname{Pr}\\left(t&lt;T&lt;t+s\\mid{T&gt;t}\\right)}{s} \\\\ &amp; = \\lim\\limits_{s\\rightarrow{0+}}\\left[\\frac{1}{\\operatorname{Pr}\\left(T&gt;t\\right)}\\cdot\\frac{\\operatorname{Pr}\\left(\\left(t&lt;T&lt;t+s\\right)\\cap\\left(T&gt;t\\right)\\right)}{s}\\right] \\\\ &amp; = \\lim\\limits_{s\\rightarrow{0+}}\\left[\\frac{1}{\\operatorname{Pr}\\left(T&gt;t\\right)}\\cdot\\frac{\\operatorname{Pr}\\left(t&lt;T&lt;t+s\\right)}{s}\\right]\\\\ &amp; = \\frac{f\\left(t\\right)}{\\operatorname{S}\\left(t\\right)}, \\end{aligned} \\] where \\(f\\left(\\cdot\\right)\\) is the probability density of \\(T\\). The hazard function can take any positive value (unlike the survival function), and for this reason \\(\\log\\left(h\\left(t\\right)\\right)\\) is often used to transform it to the real line. The hazard function can also be called the ‘hazard rate’, the ‘instantaneous death rate’, the ‘intensity rate’ or the ‘force of mortality’. As we hinted before, there are fundamentally two ways to deal with survival data: we can go about things either parametrically or non-parametrically. Unusually for statistics in general, the non-parametric paradigm is prevalent in survival analysis. We will consider some methods from both paradigms. We will look at parametric and non-parametric ways to estimate these. 7.2.1 The Kaplan-Meier estimator The Kaplan-Meier estimator is a non-parametric estimate of \\(\\operatorname{S}\\left(t\\right)\\), originally presented in Kaplan and Meier (1958). The idea behind it is to divide the interval \\(\\left[0,\\;t\\right]\\) into many short consecutive intervals, \\[\\left[0,\\,t\\right] = \\bigcup\\limits_{k=0}^K \\left[s_k,\\,s_{k+1}\\right],\\] where \\(s_k&lt;s_{k+1}\\;\\forall{k}\\), \\(s_0=0\\) and \\(s_{K+1}=t\\). We then estimate the probability of surviving past some time \\(t\\) by multiplying together the probabilities of surviving the successive intervals up to time \\(t\\). No distributional assumptions are made, and the probability of surviving interval \\(\\left[s_k,\\,s_{k+1}\\right]\\) is estimated by \\(1-Q\\), where \\[Q = \\frac{\\text{Number who die in that interval}}{\\text{Number at risk of death in that interval}}.\\] Let’s say that deaths are observed at times \\(t_1&lt;t_2&lt;\\ldots &lt; t_n\\), and that the number of deaths at time \\(t_i\\) is \\(d_i\\) out of a possible \\(n_i\\). Then for some time \\(t\\in\\left[t_J,\\,t_{J+1}\\right)\\), the Kaplan-Meier estimate of \\(\\operatorname{S}\\left(t\\right)\\) is \\[\\hat{\\operatorname{S}}\\left(t\\right) = \\prod\\limits_{j=0}^J \\frac{\\left(n_j - d_j\\right)}{n_j}.\\] The number of people at risk at time \\(t_{j+1}\\), \\(n_{j+1}\\), will be the number of people at risk at time \\(t_j\\) (which was \\(n_j\\)), minus any who died at time \\(t_j\\) (which we write as \\(d_j\\)) and any who were censored in the interval \\(\\left[t_j,\\,t_{j+1}\\right)\\). The Kaplan-Meier estimator incorporates information from individuals with censored survival times up to the point they were censored. Example 7.1 Edmonson et al. (1979) conducted a trial on patients with advanced ovarian cancer, comparing cyclophosphamide (group \\(C\\)) with a mixture of cyclophosphamide and adriamycin (group \\(T\\)). Patients were monitored, and their time of death was recorded, or a censoring time if they were alive at their last observation. Table 7.1: Ovarian cancer data. FU time gives the survival or censoring time, and FU status the type: 0 for a censored observation, 1 for death. FU_time FU_status 1 59 1 2 115 1 3 156 1 22 268 1 23 329 1 24 353 1 25 365 1 26 377 0 4 421 0 5 431 1 6 448 0 7 464 1 8 475 1 9 477 0 10 563 1 11 638 1 12 744 0 13 769 0 14 770 0 15 803 0 16 855 0 17 1040 0 18 1106 0 19 1129 0 20 1206 0 21 1227 0 There are 26 individuals, and we have the time of death for 12 of them. The remaining 14 observations are censored. Table 7.2 The columns are (from left to right): time \\(t_j\\); number at risk \\(n_j\\); number of events/deaths \\(d_j\\); number of censorings in \\(\\left[t_{j-1},\\,t_j\\right)\\); estimate of survival curve. Table 7.2: Table 7.3: Kaplan-Meier estimator calculations for ovarian cancer dataset. time n_risk n_event n_cens survival 59 26 1 0 0.9615385 115 25 1 0 0.9230769 156 24 1 0 0.8846154 268 23 1 0 0.8461538 329 22 1 0 0.8076923 353 21 1 0 0.7692308 365 20 1 0 0.7307692 431 17 1 2 0.6877828 464 15 1 1 0.6419306 475 14 1 0 0.5960784 563 12 1 1 0.5464052 638 11 1 0 0.4967320 To demonstrate the effect of including (and correctly treating) the censored data, we can do the same thing, but this time (1) leaving out all censored data and (2) treating all censored data times as deaths. Figure 7.3 shows the resulting survival curve estimates. Figure 7.3: Kaplan-Meier estimate of survival curve for ovarian cancer data, with two incorrect estiamtes also shown. Leaving out the censored observations entirely is the most problematic approach, since it causes a serious underestimate, and in this case behaves as though there are no survivors after \\(t=638\\). Treating the censored data as deaths also leads to an underestimate of the survival probability, and notably creates a rather spurious curve past the last real death observation. The ‘correct’ Kaplan-Meier estimate may seem a bit disatisfying, since it stops at \\(t=638\\) with a probabiilty of \\(0.497\\). However, this is really (in a non-parametric setting) all we can say with the data available; 10 of the participants were definitely still alive at \\(t=638\\), and some of the other censored participants may also have been. For a clinical trial, we want to plot the survival curves separately for the different treatment groups. This will give a first, visual, idea of whether there might be a difference, and also of the suitability of certain models (we’ll talk about this later). Example 7.2 Figure 7.4 shows the Kaplan Meier plots for the ovarian cancer data from Figure 7.3, this time split by treatment group. We will also use functions from the package survival, which avoids doing all the calculations ourselves. Figure 7.4: Kaplan Meier curves for the ovarian cancer data, split by treatment. Red is for group T (mixture of cyclophosphamide and adriamycin), black for group C (cyclophosphamide only). The second dataset we will use throughout this chapter has been simulated based on a trial of acute myeloid leukemia (Le-Rademacher et al. (2018)) and is from the survival package Therneau (2024). Figure 7.5: Kaplan Meier curves for the Myeloid data, split by treatment. Red is for group T, black for group C (placebo). 7.2.2 A parametric approach Assume the survival time \\(T\\) follows some probability distribution, up to unknown parameters which we will estimate from the data. The simplest distribution for time-to-event data is the exponential distribution, which has density \\[f\\left(t\\right) = \\lambda e^{-\\lambda t} \\text{ for }t&gt;0,\\] survival function \\[\\operatorname{S}\\left(t\\right) = 1- \\int\\limits_{0}^t{\\lambda e^{-\\lambda t}} = e^{-\\lambda t}, \\] and mean survival time \\(\\frac{1}{\\lambda}\\). The hazard function is therefore \\[h\\left(t\\right) = \\frac{f\\left(t\\right)}{S\\left(t\\right)} = \\lambda, \\] that is, the hazard is constant. Given some dataset, we want to be able to find an estimate for \\(\\lambda\\) (or the parameters of our distribution of choice). 7.2.2.1 Maximum likelihood for time-to-event data Suppose our dataset has \\(n\\) times \\(t_1,\\,t_2,\\,\\ldots,t_n\\). Of these, \\(m\\) are fully observed and \\(n-m\\) are censored. We also have indicators \\(\\delta_1,\\ldots,\\delta_n\\), where \\[ \\delta_i= \\begin{cases} 1\\; \\text{ if observation }i\\text{ is fully observed}\\\\ 0\\;\\text{ if observation }i\\text{ is censored} \\end{cases} \\] Usually, the likelihood function is computed by multiplying the density function evaluated at each data point, \\(f\\left(t_i\\mid{\\text{params}}\\right)\\). However, for our censored times (\\(\\delta_i=0\\)) we only know that the time-to-event is greater than \\(t_i\\). For these observations, it is the survival function (remember that this is \\(p\\left(T&gt;t\\right)\\)) that contributes what we need to the likelihood function. Therefore (for any probability distribution) we have \\[\\begin{equation} L = \\prod\\limits_{i=1}^n f\\left(t_i\\right)^\\delta_i S\\left(t_i\\right)^{1-\\delta_i}. \\tag{7.1} \\end{equation}\\] If we have \\(T\\sim Exp\\left(\\lambda\\right)\\) then the log-likelihood is \\[ \\begin{aligned} \\ell &amp;= \\sum\\limits_{i=1}^n \\delta_i\\left(\\log\\lambda - \\lambda t_i\\right) - \\sum\\limits_{i=1}^n\\left(1-\\delta_i\\right)\\lambda t_i \\\\ &amp; = m\\log\\lambda - \\lambda\\sum\\limits_{i=1}^n t_i. \\end{aligned} \\] From this we can find the maximum likelihood estimator (MLE) \\[\\hat{\\lambda} = \\frac{m}{\\sum\\limits_{i=1}^n t_i}.\\] The variance of the MLE is \\[\\begin{equation} \\operatorname{var}\\left(\\hat\\lambda\\right) = \\frac{\\lambda^2}{m}, \\tag{7.2} \\end{equation}\\] which we can approximate by \\[\\operatorname{var}\\left(\\hat\\lambda\\right) \\approx \\frac{m}{\\left(\\sum\\limits_{i=1}^n t_i\\right)^2}.\\] Notice that the numerator in Equation (7.2) is \\(m\\), the number of complete observations (rather than \\(n\\) the total number including censored observations). This shows that there is a limit to the amount we can learn if a lot of the data is censored. Example 7.3 Returning to the dataset from Example 7.1, we can fit an exponential distribution to the data simply by estimating the MLE \\[ \\begin{aligned} \\hat{\\lambda}_C &amp;= \\frac{m_C}{\\sum\\limits_{i=1}^{n_C} t_i}\\\\ &amp; =\\frac{7}{6725}\\\\ &amp; = 0.00104 \\end{aligned} \\] and \\[ \\begin{aligned} \\hat{\\lambda}_T &amp;= \\frac{m_T}{\\sum\\limits_{i=1}^{n_T} t_i}\\\\ &amp; =\\frac{5}{8863}\\\\ &amp; = 0.00056 \\end{aligned} \\] mC_ov = sum((ovarian$fustat==1)&amp;(ovarian$rx==1)) mT_ov = sum((ovarian$fustat==1)&amp;(ovarian$rx==2)) tsum_ov_C = sum(ovarian$futime[ovarian$rx==1]) tsum_ov_T = sum(ovarian$futime[ovarian$rx==2]) m_ov = mT_ov + mC_ov tsum_ov = tsum_ov_C + tsum_ov_T lamhat_ov_C = mC_ov / tsum_ov_C lamhat_ov_T = mT_ov / tsum_ov_T Figure 7.6: Kaplan Meier estimates of survival curves for the ovarian data (solid lines), with the fitted exponential S(t) shown in dashed lines (red = group T, black = group C). We can do the same for the myeloid data. Figure 7.7 shows the fitted curves, using \\(S\\left(t\\right)=\\exp\\left[-\\hat{\\lambda}_Xt\\right]\\) for group \\(X\\). Figure 7.7: Kaplan Meier estimates of survival curves for the Myeloid data (solid lines), with the fitted exponential S(t) shown in dashed lines (red = group T, black = group C). 7.2.3 The Weibull distribution Having only one parameter, the exponential distribution is not very flexible, and often doesn’t fit data at all well (like with the Myeloid data). A related, but more suitable distribution is the Weibull distribution. Definition 7.3 The probability density function of a Weibull random variable is \\[ f\\left(x\\mid \\lambda,\\,k\\right) = \\begin{cases} \\lambda\\gamma t^{\\gamma-1}\\exp\\left[{-\\lambda t^{\\gamma}}\\right] &amp; \\text{for }t\\geq{0}\\\\ 0 &amp; \\text{otherwise}. \\end{cases} \\] Here, \\(\\gamma\\) is the shape parameter, and \\(\\lambda\\) is the scale parameter. If \\(\\gamma=1\\) then this reduces to an exponential distribution. You can read more about it, should you choose to, in Collett (2003b). For the Weibull distribution, we have \\[\\operatorname{S}\\left(t\\right) = \\exp\\left(-\\lambda t^{\\gamma}\\right). \\] As with the exponential distribution, we can use Equation (7.1) for the likelihood. For the Weibull distribution this becomes \\[\\begin{align*} L\\left(\\lambda,\\gamma\\mid{\\text{data}}\\right) &amp; = \\prod\\limits_{i=1}^n\\left\\lbrace \\lambda \\gamma t_i^{\\gamma-1}\\exp\\left(-\\lambda t_i^{\\gamma}\\right) \\right\\rbrace^{\\delta_i} \\left\\lbrace \\exp\\left[-\\lambda t_i^{\\gamma}\\right] \\right\\rbrace ^{1-\\delta_i}\\\\ &amp; = \\prod\\limits_{i=1}^n \\left\\lbrace \\lambda\\gamma t_i^{\\gamma-1} \\right\\rbrace^{\\delta_i} \\exp\\left(-\\lambda t_i^{\\gamma}\\right) \\end{align*}\\] and therefore \\[\\begin{align*} \\ell\\left(\\lambda,\\gamma\\mid{\\text{data}}\\right) &amp; = \\sum\\limits_{i=1}^n \\delta_i \\log\\left(\\lambda\\gamma\\right) + \\left(\\gamma-1\\right)\\sum\\limits_{i=1}^n\\delta_i \\log{t_i} - \\lambda \\sum\\limits_{i=1}^n t_i^{\\gamma}\\\\ &amp; = r\\log\\left(\\lambda\\gamma\\right) + \\left(\\gamma-1\\right)\\sum\\limits_{i=1}^n \\delta_i\\log t_i - \\lambda\\sum\\limits_{i=1}^n t_i^{\\gamma}. \\end{align*}\\] For the maximum likelihood estimators, we differentiate (separately) with respect to \\(\\lambda\\) and \\(\\gamma\\) and equate to zero, to solve for the estimators \\(\\hat\\lambda\\) and \\(\\hat\\gamma\\). The equations we end up with are \\[\\begin{align} \\frac{r}{\\hat{\\lambda}} - \\sum\\limits_{i=1}^n t_i^{\\hat\\gamma} &amp; = 0 \\tag{7.3}\\\\ \\frac{r}{\\hat{\\gamma}} + \\sum\\limits_{i=1}^n \\delta_i\\log t_i - \\hat{\\lambda}\\sum\\limits_{i=1}^n t_i^{\\hat\\gamma} \\log t_i &amp;=0 \\tag{7.4}. \\end{align}\\] We can rearrange Equation (7.3) to \\[\\hat{\\lambda} = \\frac{r}{\\sum\\limits_{i=1}^n t_i^{\\hat\\gamma}},\\] and substitute this into Equation (7.4) to find \\[\\frac{r}{\\hat{\\gamma}} + \\sum\\limits_{i=1}^n \\delta_i\\log t_i - \\frac{r}{\\sum\\limits_{i=1}^n t_i^{\\gamma}}\\sum\\limits_{i=1}^n t_i^{\\hat\\gamma} \\log t_i=0. \\] This second equation is analytically intractable, so numerical methods are used to find \\(\\hat\\gamma\\), and then this value can be used to find \\(\\hat\\lambda\\). Example 7.4 We can fit Weibull distributions to our myeloid dataset, as shown in Figure 7.8. \\[S\\left(t\\right) = \\exp\\left(-\\lambda t^{\\gamma}\\right).\\] Figure 7.8: Weibull fit to survival curve of Myeloid data, dashed lines (Kaplan Meier estimate also shown in solid lines). Red for group T, black for group C. We see that there is some improvement compared to the exponential fit in Figure 7.7 Aside: Sample size calculations for time-to-event data There are implications here for sample size calculations, which must take into account the duration of a trial; it is important that trials monitor patients until a sufficient proportion have experienced the event (whatever it is). Sample size calculations for time-to-event data therefore have two components: The power of the trial can first be expressed in terms of \\(m\\), the number of complete observations. A separate calculation is needed to estimate the number of participants needing to be recruited, and length of trial, to be sufficiently likely to achieve that value of \\(m\\). Both of these calculations rely on a number of modelling assumptions, and on previous scientific/clinical data (if available). We will think more about how this can be used in the next section, when we come to compare treatment effects. References ———. 2003b. Modelling Survival Data in Medical Research. 2nd ed. Texts in Statistical Science. Chapman &amp; Hall. Edmonson, John H, Thomas R Fleming, David G Decker, George D Malkasian, Edward O Jorgensen, John A Jefferies, Maurice J Webb, and Larry K Kvols. 1979. “Prognosis in Advanced Ovarian Carcinoma Versus Minimal Residual.” Cancer Treatment Reports 63 (2): 241–47. Kaplan, Edward L, and Paul Meier. 1958. “Nonparametric Estimation from Incomplete Observations.” Journal of the American Statistical Association 53 (282): 457–81. Le-Rademacher, Jennifer G, Ryan A Peterson, Terry M Therneau, Ben L Sanford, Richard M Stone, and Sumithra J Mandrekar. 2018. “Application of Multi-State Models in Cancer Clinical Trials.” Clinical Trials 15 (5): 489–98. Syriopoulou, Elisavet, Tove Wästerlid, Paul C Lambert, and Therese M-L Andersson. 2022. “Standardised Survival Probabilities: A Useful and Informative Tool for Reporting Regression Models for Survival Data.” British Journal of Cancer 127 (10): 1808–15. Therneau, Terry M. 2024. A Package for Survival Analysis in r. https://CRAN.R-project.org/package=survival. "],["lecture-16-comparing-survival-curves.html", "8 (lecture 16) Comparing survival curves 8.1 Parametric: likelihood ratio test 8.2 Non-parametric: the log-rank test 8.3 (Lecture 17) Semi-parametric: the proportional hazards model", " 8 (lecture 16) Comparing survival curves In a clinical trials context, we need to be able to compare two survival curves (showing, for example, the results from different treatments), so that we can say whether one is significantly different from the other. Usually boils down to constructing a hypothesis test along the lines of \\[ \\begin{aligned} H_0:&amp;\\text{ the treatments are the same}\\\\ H_1:&amp;\\text{ the treatments are different.} \\end{aligned} \\] There are various ways to do this, and we will look at some now. 8.1 Parametric: likelihood ratio test For a parametric analysis, we can use a likelihood ratio test to test whether the parameters are the same. We know the log-likelihood and MLE for the exponential distribution \\[\\begin{align*} \\ell\\left(\\lambda\\right) &amp; = m\\log\\lambda - \\lambda\\sum\\limits_{i=1}^n t_i\\\\ \\hat{\\lambda} &amp; = \\frac{m}{\\sum\\limits_{i=1}^n t_i}. \\end{align*}\\] The survival function is \\[S\\left(t\\right) = \\begin{cases} e^{-\\lambda_Ct}\\; \\text{ for participants in group C}\\\\ e^{-\\lambda_Tt}\\; \\text{ for participants in group T} \\end{cases} \\] and the null hypothesis boils down to \\[H_0: \\; \\lambda_C = \\lambda_T = \\lambda. \\] Adapting these for the separate groups, we find \\[\\begin{equation} \\ell\\left(\\lambda_C,\\,\\lambda_T\\right) = m_C\\log\\lambda_C - \\lambda_C\\sum\\limits_{i=1}^{n_C}t_{iC} + m_T\\log\\lambda_T - \\lambda_T\\sum\\limits_{i=1}^{n_T}t_{iT} \\tag{8.1} \\end{equation}\\] and \\[\\hat{\\lambda}_X = \\frac{m_X}{\\sum\\limits_{i=1}^{n_X}t_{iX}}\\] where \\(X=C\\) or \\(T\\), \\(m_X\\) is the number of non-censored observations in group \\(X\\), \\(n_X\\) is the total number of participants in group \\(X\\) and \\(t_{iX}\\) is the time for participant \\(i\\) in group \\(X\\). To simplify notation, we will write \\[t^+_X = \\sum\\limits_{i=1}^{n_X}t_{iX},\\] and \\(t^+\\) for the sum over both groups. Substituting the MLEs into Equation (8.1) gives \\[\\ell\\left(\\hat{\\lambda}_C,\\,\\hat{\\lambda}_T\\right) = m_C\\log\\left(\\frac{m_C}{t^+_C}\\right) - m_C + m_T\\log\\left(\\frac{m_T}{t^+_T}\\right) - m_T \\] and \\[\\ell\\left(\\hat\\lambda,\\,\\hat\\lambda\\right) = m\\log\\left(\\frac{m}{t^+}\\right) - m,\\] where \\(n,\\,m\\) are the corresponding totals over both groups. We can therefore perform a maximum likelihood test by finding \\[\\begin{align*} \\lambda_{LR}&amp; = -2\\left[\\ell\\left(\\hat{\\lambda},\\hat{\\lambda}\\right) - \\ell\\left(\\hat{\\lambda}_C,\\hat{\\lambda}_T\\right)\\right] \\\\ &amp; = 2\\left[\\left(m_C\\log\\left(\\frac{m_C}{t^+_C}\\right) - m_C + m_T\\log\\left(\\frac{m_T}{t^+_T}\\right) - m_T \\right) - \\left(m\\log\\left(\\frac{m}{t^+}\\right)\\right)\\right]\\\\ &amp; = 2\\left(m_C\\log\\left(\\frac{m_C}{t^+_C}\\right) + m_T\\log\\left(\\frac{m_T}{t^+_T}\\right) - m\\log\\left(\\frac{m}{t^+}\\right)\\right) \\end{align*}\\] and referring this value to a \\(\\chi^2_1\\) distribution. We can also find a confidence interval for the difference between \\(\\lambda_T\\) and \\(\\lambda_C\\), by using the asymptotic variances of the MLEs, which are \\(\\frac{\\lambda_C^2}{m_C}\\) and \\(\\frac{\\lambda_T^2}{m_T}\\). Therefore, the limits of a \\(100\\left(1-\\alpha\\right)\\)% CI for \\(\\lambda_T - \\lambda_C\\) is given by \\[ \\frac{m_T}{t^+_T} - \\frac{m_C}{t^+_C} \\pm z_{\\alpha/2}\\sqrt{\\frac{m_T}{\\left(t^+_T\\right)^2} + \\frac{m_C}{\\left(t^+_C\\right)^2}}.\\] Example 8.1 In this example we’ll conduct a likelihood ratio test for each of the datasets in Example 7.1. For each dataset, the quantities we need are: \\(m_C,\\,m_T\\): the number of complete observations in each group \\(t^+_C,\\,t^+_T\\) the sum of all observation times (including censored times) in each group Note that \\(m=m_C + m_T\\) and \\(t^+ = t^+_C + t^+_T\\). For the ovarian data we have mC_ov = sum((ovarian$fustat==1)&amp;(ovarian$rx==1)) mT_ov = sum((ovarian$fustat==1)&amp;(ovarian$rx==2)) tsum_ov_C = sum(ovarian$futime[ovarian$rx==1]) tsum_ov_T = sum(ovarian$futime[ovarian$rx==2]) m_ov = mT_ov + mC_ov tsum_ov = tsum_ov_C + tsum_ov_T ## Can now plug these into LR test stat LRstat_ov = 2*(mC_ov*log(mC_ov/tsum_ov_C) + mT_ov*log(mT_ov/tsum_ov_T) - m_ov*log(m_ov/tsum_ov)) LRstat_ov ## [1] 1.114895 We can find the p-value of this test by 1-pchisq(LRstat_ov, df=1) ## [1] 0.2910204 and we find that it isn’t significant. A 95% confidence interval for the difference is given by ## [1] -0.0013927697 0.0004392714 Figure 7.6 shows the fitted curves, using \\(S\\left(t\\right)=\\exp\\left[-\\hat{\\lambda}_Xt\\right]\\) for group \\(X\\), along with the Kaplan Meier estimate of the survival curve. Although there isn’t much data, the exponential distribution looks to be an OK fit. For the Myeloid data we can do the same thing mC_my = sum((myeloid$death==1)&amp;(myeloid$trt==&quot;A&quot;)) mT_my = sum((myeloid$death==1)&amp;(myeloid$trt==&quot;B&quot;)) tsum_my_C = sum(myeloid$futime[myeloid$trt==&quot;A&quot;]) tsum_my_T = sum(myeloid$futime[myeloid$trt == &quot;B&quot;]) m_my = mT_my + mC_my tsum_my = tsum_my_C + tsum_my_T ## Can now plug these into LR test stat LRstat_my = 2*(mC_my*log(mC_my/tsum_my_C) + mT_my*log(mT_my/tsum_my_T) - m_my*log(m_my/tsum_my)) LRstat_my ## [1] 11.95293 Again, we refer this to \\(\\chi^2_1\\): 1-pchisq(LRstat_my, df=1) ## [1] 0.0005456153 This time we find that the difference is significant at even a very low level, and the 95% CI is given by ## [1] -3.028814e-04 -8.108237e-05 Confidence around \\(\\hat\\lambda_X\\) is high (ie. small SE), because of the large amount of data, but recall from last lecture (Fig 7.7) the fit is not good because of the inflexibility of the exponential distribution. We could also perform LR tests with the fitted Weibull distributions, but instead we will continue on through some more commonly used methods. 8.2 Non-parametric: the log-rank test The log-rank test is performed by creating a series of tables, and combining the information to find a test statistic. We work through each time \\(t_j\\) at which an event is observed (by which we mean a death or equivalent, not a censoring) in either of the groups. Notation: at time \\(t_j\\), \\(n_j\\) patients are ‘at risk’ of the event \\(d_j\\) events are observed (often the ‘event’ is death, so we will sometimes say this) For groups \\(C\\) and \\(T\\) we would therefore have a table representing the state of things at time \\(t_j\\), with this general form: Group No. surviving No. events No. at risk Treatment \\(n_{Tj}-d_{Tj}\\) \\(d_{Cj}\\) \\(n_{Cj}\\) Control \\(n_{Cj}-d_{Cj}\\) \\(d_{Tj}\\) \\(n_{Tj}\\) Total \\(n_j-d_j\\) \\(d_j\\) \\(n_j\\) Under \\(H_0\\), we expect the deaths (or events) to be distributed proportionally between groups \\(C\\) and \\(T\\), and so the expected number of events in group \\(X\\) (\\(C\\) or \\(T\\)) at time \\(t_j\\) is \\[e_{Xj} = n_{Xj}\\times{\\frac{d_j}{n_j}}.\\] This means that \\(e_{Cj}+e_{Tj} = d_{Cj} + d_{Tj} = d_j\\). If we take the margins of the table (by which we mean \\(n_j, \\,d_j\\,n_{Cj}\\) and \\(n_{Tj}\\)) as fixed, then \\(d_{Cj}\\) has a hypergeometric distribution. Definition 8.1 The hypergeometric distribution is a discrete probability distribution describing the probability of \\(k\\) successes in \\(n\\) draws (without replacement), taken from a finite population of size \\(N\\) that has exactly \\(K\\) objects with the desired feature. The probability mass function for a variable \\(X\\) following a hypergeometric function is \\[p\\left(X=k\\mid{K,N,n}\\right) = \\frac{{\\binom{K}{k}}{\\binom{N-K}{n-k}}}{{\\binom{N}{n}}}. \\] An example would be an urn containing 50 (\\(N\\)) balls, of which 16 (\\(K\\)) are green and the rest (34, \\(N-K\\)) are red. If we draw 10 (\\(n\\)) balls without replacement, \\(X\\) is the random variable whose outcome is \\(k\\), the number of green balls drawn. In the notation of the definition, the mean is \\[\\operatorname{E}\\left(X\\right)= n\\frac{K}{N} \\] and the variance is \\[\\operatorname{var}\\left(X\\right) = n\\frac{K}{N}\\frac{N-K}{N}\\frac{N-n}{N-1}.\\] In the notation of our table at time \\(t_j\\), we have \\[\\begin{align*} \\operatorname{E}\\left(d_{Cj}\\right) = e_{Cj} &amp; = n_{Cj}\\times{\\frac{d_j}{n_j}}\\\\ \\operatorname{var}\\left(d_{Cj}\\right) = v_{Cj} &amp;= \\frac{d_j n_{Cj}n_{Tj} \\left(n_j-d_j\\right)}{n_j^2\\left(n_j-1\\right)} \\end{align*}\\] With the marginal totals fixed, the value of \\(d_{Cj}\\) fixes the other three elements of the table, so considering this one variable is enough. Under \\(H_0\\), the numbers dying at successive times are independent, so \\[U = \\sum\\limits_{j}\\left(d_{Cj}-e_{Cj}\\right) \\] will (asymptotically) have a normal distribution, with \\[U \\sim N\\left(0,\\;\\sum\\limits_j v_{Cj}\\right). \\] We label \\(V = \\sum\\limits_jv_{Cj}\\), and in the log-rank test we refer \\(\\frac{U^2}{V}\\) to \\(\\chi^2_1\\). Simpler &amp; more widely used version: under \\(H_0\\), the expected number of events (eg. deaths) in group \\(X\\) is \\(E_X = \\sum\\limits_je_{Xj}\\), and the observed number is \\(O_X = \\sum\\limits_j d_{Xj}\\). The standard \\(\\chi^2\\) test formula can then be applied, and the test-statistic is \\[\\frac{\\left(O_C - E_C\\right)^2}{E_C} + \\frac{\\left(O_T - E_T\\right)^2}{E_T}.\\] It turns out that this test statistic is always smaller than \\(\\frac{U^2}{V}\\), so this test is slightly more conservative (ie. it has a larger p-value). Notice that for both of these test statistics, the actual difference between observed and expected is used, not the absolute difference. Therefore if the differences change in sign over time, the values are likely to cancel out (at least to some extent) and the log-rank test is not appropriate. Example 8.2 Let’s now perform a log-rank test on our data from Example 8.1. First, the ovarian cancer dataset. To do this, we can tabulate the key values at each time step. ## Time n_Cj d_Cj e_Cj n_Tj d_Tj e_Tj n_j d_j ## 1 59 13 1 0.5000000 13 0 0.5000000 26 1 ## 2 115 12 1 0.4800000 13 0 0.5200000 25 1 ## 3 156 11 1 0.4583333 13 0 0.5416667 24 1 ## 4 268 10 1 0.4347826 13 0 0.5652174 23 1 ## 5 329 9 1 0.4090909 13 0 0.5909091 22 1 ## 6 353 8 0 0.3809524 13 1 0.6190476 21 1 ## 7 365 8 0 0.4000000 12 1 0.6000000 20 1 ## 8 431 8 1 0.4705882 9 0 0.5294118 17 1 ## 9 464 6 0 0.4000000 9 1 0.6000000 15 1 ## 10 475 6 0 0.4285714 8 1 0.5714286 14 1 ## 11 563 5 0 0.4166667 7 1 0.5833333 12 1 ## 12 638 5 1 0.4545455 6 0 0.5454545 11 1 From this, we can find the \\(v_{j}\\) and the test statistic \\(\\frac{U^2}{V}\\): # Add up the differences UC = sum(logrank_df$d_Cj - logrank_df$e_Cj) ## Find the variance at each time t_j vCj_vec = sapply( 1:n_event, function(j){ nCj = logrank_df$n_Cj[j] nTj = logrank_df$n_Tj[j] dj = logrank_df$d_j[j] nj = logrank_df$n_j[j] (nCj*nTj*dj*(nj-1))/((nj^2)*(nj-1)) }) VC = sum(vCj_vec) cs_ov_stat = (UC^2)/VC 1-pchisq(cs_ov_stat, df=1) ## [1] 0.3025911 For the simpler, more conservative, version of the log-rank test, we have EC = sum(logrank_df$e_Cj) ET = sum(logrank_df$e_Tj) OC = sum(logrank_df$d_Cj) OT = sum(logrank_df$d_Tj) test_stat = ((EC-OC)^2)/EC + ((ET-OT)^2)/ET test_stat ## [1] 1.057393 and we can find the p-value by 1-pchisq(test_stat, df=1) ## [1] 0.3038106 As we expected, slightly larger, but not much different from the first version. These values are also pretty close to the results of our LR test in Example 8.1, where we had \\(p=0.291\\). Since the Myeloid dataset is much bigger, we won’t go through the rigmarole of making the table, but will instead use an inbuilt R function from the survival package (more on this in practicals). myeloid$trt = as.factor(myeloid$trt) survdiff(Surv(futime, death) ~ trt, data = myeloid, rho=0) ## Call: ## survdiff(formula = Surv(futime, death) ~ trt, data = myeloid, ## rho = 0) ## ## N Observed Expected (O-E)^2/E (O-E)^2/V ## trt=A 317 171 143 5.28 9.59 ## trt=B 329 149 177 4.29 9.59 ## ## Chisq= 9.6 on 1 degrees of freedom, p= 0.002 This time the p-value is quite far from the one we found using the likelihood ratio test (p=0.00055), further supporting the view that the likelihood ratio test was not appropriate because of the poor fit of the exponential distribution. 8.3 (Lecture 17) Semi-parametric: the proportional hazards model We’d like to be able to adjust our model for baseline covariates. It seems intuitively reasonable to suppose that factors like age, sex, disease status etc. might affect someone’s chances of survival (or whatever event we’re concerned with). The conventional way to do this is using a proportional hazards model, where we assume that \\[h_T\\left(t\\right) = \\psi h_C\\left(t\\right) \\] for any \\(t&gt;0\\) and for some constant \\(\\psi&gt;0\\). We call \\(\\psi\\) the relative hazard or hazard ratio. If \\(\\psi&lt;1\\) then the hazard at time \\(t\\) under treatment \\(T\\) is smaller than under control \\(C\\). If \\(\\psi&gt;1\\) then the hazard at time \\(t\\) is greater in greater in group \\(T\\) than in group \\(C\\). The important point is that \\(\\psi\\) doesn’t depend on \\(t\\). The hazard for a particular patient might be greater than for another, due to things like their age, disease history, treatment group and so on, but the extent of this difference doesn’t change over time. We adopt the concept of a baseline hazard function \\(h_0\\left(t\\right)\\), where for someone in group \\(C\\) (for now), their hazard at time \\(t\\) is \\(h_0\\left(t\\right)\\), and for someone in group \\(T\\) it is \\(\\psi h_0\\left(t\\right)\\). Since we must have \\(\\psi&gt;0\\), it makes sense to set \\[\\psi = e^{\\beta},\\] so that \\(\\beta = \\log\\psi\\) and \\(\\psi&gt;0\\;\\forall\\beta\\in\\mathbb{R}\\). Note that \\(\\beta&gt;0 \\iff \\psi&gt;1\\). We can now (re)-introduce our usual indicator variable \\(G_i\\), where \\[ G_i = \\begin{cases} 0\\text{ if participant }i\\text{ is in group }C\\\\ 1\\text{ if participant }i\\text{ is in group }T \\end{cases} \\] and model the hazard function for participant \\(i\\) as \\[h_i\\left(t\\right) = \\exp\\left[\\beta G_i\\right]h_0\\left(t\\right).\\] This is the proportional hazards model for the comparison of two groups. Now, the relative hazard is a function of the participant’s characteristics. Naturally, we can extend it to include other baseline covariates. as we have with linear models in ANCOVA, and with logistic regression. 8.3.1 General proportional hazards model Extending the model to include baseline covariates \\(B_1,\\ldots,B_p\\), we have \\[\\psi\\left(\\mathbf{x}_i\\right) = \\exp\\left(\\beta_0 G_i + \\beta_1b_{1i} + \\ldots + \\beta_p b_{pi}\\right) = \\exp\\left(\\mathbf{x}_i^T \\boldsymbol\\beta\\right),\\] where the first element of the vector \\(\\mathbf{x}_i\\) is \\(G_i\\), and the hazard function for participant \\(i\\) is \\[h_i\\left(t\\right) = \\psi\\left(\\mathbf{x}_i\\right)h_0\\left(t\\right). \\] Now, our baseline hazard function \\(h_0\\left(t\\right)\\) is the hazard function for a participant in group \\(C\\) for whom all baseline coviariates are either zero (if continuous) or the reference level (if a factor variable). For factor covariates this makes sense, since all levels are realistic values, but for continuous variables zero is likely to be unrealistic (for example you’d never expect zero for age, weight, height, blood pressure etc.). So, if any continuous variables are present, the baseline will always need to be adjusted, but if all covariates are factors, it is likely that the baseline hazard function will be applicable for some set of participants. The linear component \\(\\mathbf{x}_i^T\\boldsymbol\\beta\\) is often called the risk score or prognostic index for participant \\(i\\). We can rewrite the model as \\[\\log\\left(\\frac{h_1\\left(t\\right)}{h_0\\left(t\\right)}\\right) = \\mathbf{x}_i^T\\boldsymbol\\beta.\\] Notice that there is no constant in the linear term - if there was, it could just be absorbed into the baseline hazard function. There are various ways of fitting this model, but the method we will study (and the most widely used) is one developed by Cox (1972). 8.3.2 Cox regression The beauty of Cox regression is that it avoids specifying a form for \\(h_0\\left(t\\right)\\) altogether. To fit the model in Equation (??) we must estimate the coefficients \\(\\beta_0,\\ldots,\\beta_p\\). It also appears like we should estimate the baseline hazard \\(h_0\\left(t\\right)\\) somehow too, but the great advance made by Cox was to develop a method where this isn’t necessary. We don’t need to estimate \\(h_0\\left(t\\right)\\) to make inferences about the hazard ratio \\[\\frac{h_i\\left(t\\right)}{h_0\\left(t\\right)}.\\] We will estimate the coefficients \\(\\boldsymbol\\beta\\) using maximum likelihood, and so we’ll need to specify a likelihood function for the \\(\\boldsymbol\\beta\\), which will be a function of \\(\\mathbf{x}^T\\boldsymbol\\beta\\) and our observed data, the survival times \\(t_i\\). Suppose we have data for \\(n\\) participants, and that these include \\(m\\) complete observations (often referred to as deaths) and \\(n-m\\) right-censored survival times. Suppose also that all the complete observation times are distinct. Since time itself is continuous, this is always technically true, but in data the time will be rounded and so there may be multiple observations at one time. We can order these \\(m\\) event times \\[t_{(1)}&lt; t_{(2)} &lt; \\ldots &lt; t_{(m)},\\] such that \\(t_{(j)}\\) is the time of the \\(j^{\\text{th}}\\) event to be observed. At time \\(t_{(j)}\\), there will be some number of individuals who are ‘at risk’ of the event, because either their observation time or their censored survival time is greater than \\(t_{(j)}\\). The set of these individuals is the risk set, denoted \\(R\\left(t_{(j)}\\right)\\). Cox (1972) shows that the relevant likelihood function for the proportional hazards model in Equation (??) is \\[\\begin{equation} L\\left(\\boldsymbol\\beta\\right) = \\prod\\limits_{j=1}^m\\frac{\\exp\\left[\\mathbf{x}_{(j)}^T\\boldsymbol\\beta\\right]}{\\sum\\limits_{l\\in R\\left(t_{(j)}\\right)}{\\exp\\left[\\mathbf{x}_l^T\\boldsymbol\\beta\\right]}} \\tag{8.2} \\end{equation}\\] where \\(\\mathbf{x}_{(j)}\\) is the vector of covariates for the individual who dies (or equivalent) at time \\(t_{(j)}\\). Notice that the product is over only those individuals with complete observations, but individuals with censored data do contribute to the sum in the denominator. The numerator of the fraction inside the product in Equation (8.2) is the relative hazard for the person who actually did die at time \\(t_{(j)}\\). The denominator is the sum of the relative hazards for all those who possibly could have died at time \\(t_{(j)}\\) (the risk set \\(R\\left(t_{(j)}\\right)\\)). Very loosely: maximizing the likelihood means finding values for \\(\\boldsymbol\\beta\\) that mean the people who did die were ‘the most likely’ to die at the time they did. This is not a true likelihood, since it depends only on the ordering of the data and not the data itself; it is a partial likelihood. Justification: because \\(h_0\\left(t\\right)\\) has an arbitrary form, it’s possible that except for at these observed times, \\(h_0\\left(t\\right)=0\\), and therefore \\(h_i\\left(t\\right)=0\\). This means the intervals between successive observations convey no information about the effect of the covariates on hazard, and therefore about the \\(\\boldsymbol\\beta\\) parameters. If you want to know more detail about how this likelihood was derived, you can find in in Section 3.3 of Collett (2003b), or in Cox’s original paper (Cox (1972)). Moving on, if we set \\[ \\delta_i = \\begin{cases} 0\\;\\;\\text{ if individual }i\\text{ is censored}\\\\ 1\\;\\;\\text{ if individual }i\\text{ is observed} \\end{cases} \\] then we can write Equation (8.2) as \\[L\\left(\\boldsymbol\\beta\\mid{\\text{data}}\\right) = \\prod\\limits_{i=1}^n\\left(\\frac{\\exp\\left[\\mathbf{x}_i^T\\boldsymbol\\beta\\right]}{\\sum\\limits_{l\\in R\\left(t_i\\right)}{\\exp\\left[\\mathbf{x}_l^T\\boldsymbol\\beta\\right]}}\\right)^{\\delta_i},\\] where \\(R\\left(t_i\\right)\\) is the risk set at time \\(t_i\\). From this we can find the log-likelihood \\[\\ell\\left(\\boldsymbol\\beta\\mid{\\text{data}}\\right) = \\sum\\limits_{i=1}^n \\delta_i\\left[\\mathbf{x}_i^T\\boldsymbol\\beta - \\log\\sum\\limits_{l\\in R\\left(t_i\\right)}\\exp\\left(\\mathbf{x}_l^T\\boldsymbol\\beta\\right)\\right].\\] The MLE \\(\\hat{\\boldsymbol\\beta}\\) is found using numerical methods (often Newton-Raphson, which you’ll have seen if you did Numerical Analysis II). How can we tell if a proportional hazards model is appropriate? We can’t easily visualise the hazard function for a dataset, and instead would plot the survival curve. So can we tell if the proportional hazards assumption is met by looking at the survival curve? If two hazard functions are proportional, their survival functions won’t cross one another. Necessary but not sufficient! Suppose \\(h_C\\left(t\\right)\\) is the hazard at time \\(t\\) for an individual in group \\(C\\), and \\(h_T\\left(t\\right)\\) is the hazard for that same individual in group \\(T\\). If the two hazards are proportional then we have \\[h_C\\left(t\\right) = \\psi h_T\\left(t\\right) \\] for some constant \\(\\psi\\). Recall from Section 7.2 that \\[h\\left(t\\right) = \\frac{f\\left(t\\right)}{S\\left(t\\right)},\\] where \\(S\\left(t\\right)\\) is the survival function \\[S\\left(t\\right) = 1 - \\int\\limits_0^t f\\left(u\\right)du, \\] and \\(f\\left(t\\right)\\) is the probability density of \\(T\\). We can therefore write \\[h\\left(t\\right) = -\\frac{d}{dt}\\left[\\log\\left(S\\left(t\\right)\\right)\\right]\\] and rearrange this to \\[\\begin{equation} S\\left(t\\right) = \\exp \\left(-H\\left(t\\right)\\right) \\tag{8.3} \\end{equation}\\] where \\[H\\left(t\\right) = \\int\\limits_0^t h\\left(u\\right) du.\\] Therefore for our two hazard functions, we have \\[\\exp\\left\\lbrace - \\int\\limits_0^t h_C\\left(u\\right)du \\right\\rbrace =\\exp\\left\\lbrace -\\int\\limits_0^t\\psi h_T\\left(u\\right) du \\right\\rbrace \\] From Equation (8.3) we see that therefore \\[S_C\\left(t\\right) = \\left[S_T\\left(t\\right)\\right]^\\psi.\\] Since the survival function is always between 0 and 1, we can see that the value of \\(\\psi\\) determines whether \\(S_C\\left(t\\right)&lt;S_T\\left(t\\right)\\) (if \\(\\psi&gt;1\\)) or \\(S_C\\left(t\\right)&gt;S_T\\left(t\\right)\\) (if \\(0&lt;\\psi&lt;1\\)). The important thing is that the survival curves will not cross. This is an informal conclusion, and lines not crossing is a necessary condition but not a sufficient one. There are some more formal tests that can be conducted to assess the proportional hazards assumption, but we won’t go into them here. Example 8.3 First of all, we can use Cox regression adjusted only for the Group (or treatment arm) of the participants. For the ovarian dataset coxph(formula = Surv(futime, fustat)~rx, data=ovarian) ## Call: ## coxph(formula = Surv(futime, fustat) ~ rx, data = ovarian) ## ## coef exp(coef) se(coef) z p ## rx -0.5964 0.5508 0.5870 -1.016 0.31 ## ## Likelihood ratio test=1.05 on 1 df, p=0.3052 ## n= 26, number of events= 12 and for the myeloid1 dataset coxph(formula = Surv(futime, death)~trt, data=myeloid) ## Call: ## coxph(formula = Surv(futime, death) ~ trt, data = myeloid) ## ## coef exp(coef) se(coef) z p ## trtB -0.3457 0.7077 0.1122 -3.081 0.00206 ## ## Likelihood ratio test=9.52 on 1 df, p=0.002029 ## n= 646, number of events= 320 We see that for both results, our p-values are close to what we have found with the log rank test. We can now account for more baseline covariates. For the ovarian data we can include age and resid.ds (whether residual disease is present): coxph(formula = Surv(futime, fustat)~rx+age+resid.ds, data=ovarian) ## Call: ## coxph(formula = Surv(futime, fustat) ~ rx + age + resid.ds, data = ovarian) ## ## coef exp(coef) se(coef) z p ## rx -0.8489 0.4279 0.6392 -1.328 0.18416 ## age 0.1285 1.1372 0.0473 2.718 0.00657 ## resid.ds 0.6964 2.0065 0.7585 0.918 0.35858 ## ## Likelihood ratio test=16.77 on 3 df, p=0.0007889 ## n= 26, number of events= 12 What this shows is that the most significant factor by far is the particpant’s age, with the hazard function increasing as age increases. The coefficient for treatment group (rx) has increased in magnitude and the p-value has decreased now that age is being adjusted for (although it is still not significant). We can do the same for the myeloid data: coxph(formula = Surv(futime, death)~trt+sex, data=myeloid) ## Call: ## coxph(formula = Surv(futime, death) ~ trt + sex, data = myeloid) ## ## coef exp(coef) se(coef) z p ## trtB -0.3582 0.6989 0.1129 -3.174 0.00151 ## sexm 0.1150 1.1219 0.1128 1.020 0.30782 ## ## Likelihood ratio test=10.56 on 2 df, p=0.005093 ## n= 646, number of events= 320 We see that the only covariate we have, sex has very little effect. This concludes our section on survival data, but we will revisit the topic in the next computer practical. References ———. 2003b. Modelling Survival Data in Medical Research. 2nd ed. Texts in Statistical Science. Chapman &amp; Hall. Cox, David R. 1972. “Regression Models and Life-Tables.” Journal of the Royal Statistical Society: Series B (Methodological) 34 (2): 187–202. "],["cluster-rct.html", "9 Cluster randomised trials 9.1 What is a cluster RCT? 9.2 Sample size 9.3 Allocation 9.4 Analysing a cluster RCT", " 9 Cluster randomised trials For the trials we’ve been studying so far, the intervention is applied at an individual level. Often realistic, for example a medicine, injection or operation. However, for some treatments this is not practical, eg. a new cleaning regime in operating theatres. Almost impossible to implement this if different patients within the same hospital would be allocated to different cleaning styles. Logistically it would be very difficult, and there would likely be contamination as staff may be reluctant to clean an operating theatre in what might now seem an inferior way, for a control participant. In general it is very difficult (if not impossible) to implement changes in practice across healthcare systems at an individual level. The solution to this is to work at the group level, rather than the individual level. 9.1 What is a cluster RCT? In a cluster RCT, participants within the same natural group (eg. doctor’s surgery, hospital, school, classroom,…) are all allocated to the same group together. This means that, in the cleaning example above, the staff at a hospital in the treatment group can be trained in the new practice, all patients at that hospital will ‘receive the new treatment’, and contamination between groups is minimised. Main issue: participants within the same group are often likely to be more similar to one another than to someone in a different group. We expect that each group has its own ‘true’ mean \\(\\mu_k\\) , which is different from the underlying population mean \\(\\mu\\), and that the cluster means are distributed with mean \\(\\mu\\) and variance \\(\\sigma^2_b\\) (more on \\(\\sigma^2_b\\) soon). This violates one of the key assumptions we’ve held so far, that the data are independent, and leads us to a very important quantity called the intracluster correlation (ICC). 9.1.1 Intracluster correlation The ICC quantifies the relatedness of data that are clustered in groups by comparing the variance within groups by the variance between groups. \\[ICC = \\frac{\\sigma^2_b}{\\sigma^2_b + \\sigma^2_w}, \\] where \\(\\sigma^2_b\\) is the variance between groups and \\(\\sigma^2_w\\) is the variance within groups. \\(\\sigma_w^2=0\\;\\implies\\;ICC=1\\) and all measurements within each group are the same. At the other extreme, where * \\(\\sigma^2_b=0\\;\\implies\\;ICC=0\\) and all groups are independent and identically distributed. We can estimate \\(\\sigma_w^2\\) and \\(\\sigma_b^2\\) using \\(s_w^2\\) and \\(s_b^2\\), which we find by decomposing the pooled variance. Here, \\(g\\) is the number of groups, \\(n_j\\) is the number of participants in group \\(j\\) and \\(n\\) is the total number of participants. \\[ s^2_{Tot} = \\frac{\\sum\\limits_{j=1}^g \\sum\\limits_{i=1}^{n_j}\\left(x_{ij}-\\bar{x}\\right)^2}{n-g}\\\\ \\] We can split this up as \\[ \\begin{aligned} s^2_{Tot} &amp; = \\frac{1}{n-g}\\sum\\limits_{j=1}^g \\sum\\limits_{i=1}^{n_j}\\left(x_{ij} - \\bar{x}_j + \\bar{x}_j-\\bar{x}\\right)^2\\\\ &amp; = \\frac{1}{n-g}\\sum\\limits_{j=1}^g \\sum\\limits_{i=1}^{n_j}\\left[\\left(x_{ij} - \\bar{x}_j\\right)^2 + \\left(\\bar{x}_j-\\bar{x}\\right)^2 + 2\\left(x_{ij} - \\bar{x}_j\\right)\\left(\\bar{x}_j-\\bar{x}\\right)\\right]\\\\ &amp; = \\frac{1}{n-g}\\sum\\limits_{j=1}^g \\sum\\limits_{i=1}^{n_j}\\left[\\left(x_{ij} - \\bar{x}_j\\right)^2 + \\left(\\bar{x}_j-\\bar{x}\\right)^2 \\right]\\\\ &amp;= \\underbrace{\\frac{1}{n-g}\\sum\\limits_{j=1}^g \\sum\\limits_{i=1}^{n_j}\\left(x_{ij} - \\bar{x}_j\\right)^2}_{\\text{Within groups}} + \\underbrace{\\frac{1}{n-g}\\sum\\limits_{j=1}^g n_j\\left(\\bar{x}_j - \\bar{x}\\right)^2}_{\\text{Between groups}} \\end{aligned} \\] Example 9.1 We will demonstrate the ICC using a dataset that has nothing to do with clinical trials. The cheese dataset contains the price per unit and volume of sales of cheese (who knows what kind) at many Kroger stores in the US. We also know which city the Krogers are in, and we have data for 706 stores across 11 cities. It might be reasonable to expect that if we have information about the price and sales volume for several stores within a particular city, this gives us more information about the price and volume for another store in that same city than for a store in another city. Figure 9.1 shows the price and volume for all stores, coloured by city. cheese = read.csv(&quot;kroger.csv&quot;, header=T) cheese$city = as.factor(cheese$city) ggplot(data=cheese, aes(x=price, y=vol, col=city)) + geom_point() Figure 9.1: Price per unit and volume of sales of cheese for 706 Kroger stores. To calculate the ICC we define two functions, between.var and within.var, to calculate the between group and within group variance, as explained above. Click to show R functions # Firstly we define functions for the estimates between.var = function( data, groupvec ){ groups = levels(as.factor(groupvec)) ng = length(groups) ntot = length(data) means = sapply(1:ng, function(i){mean(data[groupvec == groups[i]])}) njvec = sapply(1:ng, function(i){length(data[groupvec == groups[i]])}) mean = mean(data) ssqvec = sapply(1:ng, function(i){(njvec[i]*(means[i]-mean)^2)}) sum(ssqvec)/(ntot-ng) } within.var = function( data, groupvec ){ groups = levels(as.factor(groupvec)) ng = length(groups) ntot = length(data) means = sapply(1:ng, function(i){mean(data[groupvec == groups[i]])}) njvec = sapply(1:ng, function(i){length(data[groupvec == groups[i]])}) g_sums = rep(NA, ng) for (j in 1:ng){ data_j = data[groupvec == groups[j]] ssqvec = rep(NA, njvec[j]) for (i in 1:njvec[j]){ ssqvec[i] = (data_j[i] - means[j])^2 } g_sums[j] = sum(ssqvec)/(ntot - ng) } sum(g_sums) } ## Now we can calculate them bvar = between.var(iris$Sepal.Length, iris$Species) bvar ## [1] 0.4300145 wvar = within.var(iris$Sepal.Length, iris$Species) wvar ## [1] 0.2650082 ## And find the ICC: icc = bvar/(bvar+wvar) icc ## [1] 0.6187057 wv_price = within.var(cheese$price, cheese$city) bv_price = between.var(cheese$price, cheese$city) icc = bv_price / (bv_price + wv_price) icc ## [1] 0.253104 We can see that for each city we know a fair amount about the cost of cheese. But, if we had to predict the price of cheese in a new city, for which we have no data, we would have no idea what the mean for that city would be, apart from that it would come from \\(N\\left(\\mu,\\;\\sigma^2_b\\right)\\), where \\(\\mu\\) is the mean price of cheese in the overall population and \\(\\sigma^2_b\\) is the between group variance. Estimating the ICC when planning a study is an important step, but isn’t always easy. Sometimes estimated from existing data, which is likely to cover many sites. In non-medical studies like education or social interventions (where cluster RCTs are very common), it can be much more difficult because there is generally less data. Statistical studies are much newer in these areas, though they are becoming increasingly common, and even mandated by some organisations (for example the Educational Endowment Foundation). 9.2 Sample size The upshot of the non-independence of the sample is that we have less information from \\(n\\) participants in a cluster RCT than we would do for an individual-based RCT where all the participants were independent (at least conditional on some covariates). At extremes: ICC=0: same as a normal RCT. ICC=1, all measurements within a cluster are identical, and to achieve the same power as with \\(n\\) particpants in a standard RCT, we would need \\(n\\) clusters (and their size would be irrelevant). Obviously neither of these is ever true! In most studies, the ICC is in \\(\\left(0,\\;0.15\\right)\\). We will work through the sample size (and indeed most other things) for a cluster RCT in which the outcome is continuous (as in Chapter 2), but you can equally do a cluster RCT with a binary or time-to-event outcome. The first step is to think about how the clustering affects the variance. An estimate of the outcome variance in the control group, ignoring the clustering, is \\[\\begin{equation} \\hat{\\sigma}^2 = \\frac{\\sum\\limits_{j=1}^g\\sum\\limits_{i=1}^{n_j}\\left(X_{ij} - \\bar{X}\\right)^2}{n-1}, \\tag{9.1} \\end{equation}\\] where as before there are \\(g\\) clusters, cluster \\(j\\) has size \\(n_j\\) and \\(\\sum\\limits_{j=1}^gn_j=n\\) is the total sample size. The mean is also calculated without reference to the clustering, so \\[\\bar{X} = \\frac{\\sum\\limits_{j=1}^g\\sum\\limits_{i=1}^{n_j}X_{ij}}{n}.\\] It can be shown that the variance of the overall mean in either group is inflated by a factor of \\[1 + \\rho\\left[ \\frac{\\sum\\limits_j n_j^2}{N} - 1 \\right]. \\] This quantity is known as the design effect. Notice that if all the groups are the same size \\(n_g = \\frac{n}{g}\\) then the design effect simplifies to \\[1 + \\rho\\left(n_g - 1\\right).\\] We will assume from now on that this is the case. 9.2.0.1 A formula for sample size Now that we know \\(\\operatorname{E}\\left(\\hat{\\sigma}^2\\right)\\) we can adapt our sample size formula from Section 2.5. For an individual-level RCT with a continuous outcome, we had \\[\\begin{equation} n = \\frac{2\\sigma^2\\left(z_{\\beta} + z_{\\alpha/2}\\right)^2}{\\tau^2_M}, \\tag{9.2} \\end{equation}\\] and the reason we were able to do this was because the variance of the treatment effect estimate was \\(\\sigma^2/n\\). For a cluster RCT, the variance of the treatment effect is \\[\\begin{equation} \\frac{\\sigma^2}{n} \\left[1 + \\rho\\left(\\frac{\\sum\\limits_{j=1}^g n_j}{n}-1\\right)\\right] \\tag{9.3} \\end{equation}\\] At the planning stage of a cluster RCT we are unlikely to know the size of each cluster, so usually specify a [conservative] average cluster size \\(n_g\\). each individual involved will usually need to give their consent, so knowing the size of the hospital / GP surgery / class is not enough. In this case, the variance of the treatment effect in Equation (9.3) becomes \\[\\begin{equation} \\frac{\\sigma^2}{n} \\left[1 + \\rho\\left(n_g-1\\right)\\right] . \\tag{9.4} \\end{equation}\\] Equation (9.4) can be combined with Equation (9.2) to give the sample size formula for a cluster RCT: \\[n = \\frac{2\\sigma^2\\left[1 + \\rho\\left(n_g-1\\right)\\right]\\left(z_{\\beta} + z_{\\alpha/2}\\right)^2}{\\tau^2_M}. \\] Since \\(n=n_gg\\), this can be rearranged to find the number of clusters of a given size needed, or the size of cluster if a given number of clusters is to be used. The sample size (and therefore the real power of the study) depends on two additional quantities that are generally beyond our control, and possibly knowledge: ICC and \\(n_g\\). It is therefore sensible to conduct some sensitivity analysis, with several scenarios of ICC and \\(n_g\\), to see what the implications are for the power of the study if things don’t quite go to plan. Example 9.2 This is something I wrote for an education study I’m involved in (funded by the EEF) where the treatment is a particular way of engaging 2 year olds in conversation. The outcome variable is each child’s score on the British Picture Vocabulary Scale (BPVS), a test aimed at 3 - 16 year olds designed to assess each child’s vocabulary. We needed to recruit some number of nurseries, but had very little information about the IIC (notice that the age in our study is outside the intended range of the BPVS test!). To help the rest of the evaluation team understand the sensitivity of the power of the study to various quantities, I designed this dashboard, so that they could play around with the variables and see the effect. As well as giving the sample size for a simple t-test (as we’ve done above) it also shows the size for a random effects model (similar to ANCOVA, more on this soon), which is why the baseline-outcome correlation (about which we also know very little!) is included. The plot shows the minimum detectable effect size (MDES, or \\(\\tau_M\\), in SD units), since the evaluation team wanted to know what size of effect we could find with our desired power. 9.3 Allocation In a cluster RCT, everyone within a particular cluster will be in the same group (\\(T\\) or \\(C\\)). Therefore, the allocation needs to be performed at the cluster level, rather than at the individual level as we did in Chapter ??. In theory we could use any of the first few methods we learned (simple random sampling, random permuted blocks, biased coin, urn design) to allocate the clusters. However, there are often relatively few clusters, and so the potential for imbalance in terms of the nature of the clusters would be rather high. This means we are more likely to use a stratified method or minimisation. In terms of prognostic factors, there are now two levels: cluster level and individual level. Eg, with GP practices as clusters: cluster-level covariates: size of the practice, rural or urban, the IMB (index of mass deprivation) … It would be sensible to make sure there was balance in each of these in the allocation. Could include aggregates of individual-level characteristics: mean age, or proportion of people with a particular condition, … (especially if the study relates to a particular condition). However, a key feature of cluster RCTs means that in fact some different, and perhaps more effective, allocation methods are open to us. 9.3.1 Allocating everyone at once The methods we’ve covered so far assume that participants are recruited sequentially, and begin the intervention at different points in time. When allocating participant \\(n\\), we only know the allocation for participants \\(1,\\ldots,n-1\\). It is very likely that we don’t know the details of the following participants, in particular their values of any prognostic variables. This makes sense in many medical settings, where a patient would want to begin treatment as soon as possible, and there may be a limited number of patients with particular criteria at any one time. However, cluster RCTs rarely deal with urgent conditions (at least in the sense of administering a direct treatment), and so the procedure is usually that the settings (the clusters) are recruited over some recruitment period and all begin the intervention at the same time. This means that at the point of allocation, the details of all settings involved are known. There are a couple of proposals for how to deal with allocation in this scenario, and we will look at one now. 9.3.2 Covariate constrained randomization This method is proposed in (dickinson2015pragmatic?), and implemented in the R package cvcrand. We’ll review the key points of the method, but if you’re interested you can find the details in the article. Baseline information must be available for all settings, for any covariate thought to be potentially important. These can be setting-level variables or aggregates of individual-level variables. Once all this data has been collected, the randomisation procedure is as follows. Allocation procedure: Generate all possible allocations of the clusters into two arms (\\(T\\) and \\(C\\)). Rule out all allocations that don’t achieve the desired balance criteria to leave optimal set. Choose one allocation at random from the optimal set Balance criteria: Factor covariates: eg. want groups C and T to have same number (or close) of rural GP practices Continuous covariates: standardised (to mean 0, SD 1) and used to calculate a ‘Balance score’ 9.4 Analysing a cluster RCT As with the other stages of a cluster RCT, to conduct an effective and accurate analysis we need to take into account the clustered nature of the data. There are several ways to do this, and we will whizz through the main ones now. Example 9.3 Data from an educational trial, contained in crtData in the package eefAnalytics, shown in Figure 9.2 . The dataset contains 22 schools and 265 pupils in total. Each school was assigned to either 1 (group \\(T\\)) or 0 (group \\(C\\)). Each pupil took a test before the trial, and again at the end of the trial. We also know the percentage attendance for each pupil. We will use this data to demonstrate each method. ## &#39;data.frame&#39;: 265 obs. of 4 variables: ## $ School : Factor w/ 22 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ Posttest : num 16 13 18 14 25 13 23 26 16 8 ... ## $ Intervention: Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ Prettest : num 1 4 5 4 5 2 5 5 2 2 ... ggplot(data=crt_df, aes(y=Posttest, fill=Intervention, group = School)) + geom_boxplot() Figure 9.2: Box plots of the outcome Posttest for each school, coloured by Intervention. 9.4.1 At the cluster level Idea: aggregate data to the cluster level, so that for each cluster (school, in our example), there is effectively one data point. Advantages: Fast Simple methodology (eg. t-test) Disadvantages: Loses a lot of information Not appropriate if group sizes vary (the larger group \\(j\\), the smaller the SE of \\(\\bar{x}_j\\)) There are methods designed to account for this, such as the weighted t-test, but these methods are generally inefficient and less robust. These methods are generally thought to be appropriate for fewer than 15-20 clusters per treatment arm. One possibility for our trial would be to collect the mean and SD of scores within each school, and perform a t-test to find out if there is a significant difference between the intervention and control arms. If we wanted to find out whether this depended on, say, gender, we could split the data set and perform seperate t-tests for the different gender groups. This has the advantage that it is simple to implement, but the disadvantage that it is difficult to take into account covariates (apart from in the simple way discussed for eg. gender). With a small study, it is likely that there is some imbalance in the design in terms of covariates. The required sample size for this option would be \\[ g = \\frac{2 \\sigma^2 \\left[1+ \\left(n_g-1\\right)\\rho_{icc}\\right]}{n_g\\tau^2_M}\\left(z_{\\beta} + z_{\\frac{1}{2}\\alpha}\\right)^2, \\] where \\(n_g\\) is the average cluster size and \\(g\\) is the number of clusters per treatment arm. This is the value we worked out in Section 9.2.0.1 Example 9.4 We can perform this analysis on our schools data. The first step is to calculate the difference between Posttest and Pretest for each pupil (we could also use just Posttest). crt_df$diff = crt_df$Posttest - crt_df$Prettest We can then aggregate this to find the mean of diff for each school, shown in Table 9.1: Table 9.1: EEF data with difference calculated (first 10 rows). School MeanDiff ng Group 1 13.76923 13 1 2 19.09091 33 1 3 20.43333 30 1 4 19.00000 30 0 5 16.66667 15 1 6 14.40000 5 1 7 16.75000 24 1 8 13.83333 12 0 9 15.00000 4 0 10 18.28571 14 1 We can also visualise the mean differences by group Figure 9.3: Boxplots of the mean differences for each trial arm From Figure 9.3 it certainly looks likely that a significant difference will be found. t.test( x=crt_summ$MeanDiff[crt_summ$Group==0], y=crt_summ$MeanDiff[crt_summ$Group==1], alternative = &quot;two.sided&quot;, paired = F, var.equal=F ) ## ## Welch Two Sample t-test ## ## data: crt_summ$MeanDiff[crt_summ$Group == 0] and crt_summ$MeanDiff[crt_summ$Group == 1] ## t = -2.2671, df = 19.983, p-value = 0.03463 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -6.4133879 -0.2667059 ## sample estimates: ## mean of x mean of y ## 13.72454 17.06459 This is fairly easy to implement, but it seems rather unsatisfactory. It’s also probably not entirely appropriate because the group sizes vary from one (!) to 33. What we need is a linear (in the continuous outcome case at least) model that takes into account the covariates, and makes the most of the available data. 9.4.2 At the individual level: mixed effects models Mixed effects model or random effects model, or multilevel models. §used when it cannot be assumed that the outputs are all independent of one another. In the cluster randomized trial setting, this is because outcomes for participants within the same cluster can be expected to be more highly correlated than outcomes for patients from different clusters, but we will see examples of other designs in the next lecture. To understand mixed effects models, we need to think about the difference between fixed effects and random effects. These are two different types of factor variables. Fixed effects These are the sorts of factor variables we’re used to dealing with in linear models: we don’t assume any relationship between the levels we are generally interested in comparing the groups or categories represented by the fixed effect factors. We’ve seen these throughout the course, most notably with the treatment group variable \\(G_i\\), but also with things like smoking history, sex, disease details. When conducting a clinical trial, we’re likely to try to include participants with all levels of the fixed effects we’re interested in, so that we can make inferences about the effect of the different levels of those effects. Random effects Random effects are probably just as common in real life, but we haven’t seen them yet. Factor variables that we think of as being drawn from some underlying model or distribution. For example, this could be GP surgery or school class, or an individual. We expect each GP surgery / school class / individual to behave slightly differently (depending on the extent of the intracluster correlation) but to behave as though from some overall distribution. Unlike fixed effects, random effects are generally things we aren’t specifically interested in understanding the effect of, but we want to account for the variation they bring. We’re also unable to include all levels of the random effect in our study - for example, a study looking at the effect of an intervention in schools will involve perhaps 50 schools, but we want to apply to results to all schools in the UK (say). We therefore assume that the schools are drawn from some normal distribution (in terms of the outcome we’re interested in), and that therefore all the schools we haven’t included also belong to this distribution. In Example 9.1 we aren’t trying to compare different cities, and we certainly don’t have data for Kroger stores in all cities, but we’re assuming that the mean cheese price \\(\\mu_{\\text{city}_i}\\) in the different cities is drawn from \\(N\\left(\\mu,\\,\\sigma^2_B\\right)\\), and that within each city the cheese price is drawn from \\(N\\left(\\mu_{\\text{city}_i},\\;\\sigma^2_W\\right)\\). Including random effects allows us to account for the fact that some schools might be in general a bit better / worse performing, or some individuals might be a bit more / less healthy, because of natural variation. We will see more examples of the use of random effects in the next lecture. The mixed effects model Mixed effects models allow us to combine fixed effects and random effects. We’ll look at them next lecture too, because they are useful for many more situations than cluster RCTs, but this is as good a place as any to start! \\[\\begin{equation} x_{ijk} = \\alpha + \\beta G_i + \\sum\\limits_l \\gamma_l z_{ijkl} + u_{ij} + v_{ijk} \\tag{9.5} \\end{equation}\\] where \\(x_{ijk}\\) is the outcome for the \\(k\\)-th individual in the \\(j\\)-th cluster in the \\(i\\)-th treatment arm (usually \\(i=0\\) is the control arm and \\(i=1\\) is the intervention arm) \\(\\alpha\\) is the intercept of the model \\(\\beta\\) is the intervention effect, and \\(G_i\\) the group indicator variable (0 for group \\(C\\), 1 for group \\(T\\)). Our null hypothesis is that \\(\\beta\\) is also zero) The \\(z_{ijkl}\\) are \\(L\\) different individual level covariates that we wish to take into account, and the \\(\\gamma_l\\) are the estimated coefficients. \\(u_{ij}\\) is a random effect relating to the \\(j\\)-th cluster in the \\(i\\)-th treatment arm. This is the term that accounts for the between-cluster variation. We assume \\(u_{ij}\\) is normally distributed with mean 0 and variance \\(\\sigma^2_B\\) (the between-cluster variance). \\(v_{ijk}\\) is a random effect relating to the \\(k\\)-th individual in the cluster (ie. an individual level random error term), assumed normally distributed with mean 0 and variance \\(\\sigma^2_W\\) (the within-cluster variance). The part of the model that makes this particularly suitable to a cluster randomized trial is \\(u_{ij}\\). Notice that this has no \\(k\\) index, and is therefore the same for all participants within a particular cluster. With a random effects model we can take into account the effects of individual-level covariates and also the clustered design of the data. Approximately, our sample size requirements are \\[ k = \\frac{2 \\sigma^2 \\left[1+ \\left(m-1\\right)\\rho_{icc}\\right]\\left(1-\\rho^2\\right) }{m\\tau^2_M}\\left(z_{\\beta} + z_{\\frac{1}{2}\\alpha}\\right)^2. \\] Broadly this follows on from the logic we used to show the reduction in variance from the ANCOVA model in Section 4.3.1.2, and you’ll notice that the factor of \\(1-\\rho^2\\) is the same. The details for cluster randomized trials are given in (teerenstra2012simple?). This is the ‘Random effects model’ line in the shiny dashboard.) The random effects model is more suitable when there are more than around 15-20 clusters in each arm. Example 9.5 We’ll now fit a random effects model to the crtData dataset from eefAnalytics. A good starting point is to plot the data with this in mind, to see what we might expect. Figure 9.4: Posttest against Prettest, coloured by Intervention From Figure 9.4 it appears there might be a positive relationship between Prettest and Posttest, and also that the Posttest scores might be higher in the intervention group. We’ll do this using the R package lme4. The function to specify a linear mixed effects model is called lmer, and works very similarly to lm. The term (1|School) tells R that the variable School should be treated as a random effect, not a fixed effect. library(lme4) library(sjPlot) lmer_eef1 = lmer(Posttest ~ Prettest + Intervention + (1|School), data=crt_df ) The package sjPlot contains functions to work with mixed effect model objects, for example plot_model sjPlot::plot_model(lmer_eef1) Figure 9.5: CIs for the model coefficients for the mixed effects model of crtData. and tab_model   Posttest Predictors Estimates CI p (Intercept) 11.23 9.01 – 13.44 &lt;0.001 Prettest 1.79 1.39 – 2.18 &lt;0.001 Intervention [1] 3.11 0.73 – 5.49 0.011 Random Effects σ2 14.78 τ00 School 5.67 ICC 0.28 N School 22 Observations 265 Marginal R2 / Conditional R2 0.256 / 0.462 Perhaps unsurprisingly, the coefficient of the baseline test score is very significant, and the intervention also has a significant effect. This function also estimates the intracluster correlation, which we see is 0.28. ::: There are various different ways we can include random effects in the model, as shown in Figure 9.6. In our EEF data example we have used a fixed slope and fixed intercept. Figure 9.6: Different ways of including random effects in a mixed effects model. The mixed effects model can be extended to a generalized mixed effects model, which is akin to a generalized linear model. For example, with a binary outcome \\(X_{ijk}\\) we can adopt the model \\[ \\operatorname{logit}\\left(\\pi_{ijk}\\right) = \\alpha + \\beta G_i + \\sum\\limits_l \\gamma_l z_{ijkl} + u_{ij} + v_{ijk}. \\] We will look in the next lecture at some more trial designs for which mixed effects models are useful. "],["references.html", "References", " References This sections lists the references used in the course - it will be updated as the notes are updated. Some of the more accessible (dare I say ‘interesting’) resources are linked from the notes. If you want to read any of these articles, the easiest way is to copy the title into Google scholar. al, Geoffrey Marshall et. 1948. “STREPTOMYCIN TREATMENT OF PULMONARY TUBERCULOSIS a MEDICAL RESEARCH COUNCIL INVESTIGATION.” British Medical Journal. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2091872/pdf/brmedj03701-0007.pdf. Altman, Douglas G. 1990. Practical Statistics for Medical Research. CRC press. ———. 1998. “Confidence Intervals for the Number Needed to Treat.” Bmj 317 (7168): 1309–12. Collett, David. 2003a. Modelling Binary Data. 2nd ed. Texts in Statistical Science. Chapman &amp; Hall. ———. 2003b. Modelling Survival Data in Medical Research. 2nd ed. Texts in Statistical Science. Chapman &amp; Hall. Cottingham, Marci D, and Jill A Fisher. 2022. “Gendered Logics of Biomedical Research: Women in US Phase i Clinical Trials.” Social Problems 69 (2): 492–509. Cox, David R. 1972. “Regression Models and Life-Tables.” Journal of the Royal Statistical Society: Series B (Methodological) 34 (2): 187–202. Edmonson, John H, Thomas R Fleming, David G Decker, George D Malkasian, Edward O Jorgensen, John A Jefferies, Maurice J Webb, and Larry K Kvols. 1979. “Prognosis in Advanced Ovarian Carcinoma Versus Minimal Residual.” Cancer Treatment Reports 63 (2): 241–47. Fentiman, Ian S, Robert D Rubens, and John L Hayward. 1983. “Control of Pleural Effusions in Patients with Breast Cancer a Randomized Trial.” Cancer 52 (4): 737–39. Freedman, LS, and Susan J White. 1976. “On the Use of Pocock and Simon’s Method for Balancing Treatment Numbers over Prognostic Factors in the Controlled Clinical Trial.” Biometrics, 691–94. Health, National Institute of. 2023. “History of Women’s Participation in Clinical Research.” Office of Research on Women’s Health. https://orwh. od. nih. gov/toolkit …. https://orwh.od.nih.gov/toolkit/recruitment/history. Hommel, EHEBMJ, Hans-Henrik Parving, Elisabeth Mathiesen, Berit Edsberg, M Damkjaer Nielsen, and Jørn Giese. 1986. “Effect of Captopril on Kidney Function in Insulin-Dependent Diabetic Patients with Nephropathy.” Br Med J (Clin Res Ed) 293 (6545): 467–70. Kallis, P, JA Tooze, S Talbot, D Cowans, DH Bevan, and T Treasure. 1994. “Pre-Operative Aspirin Decreases Platelet Aggregation and Increases Post-Operative Blood Loss–a Prospective, Randomised, Placebo Controlled, Double-Blind Clinical Trial in 100 Patients with Chronic Stable Angina.” European Journal of Cardio-Thoracic Surgery: Official Journal of the European Association for Cardio-Thoracic Surgery 8 (8): 404–9. Kaplan, Edward L, and Paul Meier. 1958. “Nonparametric Estimation from Incomplete Observations.” Journal of the American Statistical Association 53 (282): 457–81. Kar, Sumit, Ajay Krishnan, Preetha K, and Atul Mohankar. 2012. “A Review of Antihistamines Used During Pregnancy.” Journal of Pharmacology and Pharmacotherapeutics 3 (2): 105–8. Kendall, John. 2003. “Designing a Research Project: Randomised Controlled Trials and Their Principles.” Emergency Medicine Journal: EMJ 20 (2): 164. Le-Rademacher, Jennifer G, Ryan A Peterson, Terry M Therneau, Ben L Sanford, Richard M Stone, and Sumithra J Mandrekar. 2018. “Application of Multi-State Models in Cancer Clinical Trials.” Clinical Trials 15 (5): 489–98. Pocock, Stuart J, and Richard Simon. 1975. “Sequential Treatment Assignment with Balancing for Prognostic Factors in the Controlled Clinical Trial.” Biometrics, 103–15. Ruetzler, Kurt, Michael Fleck, Sabine Nabecker, Kristina Pinter, Gordian Landskron, Andrea Lassnigg, Jing You, and Daniel I Sessler. 2013. “A Randomized, Double-Blind Comparison of Licorice Versus Sugar-Water Gargle for Prevention of Postoperative Sore Throat and Postextubation Coughing.” Anesthesia &amp; Analgesia 117 (3): 614–21. Smith, AC, JF Dowsett, RCG Russell, ARW Hatfield, and PB Cotton. 1994. “Randomised Trial of Endoscopic Steriting Versus Surgical Bypass in Malignant Low Bileduct Obstruction.” The Lancet 344 (8938): 1655–60. Syriopoulou, Elisavet, Tove Wästerlid, Paul C Lambert, and Therese M-L Andersson. 2022. “Standardised Survival Probabilities: A Useful and Informative Tool for Reporting Regression Models for Survival Data.” British Journal of Cancer 127 (10): 1808–15. Taves, Donald R. 1974. “Minimization: A New Method of Assigning Patients to Treatment and Control Groups.” Clinical Pharmacology &amp; Therapeutics 15 (5): 443–53. Therneau, Terry M. 2024. A Package for Survival Analysis in r. https://CRAN.R-project.org/package=survival. Treasure, Tom, and Kenneth D MacRae. 1998. “Minimisation: The Platinum Standard for Trials?: Randomisation Doesn’t Guarantee Similarity of Groups; Minimisation Does.” Bmj. British Medical Journal Publishing Group. Vitale, Cristiana, Massimo Fini, Ilaria Spoletini, Mitja Lainscak, Petar Seferovic, and Giuseppe MC Rosano. 2017. “Under-Representation of Elderly and Women in Clinical Trials.” International Journal of Cardiology 232: 216–21. Zhong, Baoliang. 2009. “How to Calculate Sample Size in Randomized Controlled Trial?” Journal of Thoracic Disease 1 (1): 51. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
