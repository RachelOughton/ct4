[["index.html", "Clinical Trials 4H Welcome to Clinical Trials 4H! Practical details What to expect from this module", " Clinical Trials 4H Rachel Oughton 2026-01-26 Welcome to Clinical Trials 4H! This page contains the notes for Clinical Trials IV 2025-26. You can also download the PDF version (see the icon at the top left). If you notice any typos, mistakes or places that are unclear, please do let me know! In case you’re reading the PDF, the notes are best viewed in HTML (the link is https://racheloughton.github.io/ct4/). The PDF will be up-to-date, but the formatting is designed for HTML. Practical details Lectures Our lectures are 3 pm on Mondays and 10 am on Wednesdays, both in W414 (this is the Appleby Building, between Physics and the main library). Computer classes We have four computer practicals for this module. They are 12-1 pm on the Fridays of weeks 13, 15, 17 and 19. These classes are in MCS 3097. Office Hour The office hour will be every Monday, 10-11am, in my office (MCS 3016). Assessment This module is assessed through two equally weighted pieces of coursework. The first will be assigned on Wednesday 11th February (due 2nd March), the second on Wednesday 18th March (due 4th May). I have given you more time than you need, so that you can plan ahead. If you will need an extension please contact the maths teaching and learning team (maths.teaching@durham.ac.uk) as soon as possible. This page explains the university policy on extensions for coursework. There will also be two formative assignments during the course. More details on these to follow. Books and resources The main reference for the first half of the course is Matthews (2006). There are a couple of copies in the Bill Bryson Library. Some other books we will make use of are Hulley et al. (2013), Hayes and Moulton (2017). You shouldn’t need to use any of these books, but of course you’re welcome to if you want to read further. I will share links along the way for relevant resources like articles and podcast episodes, which I hope will help you to get a feel for the topic more generally. What to expect from this module Clinical Trials IV is somewhat different from the majority of statistics modules, because Its main focus is on application It is assessed purely through coursework. This means that your experience of it might be different from what you’re used to We will cover quite a lot of different statistical methods (drawing on most of the 1H and 2H courses, and some 3H!) but not in great depth There is no pressure to memorize anything - indeed, if you really were a trial statistician, you would definitely have access to the internet, various textbooks and even these notes (should they prove useful!). There is an emphasis on understanding which method we use and why, and what it means. Hopefully this has been the case in some of your other modules too! What I expect from you Because we will be covering quite a lot of different areas within statistics, there may be some things that you haven’t seen before (or can’t remember very well). I will try my best to explain them as clearly as I can, but there isn’t time to go into the nuts and bolts of everything we come across. Therefore, if you do feel a bit rusty on some area, you may need to read up on that a bit, so that you’re happy with it. I am very happy to suggest resources from time to time, and you’re welcome to come to the office hour to talk about such things. This course is in its infancy, and so I would also really appreciate your feedback. I may not be able to address everything (or I may only be able to implement things for following years), but if I can act on it quickly then I will! References Hayes, Richard J, and Lawrence H Moulton. 2017. Cluster Randomised Trials. CRC press. Hulley, Stephen B, Steven R. Cummings, Warren S. Browner, Deborah G. Grady, and Thomas B. Newman. 2013. Designing Clinical Research, Fourth Edition. Lippincott Williams &amp; Wilkins. Matthews, John NS. 2006. Introduction to Randomized Controlled Clinical Trials. CRC Press. "],["rct-intro.html", "1 Introduction to Clinical Trials 1.1 Causal inference and clinical trials 1.2 The structure of a clinical trial 1.3 Ethical issues 1.4 Phases of clinical trials", " 1 Introduction to Clinical Trials A clinical trial is an experiment, usually performed on human subjects, to test the effect of some sort of treatment or intervention. We may also use the term Randomised controlled trial (RCT). These are not fully the same thing; a clinical trial may not have been randomised, for example if it follows a pre-determined cohort through some sort of process. Likewise, an RCT may not be clinical, but instead may be about an intervention in some other setting like agriculture or education. For this module, we are really focussing on RCTs, and almost all of our examples will be clinical. For the purposes of this module, a clinical trial will have two groups: The treatment group or intervention group: this group of people will be subject to the new treatment. The control group: this group of people will be subject to the status quo - the ‘standard’ or most widely used treatment path for their cohort (sometimes this is no treatment). These groups are usually, though not always, of the same size. Which group each patient is assigned to is usually decided by randomization, which is something we will go on to explore in later lectures. In reality, trials can have more than two groups, and many statistical methods extend quite naturally to this. The goal of the trial is to estimate the treatment effect: is the treatment better than the control, and if so, how much? This short description raises lots of statistical issues, which will take up the next few weeks! Before we get into the theory, we’ll think about some of the background to clinical trials, and introduce some key ideas. Put (very!) simply, the goal of a clinical trial is to determine what works to make people better. Although clinical trials as we know them now have only been around since the Second World War, similar sorts of experiments can be seen from much longer ago. If you’re interested in learning about the evolution of clinical trials from Biblical times to now, the James Lind Library has some fascinating resources and articles. Example 1.1 Scurvy (James Lind, 1757) Scurvy was a serious disease, particularly affecting seamen on long voyages. Symptoms were unpleasant (mouth sores, skin lesions etc.) and it could often be fatal. Lind was the ship’s surgeon on board the HMS Salisbury, and had several patients with scurvy. Many remedies were proposed and in popular use at the time (with only anecdotal evidence, if any, to support them). In 1757 Lind decided to test six such treatments, on two patients each: cider dilute sulfuric acid vinegar sea water citrus (oranges and lemons) purgative mixture (a paste of garlic, mustard seed, horseradish, balsam of Peru, and gum myrrh) Lind chose twelve seamen with similar severity of symptoms, and subjected them to their assigned treatment for 6 days. They were kept in the same quarters, and fed the same diet apart from their treatment. Unsurprisingly (to us!) “The most sudden and visible good effects were perceived from the use of oranges and lemons,” A key thing to notice about the Scurvy example is that Lind went to great lengths to ensure that the treatment was the only thing affecting these 12 sailors: they all started with a similar severity of symptoms, they were kept in the same place and their diet was identical apart from their treatment. This links to one of the foundational principles of clinical trials: causal inference. 1.1 Causal inference and clinical trials You’re probably familiar with the mantra that “correlation does not imply causation”: just because two things are correlated, it doesn’t mean we can conclude that one causes the other. If you’re not convinced, here are some humorous (and slightly macabre) examples. Causal inference is concerned with the design and analysis of data for uncovering causal relationships. This is important for us, because we really want to be able to conclude that a treatment works (or doesn’t) - that it causes recovery, or a reduction in symptoms, or helps the patient in some way. If we were experimental scientists in some laboratory, we could conduct some controlled experiment in which everything was kept under very specific conditions, and could fairly easily make conclusions about the treatment we were testing, and how it behaved in a range of conditions. However, testing treatments on real people is different: we don’t have several identical versions of the same person to test the treatment on, and even if we did, it doesn’t take very much to completely alter the conditions of someone’s existence! Neither can we just base our conclusion of whether a treatment works on lab-based tests or theory (although undoubtedly these will both play a part in developing the treatment in the first place). The treatment needs to be tested on actual people. Because, as we noted, people are all different, and living different lives (and unlike James Lind we can’t force them all to live in the same part of a ship and eat the same food!) we will need to test the treatment on lots of people in order to gather empirical evidence. This is why statistics is so important in the design and analysis of clinical trials. The results of the trial must concluded beyond reasonable doubt, and must be able to be generalised to as-yet-untreated patients. We want to avoid any spurious correlations that are down to chance, or to associations we haven’t taken into account. For example, what if the two seamen given citrus were also much younger and generally healthier than the other ten? Maybe they would have recovered quickly anyway? Or what if another treatment was actually much better than citrus, but just happened to have been given to two sailors who had some other pre-existing illness, causing them to suffer much worse with scurvy? Clinical trials are therefore crucial for modern medicine, and statistics is crucial to clinical trials. But why exactly are clinical trials given this position of importance? Do we really have to do things this way? 1.2 The structure of a clinical trial In a clinical trial, people are grouped and subdivided in various ways. The population of eligible patients One of the first steps in conducting a trial is to specify exactly what sort of person you want to test the treatment on, and where these people will be found. They may be of a certain sex and/or age range, they may have (or definitely not have) certain conditions. They may suffer from some particular symptom, or be at a particular stage of an illness. A clear set of criteria is key to consistency. Patients are usually recruited as they present (eg. to hospital or a GP centre) and may be being recruited over several years, or by several different clinicians, so it is important that everyone is sticking to the same plan. Example 1.2 In a study by Hjalmas and Hellstrom (1998) of the use of desmopressin in children with nocturnal enuresis (bed-wetting), children had to be aged 6 - 12 with a history of PMNE (primary monosymptomatic nocturnal enuresis) and no organic pathology (no disease that alters the structure or function of organs). The children had to be free of other urinary problems (such as frequency, urgency or daytime incontinence) and not to have received any treatment for nocturnal enuresis during the 2 months before entering the trial. Children with clinically significant endocrine, metabolic, hepatic, psychiatric, neurological, musculoskeletal, cardiovascular, haematological, renal or genitourinary disease were excluded from the trial. Knowing exactly what type of patients were recruited into the trial is also key when generalising the results to the population. If the trial recruited males aged 55-70, we cannot confidently conclude that the results will apply to a female aged 26. Entry to the trial The group of patients recruited will be some subset of the possible population. Patients are allowed to refuse consent to take part, or individual patients may be judged unsuitable despite meeting the criteria. Knowing how many patients to recruit is a statistical question, which we will deal with soon. Allocation to groups These patients are then allocated to receive either the treatment, or to be part of the control group (or more, if there are more than two groups). These groups are often referred to as the trial arms - the treatment arm and the control arm. Deciding which patients should be allocated to which group is another statistical question. Once the patients have been allocated, they will receive the treatment (or not) and important measurements will be taken during the trial period. The primary outcome In a clinical trial, there are usually many measurements performed on patients, and possibly at various different points throughout the trial. However, for the sake of the analysis, we usually determine one to be the primary outcome variable. The research questions should be phrased in terms of this variable, and the goal of our design should be to be able to answer questions about this variable. Example 1.3 In a trial by Villar et al. (2020) investigating the use of Dexamethasone treatment for acute respiratory distress syndrome , the primary outcome was the ‘number of ventilator free days up to 28 days’, while other outcomes included ‘all-cause mortality after 60 days’ and ‘incidence of infections in ICU’. Comparing results Now that the trial has been run, we have two sets of measurements (in particular, measurements of the primary outcome): one for the treatment group and one for the control group. But guess what?! Comparing these and coming to a conclusion about the effect of the treatment is a statistical question. Trial timeline Now that we’ve thought (very briefly) about the main stages in a clinical trial, we can look at how they fit together. As we see above, the recruitment, allocation and trial phases overlap. Once a participant is recruited into the trial, they are (usually) allocated to the control or treatment group very soon after, and then their course of treatment (whichever it is) begins. The recruitment phase will continue until enough participants have been recruited (or sometimes until some deadline). Depending on how long this takes, and how long the actual trial phase lasts, some participants may have finished their course of treatment before others have even been allocated. This has implications for the allocation in particular, since we can’t just allocate all the participants at once. Results are collected during the trial, and once everyone has finished their course of treatment the analysis phase can begin. Trial protocol and analysis plan Before doing any of the above stages, it is important to write a protocol and analysis plan. This is usually a requirement of any clinical body (and indeed some other bodies who run trials, like the Educational Endowment Foundation). The protocol and analysis plan will include the rationale for the trial itself, and for the choice of primary outcome variable. It will also include practical details such as eligibility criteria, recruitment plans and methods for keeping participants (and probably clinicians) blind to who is in which group. There will also be lots of detail about the statistical plan, for example the sample size will be determined using existing knowledge about the primary outcome and details about the planned analysis. The planned approach to allocation will be detailed, and finally the statistical analysis plan for the primary outcome and any subgroup or secondary analyses. There should be contingencies for possible issues, for example “If 20% or more of the primary outcome values are missing, we will use X missing data approach”. Example 1.4 Accelerated Reader (EEF, 2018) Accelerated Reader is a web-based programme that encourages children to read for pleasure. You can find the protocol at https://educationendowmentfoundation.org.uk/projects-and-evaluation/projects/accelerated-reader-effectiveness-trial. The protocol for this trial lays out (among other things) the design of the trial: cluster-randomised controlled trial School eligibility: approximately half to be located in the North East, not used AR in last two years, containing years 3 to 6, no special or independent schools Pupil eligibility within schools: depends on ability to take a STAR reading test, parental consent Strategy for recruitment the mode of random allocation, and which variables will be taken into account and how Primary outcome measure (different for different year groups) Sample size calculations (informed by lots of the other points) Statistical modelling plan: use both multilevel and school-dummy fixed effect models (don’t worry if you don’t know these!) How significance of results will be determined based on the model fit. The evaluation report then follows this plan exactly, unless there is very good reason not to. Why bother with a control group? Surely if we want to see whether a treatment works, we should just give it to a patient and see if they get better? Why do we need to also have a group of people not receiving the treatment? In rare and extreme cases, this is a decent strategy: if a disease has always been fatal, but we start giving patients the treatment and some live, that is pretty solid evidence that the treatment works. This was the case with tuberculous meningitis, until the introduction of Streptomycin in 1944. This was also the case when Edward Jenner tested cowpox as a vaccination for the fatal disease smallpox. After observing that milkmaids, once they had suffered from the mild condition cowpox (which they did often), seemed to be immune to smallpox, Jenner tested his theory by injecting an 8 year old boy called James Phipps with fluid from a milkmaid’s cowpox lesions (yum). Once the boy’s cowpox infection had run its course, he injected him again, this time with matter from a fresh smallpox lesion. Thankfully, James Phipps did not contract smallpox. After several more successful such tests, and a gradual shift in attitudes to the idea of vaccination (a word coined by Jenner, from the latin ‘vaccinia’, meaning cowpox) Jenner’s results were published and vaccination became commonplace. Clearly, injecting people with smallpox who had not been given the cowpox innoculation would be very cruel (they would almost certainly die) and would prove nothing; there was already plenty of evidence for the fatality of smallpox. However, most diseases have a fairly uncertain and variable trajectory. If we give a group of patients the treatment, we can’t know what would have happened to them if they hadn’t received the treatment, or had received a different treatment. Comparing them to patients from the past is dodgy because lots of other things may have changed since even the recent past. This is why we have a concurrent control group (usually known as just the control group). These patients do not receive the new treatment, but instead carry on as usual. The aim is to make the control and treatment groups as similar as possible in all other respects (especially those we deem important) so that at the end we can attribute the difference between the two groups to the treatment. 1.3 Ethical issues Clinical trials differ from most scientific experiments in that they are experimenting on people. This means that the team designing, conducting and analysing the trial have various ethical responsibilities. This is a huge area; we will touch on it from time to time but will not go into anywhere near enough detail! Some key things to note though are…. A patient must never be given a treatment that is known to be inferior. Patients must be fully informed about the trial, the treatments used, possible adverse effects and side-effects and so on. Patients should only be recruited into the trial if, after being given all this information (and having had it communicated clearly and at an appropriate level) they give their consent. After entering a trial, a patient has the right to withdraw at any point, and should then receive whatever treatment is most appropriate for them. They should not face any negative repercussions for withdrawing. The patients’ interests are safeguarded by the Declaration of Helsinki. This statement is implemented differently by different countries. In the UK, health authorities each have their own ethics committee, by which proposals for experiments involving human subjects must be approved. You might think that these ethical issues largely concern the clinicians, and that we statisticians don’t need to worry too much about the ethics of clinical trials. After all, we are likely never to meet any patients or to get our hands dirty in any way! But as we will see, at each stage the choices made by the statistician can in fact have serious ethical implications. 1.4 Phases of clinical trials If you read about clinical trials (or hear about them in the news), you’ll hear talk of a ‘phase 3 trial’ or similar. Broadly speaking, clinical trials follow a progression from phase one (or sometimes zero) to phase four. These phases apply to most countries, and any for any drug to be licensed multinationally it must get through phase III. Phase zero The first step is to test a low dose of the treatment on a small number of people, to check that it isn’t harmful. The dose is too low to have any medicinal effect, but is designed to verify that the drug behaves as expected from laboratory studies, and doesn’t have any harmful effects. There may only be 10-20 participants, and there is no randomisation (or control group). Phase one Phase one trials are also quite small (around 20-50 participants) and are designed to find the best dose of the treatment and what any side effects are. Phase one trials tend to be recruited very slowly: a small group will be recruited onto a low dose and monitored closely. If all goes well, another small group will be recruited on a slightly higher dose, and so on. This is known as a dose escalation study. Participants at this phase are monitored very closely, for example through regular blood tests and recording daily symptoms. Phase two If the drug makes it through the phase one trial, it can progress to phase two (often written as ‘phase II’). These involve more people than phase one, possibly up to 100. The cohort may now be restricted to people with a particular version of a condition (eg. a particular type of cancer), but it may still be broader than the sorts of trials we will be looking at. Now the aim is to find out if the new treatment works well enough to progress to a large phase three (phase III) trial: Exactly what conditions (or versions of a condition) does this treatment work for? What are the side effects and can they be managed? What is the best dose to administer? Phase II trials sometimes compare the treatment to a placebo, and sometimes use randomisation to group participants. This is the stage at which most drugs fail, for a multitude of reasons (cost, safety, efficacy,…). Phase three Phase III trials are much bigger, often involving hundreds or thousands of participants, and aim to compare the new treatment to the best currently available treatment (which may be another treatment, or may be nothing). Side effects are still monitored, as some of the rarer ones may not show themselves at the smaller phases, because there are fewer participants. In phase III trials, the aim is to find out if the new treatment is better, and if so by how much. Phase III trials almost always use randomisation to allocate participants to groups, and go to great lengths to make the trial as reliable as possible, for example using a placebo for the control group (who aren’t getting the real treatment) that looks identical to the real drug. These are the sorts of trials we will mainly be concerned with in this course. To be licensed, a treatment has to get through phase III and be found to be effective (and of course safe). Phase four Phase IV trials happen after the treatment has been found to work, has been licensed and is in use. The aims of phase IV trials are to find out more about the rarer side effects to investigate the long term risks and benefits to find out how well the treatment works when given to a broader group of people than in phase III. References Hjalmas, Hanson, and Kruse Hellstrom. 1998. “Long‐term Treatment with Desmopressin in Children with Primary Monosymptomatic Nocturnal Enuresis: An Open Multicentre Study.” British Journal of Urology. Villar, Jesús, Carlos Ferrando, Domingo Martı́nez, Alfonso Ambrós, Tomás Muñoz, Juan A Soler, Gerardo Aguilar, et al. 2020. “Dexamethasone Treatment for the Acute Respiratory Distress Syndrome: A Multicentre, Randomised Controlled Trial.” The Lancet Respiratory Medicine 8 (3): 267–76. "],["rct-plan.html", "2 Sample size for a normally distributed primary outcome variable 2.1 The treatment effect 2.2 Reminder: hypothesis tests (with a focus on RCTs) 2.3 Constructing a measure of effect size 2.4 Power: If \\(H_0\\) is false 2.5 A sample size formula", " 2 Sample size for a normally distributed primary outcome variable For most of this module, we’ll focus on randomised controlled trials (RCTs). These have mainly been used for clinical applications (for example, to test a particular drug), but have also recently become popular ways to test interventions in areas such as education and policing. Having laid the groundwork in Chapter 1, we now go on to some more technical details. In this Chapter, we focus on the ‘vanilla’ scenario, where we have a trial with two arms, and our unit of randomization is individuals. At first we will focus only on continuous outcomes, but in later weeks we will go on to think about binary and time-to-event data. The topics we cover fall into the categories of ‘before the trial’ (design and planning) or ‘after the trial’ (analysis), although as we’ll see there is some interaction between these stages. The first big question asked of a trial statistician is usually how many participants does the trial need in order to be viable: the sample size. We will clarify what is meant by ‘viable’ later in this section. Broadly speaking, there are two (opposing) ethical issues around sample size: If we don’t recruit enough patients, then we may not gather enough evidence to draw any conclusion about the research question (ie. whether there is a treatment effect). As well as being scientifically disappointing, this is unethical. To conduct the trial, some of the patients will have been subject to an inferior treatment (assuming one treatment was actually better), and if there is no conclusion then this was effectively for no purpose. If we recruit too many patients (ie. we would be sufficiently likely to reach a conclusion with fewer) then we have subjected more patients than necessary to an inferior treatment (assuming one treatment was better), and have probably taken up more time and resources than was necessary. It is therefore important to think about this issue carefully. We’ve framed the question in quite a woolly way so far, but now we’ll start to think more carefully. 2.1 The treatment effect In Section 1.2 we discussed the need to settle on a primary outcome variable. One reason this is important is that we base our sample size calculations on the primary outcome variable. Definition 2.1 Suppose our primary outcome variable is \\(X\\), which has mean \\(\\mu\\) for the control group and mean \\(\\mu + \\tau\\) for the treatment group. The variable \\(\\tau\\) is the treatment effect. The goal of our RCT is to learn about \\(\\tau\\). The larger \\(\\tau\\) is (in magnitude), the more pronounced the effect of the intervention. This problem is usually framed as a hypothesis test, where the null hypothesis is that \\(\\tau=0\\). 2.2 Reminder: hypothesis tests (with a focus on RCTs) When performing a hypothesis test, what we are aiming to find is the P-value. Definition 2.2 The P-value is the probability of obtaining a result as extreme as or more extreme (ie. further away from the null hypothesis value) than the one obtained given that the null hypothesis is true. Put simply, the P-value is a measure of the probability of obtaining whatever result (eg. treatment effect) we have have found simply by random chance, when in fact there is no treatment effect (ie. \\(\\tau=0\\)). We use the data to calculate a test statistic, whose distribution under the null hypothesis we understand (given some assumptions), and this allows us to find the P-value. Generally, a P-value below \\(\\alpha = 0.05\\) is accepted as sufficient evidence to reject the null hypothesis, although in clinical settings the threshold can often be smaller (eg. \\(\\alpha = 0.01\\)). It is conventional to present the P-value by simply saying whether it is smaller than some threshold (often 0.05), rather than giving the exact value. Definition 2.3 The threshold for the P-value below which the results are considered ‘significant’ is known as the significance level of the test, and is generally written \\(\\alpha\\) (as above). This use of a significance level is (in part) a legacy from early days when computers were rare and values were looked up in \\(t\\)-tables (or similar). Now that it is very simple to find the exact P-value, it is becoming more and more common to report the actual number. Indeed, there is a big difference between \\(p=0.049\\) and \\(p=0.000049\\). 2.2.1 One-tailed or two-tailed? It is highly likely that the scientists running the trial will have a strong idea of the likely ‘direction’ of the treatment effect. Assuming that a larger value of the primary outcome variable \\(X\\) is good, they will expect / hope for a positive value of the treatment effect \\(\\tau\\) (or be prepared to accept that the effect might be minimal). It would therefore be tempting to perform a one-sided test, with \\[\\begin{align*} H_0\\,&amp;:\\, \\tau=0\\\\ H_1\\,&amp;:\\, \\tau&gt;0. \\end{align*}\\] For example, suppose our test statistic \\(t\\) has a \\(t\\) distribution with 31 degrees of freedom and we obtain a value of 2, as shown in Figure 2.1. In this case our P-value is \\(1 - F_t\\left(2, df=31\\right)= 0.0272\\) (where \\(F_t\\left(\\cdot\\right)\\) is the cumulative distribution function of the \\(t\\) distribution) , and the result would be considered significant at the 0.05 level. Figure 2.1: The distribution \\(t_{31}\\), with the area corresponding to \\(t &gt; 2\\) shaded. For a large positive value of \\(t\\), we obtain a small P-value, and reject \\(H_0\\), concluding that the intervention is effective (in a good way). However, what if we obtain a large negative value of \\(t\\)? In this one-sided set-up, there is no value of \\(t&lt;0\\) that would give a significant result; negative values of \\(t\\) are simply considered consistent with \\(H_0\\), and there is no mechanism to conclude that an intervention has a significantly negative effect. For this reason, we always conduct two sided hypothesis tests, with \\[\\begin{align*} H_0\\,&amp;:\\, \\tau=0\\\\ H_1\\,&amp;:\\, \\tau\\neq 0. \\end{align*}\\] In this scenario, Figure 2.1 is replaced by the plot shown in Figure 2.2, where values of \\(t\\) with \\(t&lt;-2\\) are considered ‘equivalent’ to those with \\(t&gt;2\\), in the sense of how unlikely they are under \\(H_0\\). Figure 2.2: The distribution \\(t_{31}\\), with the area corresponding to \\(|t| &gt; 2\\) shaded. The P-value for the two-sided test as shown in Figure 2.2 is \\[ F\\left(-2, df=31\\right) + \\left[1 - F\\left(2, df=31\\right)\\right] = 2\\times{0.0272} = 0.0543\\] and the result is no longer significant at the 0.05 level. Throughout this course, we will always assume two-tailed tests. 2.2.2 Insignificant results If our P-value is relatively large, say 0.3 or 0.5 (or really, greater than \\(\\alpha\\)), then our result is not at all unlikely (or sufficiently unlikely) under the null hypothesis, and provides insufficient evidence to reject \\(H_0.\\) However, it is not inconsistent with the existence of a treatment effect, so we don’t say there is evidence to accept \\(H_0\\). One can imagine that if the true treatment effect \\(\\tau\\) were tiny, many trials would fail to find evidence to reject \\(H_0\\). However, if our sample size were sufficiently large, we should be able to detect it. Conversely, if \\(\\tau\\) is very large, even a relatively small sample size is likely to provide enough evidence to reject \\(H_0\\). A non-significant P-value means that our results are consistent with the null hypothesis \\(\\tau=0\\), but they are also consistent with some small treatment effect, and therefore we can’t conclude very much. The key issue is, what size of treatment effect do we care about? We must ensure that our sample size is large enough to be sufficiently likely to detect a clinically meaningful treatment effect. We are being vague for now, but this is a key issue in determining an appropriate sample size. 2.3 Constructing a measure of effect size Let’s say we are recruiting participants into two groups: group \\(T\\) will be given the new treatment (they will sometimes be referred to as the treatment group or treatment arm, or alternatively the intervention group) and group \\(C\\) will be given the control (they are the control group or control arm). Suppose that we have \\(n\\) patients in group \\(C\\), and \\(m\\) in group \\(T\\). The primary outcome variable \\(X\\) is normally distributed with mean \\(\\mu\\) in group C (the control group) and mean \\(\\mu+\\tau\\) in group T (the intervention group), and common standard deviation \\(\\sigma\\). So \\[\\begin{align*} X &amp; \\sim N\\left(\\mu, \\sigma^2\\right) \\text{ in group }C\\\\ X &amp; \\sim N\\left(\\mu + \\tau, \\sigma^2\\right) \\text{ in group }T. \\end{align*}\\] We are testing the null hypothesis \\(H_0: \\tau=0\\) against the alternative hypothesis \\(H_1: \\tau\\neq{0}\\). Using the data obtained in the trial, we will be able to obtain sample means \\(\\bar{x}_C\\) and \\(\\bar{x}_T\\) from each group, and a pooled estimate of the standard deviation \\[ s = \\sqrt{\\frac{\\left(n-1\\right)s_C^2 + (m-1)s_T^2}{n+m - 2}}, \\] where \\(s_C\\) and \\(s_T\\) are the sample standard deviations for groups \\(C\\) and \\(T\\) respectively, for example \\[ s_C = \\sqrt{\\frac{\\sum\\limits_{i=1}^n{\\left(x_i - \\bar{x}_C\\right)^2}}{n-1}}. \\] Using these values we can compute \\[D = \\frac{\\bar{x}_T - \\bar{x}_C}{s\\sqrt{\\frac{1}{n} + \\frac{1}{m}}}\\] as a standardised measure of the effect \\(\\tau\\). Theorem 2.1 Under \\(H_0\\), and the model we’ve specified, \\(D\\) has a \\(t\\)-distribution with \\(n+m-2\\) degrees of freedom. Proof. Under \\(H_0\\) the \\(x_i\\) are iid \\(N\\left(\\mu,\\;\\sigma^2\\right)\\), and so \\[\\begin{align*} \\bar{x}_C &amp; \\sim{N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)}\\\\ \\bar{x}_T &amp; \\sim{N\\left(\\mu, \\frac{\\sigma^2}{m}\\right)} \\end{align*}\\] and therefore \\[ \\bar{x}_T - \\bar{x}_C \\sim{N \\left(0, \\sigma^2\\left[\\frac{1}{n} + \\frac{1}{m}\\right] \\right)}\\] and \\[ \\frac{\\bar{x}_T - \\bar{x}_C}{\\sigma \\sqrt{\\frac{1}{n}+\\frac{1}{m}}} \\sim{N\\left(0,1\\right)}.\\] We know that for \\(x_1,\\ldots,x_n,\\sim N\\left(\\mu,\\sigma^2\\right)\\) for some arbitrary \\(\\mu\\) and \\(\\sigma^2\\), \\[\\frac{1}{\\sigma^2}\\sum\\limits_{i=1}^n\\left(x_i - \\bar{x}\\right)^2 \\sim{\\chi^2_{n-1}},\\] and so we have \\[\\begin{align*} \\frac{n-1}{\\sigma^2}s_C^2 &amp; \\sim \\chi^2_{n-1}\\\\ \\frac{m-1}{\\sigma^2}s_T^2 &amp; \\sim \\chi^2_{m-1}\\\\ \\text{and} &amp;\\\\ \\frac{1}{\\sigma^2}\\left[\\left(n-1\\right)s_C^2 + \\left(m-1\\right)s_T^2\\right] &amp; = \\frac{n+m-2}{\\sigma^2}s^2\\\\ &amp;\\sim \\chi^2_{n+m-2}. \\end{align*}\\] The definition of a \\(t\\)-distribution is that if \\(Z\\sim N\\left(0,1\\right)\\) and \\(Y \\sim{\\chi^2_n}\\) then \\[X = \\frac{Z} {\\sqrt{\\frac{Y}{n}}} \\sim{t_n},\\] that is \\(X\\) has a \\(t\\) distribution with \\(n\\) degrees of freedom. Plugging in our \\(N\\left(0,1\\right)\\) variable for \\(Z\\) and our \\(\\chi^2_{n+m-2}\\) variable for \\(Y\\), we have \\[\\begin{align*} \\frac{\\frac{\\bar{x}_T - \\bar{x}_C}{\\sigma\\sqrt{\\frac{1}{n} + \\frac{1}{m}}}}{\\sqrt{\\left(\\frac{n+m-2}{\\sigma^2}s^2\\right) \\bigg/ \\left(n+m-2\\right)}} &amp; = \\frac{\\bar{x}_T - \\bar{x}_C}{\\sigma\\sqrt{\\frac{1}{n} + \\frac{1}{m}}} \\bigg/ \\frac{s}{\\sigma} \\\\ &amp; = \\frac{\\bar{x}_T - \\bar{x}_C}{s\\sqrt{\\frac{1}{n} + \\frac{1}{m}}} \\\\ &amp; = D \\end{align*}\\] and therefore \\(D\\) has a \\(t\\) distribution with \\(n+m-2\\) degrees of freedom. We can therefore use \\(D\\) as our test statistic; if \\(D\\) is such that \\[ |D| &gt; t_{n+m-2}\\left(\\alpha/2\\right)\\] where \\(t_{n+m-2}\\left(\\cdot\\right)\\) is the function such that \\(P\\left(T&gt;t_{df}\\left(\\xi\\right)\\right) = \\xi\\) when \\(T \\sim{t_{df}}\\) then we can reject \\(H_0\\). In practical terms, for more than around 40 degrees of freedom, the \\(t\\) distribution is indistinguishable from the normal distribution, and since it is rare to have fewer than 40 participants in an RCT, we use a normal approximation in what follows, and a difference is significant at the \\(100\\left(1-\\alpha\\right) \\%\\) level if \\(|D| &gt; z_{\\alpha/2}\\), where \\(z\\) are standard normal quantile values. For example, for \\(\\alpha=0.05\\) we have \\(z_{\\alpha/2} = 1.960\\), since the probability of a standard normal variable exceeding this value is 0.025. So, if we have run a trial, and have obtained \\(n\\) values of \\(X\\) from group \\(C\\) and \\(m\\) values of \\(X\\) from group \\(T\\), we can compute \\(D\\). If \\(D\\) lies outside the interval \\(\\left[-z_{\\alpha/2}, z_{\\alpha/2}\\right]\\) then we reject \\(H_0\\). This is equivalent to \\(\\bar{x}_T - \\bar{x}_C\\) falling outside the interval \\[\\left[-z_{\\alpha/2}s\\sqrt{\\frac{1}{n} + \\frac{1}{m}},\\; z_{\\alpha/2}s\\sqrt{\\frac{1}{n} + \\frac{1}{m}} \\right]. \\] Brief aside on notation We’ll see a lot of the notation \\(z_{\\alpha/2}\\) and similar, so to clarify: In R, we have \\(\\Phi\\left(z_{\\alpha/2}\\right) = \\texttt{pnorm}\\left(z_{\\alpha/2}\\right)\\) and \\(z_{\\alpha/2} = \\texttt{qnorm}\\left(\\Phi\\left(z_{\\alpha/2}\\right)\\right)\\). qnorm is the quantile and pnorm is the cumulative distribution function. So, \\[\\frac{\\alpha}{2} = 1 - \\Phi\\left(z_{\\alpha/2}\\right).\\] We have constructed our whole argument under the assumption that \\(H_0\\) is true, and that the probability of such a value is therefore \\(\\alpha\\). We want this probability to be small, since it constitutes an error; \\(H_0\\) is true, but our value of \\(D\\) (or the difference in means) leads us to reject \\(H_0\\). This is sometimes called the ‘type I’ error rate. But what if \\(H_0\\) is false? 2.4 Power: If \\(H_0\\) is false We have constructed things so that if \\(H_0\\) is true, we have a small probability of rejecting \\(H_0\\). But if \\(H_0\\) is false, and \\(\\tau\\neq{0}\\), we want our test to have a high probability of rejecting \\(H_0\\). Definition 2.4 The power of a test is the probability that we reject \\(H_0\\), given that \\(H_0\\) is false. The power function depends on the value of \\(\\tau\\) and is \\[\\Psi\\left(\\tau\\right) = \\Pr\\left(\\text{Reject } H_0\\mid{\\tau\\neq{0}}\\right) = 1 - \\beta.\\] We therefore also have \\[\\beta = \\Pr\\left(\\text{Fail to reject } H_0\\mid{\\tau\\neq{0}}\\right). \\] We call \\(\\beta\\) the type II error rate. If you find the notation confusing (as I do!) then it might be helpful to remember that both \\(\\alpha\\) and \\(\\beta\\) are error rates - probabilities of coming to the wrong conclusion. It is common to talk in terms of \\(\\alpha\\), the significance level, (which will be a low number, often 0.05) and of \\(1-\\beta\\), the power (which will be a high number, often 0.8). I’ve found though that it is not uncommon to find people refer to \\(\\beta\\) (rather than \\(1-\\beta\\)) as the power. If in doubt, keep in mind that we require \\(\\alpha,\\;\\beta \\ll 0.5\\). It is also common to use percentages: a significance level of \\(\\alpha=0.05\\) can also be referred to as “the 95% level”, and \\(\\beta=0.2\\) is the same as a “power of 80%”. When using percentages, we talk in terms of the amount of time we expect the test to come to the correct conclusion. If you notice any mistakes in these notes along these (or other!) lines, please point them out. Under \\(H_1\\), we have \\[D = \\frac{\\bar{x}_T - \\bar{x}_C}{s\\sqrt{\\frac{1}{n} + \\frac{1}{m}}},\\] and therefore (approximately) \\[D \\sim{N\\left(\\frac{\\tau}{\\sigma\\lambda\\left(n,m\\right)}, 1\\right)},\\] where \\(\\lambda\\left(n,m\\right) = \\sqrt{\\frac{1}{n}+\\frac{1}{m}}\\). Figure 2.3 shows the distribution of \\(D\\) under \\(H_0\\) and \\(H_1\\) for some arbitrary (non-zero) effect size \\(\\tau\\). The turquoise bar shows the ‘acceptance region’ of \\(D\\), ie. the range of observed values of \\(D\\) for which we will fail to reject \\(H_0\\). We see that this contains 95% of the area of the \\(H_0\\) distribution (we have set \\(\\alpha = 0.05\\) here), so under \\(H_0\\), we have a 0.95 probability of observing a value of \\(D\\) that is consistent with \\(H_0\\). This could easily generalise to another value of \\(\\alpha\\). Figure 2.3: The distribution of \\(D\\) under both \\(H_0\\) and \\(H_1\\) for some arbitrary values of effect size, population variance, \\(n\\) and \\(m\\), with the region in which we fail to reject \\(H_0\\) shown by the turquoise bar and the red shading. However, if \\(H_1\\) is true, and \\(\\tau\\neq{0}\\), there is a non-zero probability of observing a value of \\(D\\) that would lead us to fail to reject \\(H_0\\). This is shown by the area shaded in red, and it has area \\(\\beta\\). One minus this area (ie. the area under \\(H_1\\) that leads us to reject \\(H_0\\)) is the power, \\(1-\\beta\\). We can see that if the distributions have better separation, as in Figure 2.4, the power becomes greater. This can be as a result of a larger \\(\\tau\\), a smaller \\(\\sigma\\) or a smaller \\(\\lambda\\) (therefore larger \\(m\\) and/or \\(n\\)). Figure 2.4: The distribution of D under both \\(H_0\\) and \\(H_1\\) for some arbitrary values of effect size, population variance, \\(n\\) and \\(m\\), with the region in which we fail to reject \\(H_0\\) shown by the turquoise bar and the red shading. For given values of \\(\\alpha\\), \\(\\sigma\\) and \\(\\lambda\\left(n,m\\right)\\), we can calculate the power as a function of \\(\\tau\\) by finding the area of the distribution of \\(D\\) under \\(H_1\\) for which we accept \\(H_1\\). \\[\\begin{equation} \\Psi\\left(\\tau\\right) = 1-\\beta = \\left[1 - \\operatorname{\\Phi}\\left(z_{\\frac{\\alpha}{2}} - \\frac{\\tau}{\\sigma\\lambda}\\right)\\right] + \\operatorname{\\Phi}\\left(-z_{\\frac{\\alpha}{2}} - \\frac{\\tau}{\\sigma\\lambda}\\right) \\tag{2.1} \\end{equation}\\] The first term in Equation (2.1) is the area in the direction of \\(\\tau\\). In Figures 2.3 and 2.4 this is the region to the right of the interval for which we fail to reject \\(H_0\\), ie. where \\[D &gt; z_{\\frac{\\alpha}{2}}.\\] The second term in Equation (2.1) represents the area away from the direction of \\(\\tau\\), ie. a value of \\(D\\) such that \\[ D &lt; - z_{\\frac{\\alpha}{2}},\\] assuming without loss of generality that \\(\\tau&gt;0\\). Figure 2.5 shows the power function \\(\\Psi\\left(\\tau\\right)\\) for \\(\\tau\\) in units of \\(\\sigma\\) (or you could think of this as for \\(\\sigma=1\\)), for three different pairs of values of \\(n\\) and \\(m\\) (remember that these enter the power function via \\(\\lambda\\)) with \\(\\alpha=0.05\\). We see that in general the power is higher for larger sample sizes, and that of the two designs where \\(n+m=200\\), the balanced one with \\(n=m=100\\) achieves the greatest power. In general, the probability of rejecting \\(H_0\\) increases as \\(\\tau\\) moves away from zero. Notice also that all the curves pass through the point \\(\\tau=0,\\,\\beta=0.05\\). Since \\(\\tau=0\\) corresponds to \\(H_0\\) being true, it makes sense that the probability of rejecting the \\(H_0\\) is the significance level \\(\\alpha\\). Figure 2.5: Power curves for various values of \\(n\\) and \\(m\\), with effect size in units of standard deviation, given a type I error rate of 0.05. It is common to think of the effect size in units of \\(\\sigma\\), as we have done here. This makes results more intuitive, since we don’t need to have a good knowledge of the actual outcome variable to know what is a small or large effect size. It is also helpful in situations where the population standard deviation is not well understood, since the trial can be planned with this sort of effect size in mind. To denote the effect size in units of \\(\\sigma\\), we will write \\(\\tau_\\sigma\\), although in practice it is more usual to give both the same notation. In a medical setting we will often have an estimate for \\(\\sigma\\) (for example from previous studies). 2.5 A sample size formula Equation (2.1) allows us to find any one of \\(\\tau_\\sigma,\\,\\alpha,\\,\\beta\\) and \\(\\lambda\\left(n,m\\right)\\) given values for the others. Values for \\(\\alpha\\) and \\(\\beta\\) are often specified by those planning the trial as around \\(\\alpha \\in \\left[0.01,0.05\\right],\\,1-\\beta\\in\\left[0.8,0.9\\right]\\). The remaining two values, \\(\\tau_\\sigma\\) and \\(\\lambda\\left(n,m\\right)\\) are generally settled using one or both of the following questions: Given our budget constraints, and their implications for \\(n\\) and \\(m\\), what is the smallest value of \\(\\tau_\\sigma\\) we can achieve? What is the smallest value of \\(\\tau_\\sigma\\) that would be clinically useful to detect, and what value of \\(\\lambda\\left(n,m\\right)\\) do we need in order to achieve it? An important quantity is therefore the minimum detectable effect size, which we will denote \\(\\tau_M\\). Definition 2.5 The minimum detectable effect size \\(\\tau_M\\) for a particular trial is the smallest value of effect size that is able to be detected with power \\(1-\\beta\\) and at significance level \\(\\alpha\\) (for some specified values of \\(\\alpha,\\;\\beta\\)). Note that we will not definitely detect an effect of size \\(\\tau_M\\), if it exists; by construction, we will detect it with probability \\(1-\\beta\\). If \\(|\\tau| &gt; |\\tau_M|\\) (ie. the true effect size is further from zero than \\(\\tau_M\\) is) then the probability of detecting it will be greater than \\(1-\\beta\\). If \\(|\\tau| &lt; |\\tau_M|\\) then the probability of detecting it will be less than \\(1-\\beta\\). Although we could solve Equation (2.1) numerically, in practice we use an approximation. The second term, representing observed values of \\(D\\) that are far enough away from 0 in the opposite direction from the true \\(\\tau\\) to lead us to reject \\(H_0\\) is so negligible as to be able to be discounted entirely. Indeed, if we were to observe such a value of \\(D\\), we would come to the wrong conclusion about \\(\\tau\\). Therefore, Equation (2.1) becomes \\[\\begin{equation} \\Psi\\left(\\tau\\right) = 1-\\beta = \\left[1 - \\operatorname{\\Phi}\\left(z_{\\frac{\\alpha}{2}} - \\frac{\\tau_M}{\\sigma\\lambda}\\right)\\right]. \\tag{2.2} \\end{equation}\\] Because \\(\\operatorname{\\Phi}\\left(z_\\beta\\right) = 1 - \\beta\\) (by definition) and \\(\\operatorname{\\Phi}\\left(-z\\right) = 1 - \\operatorname{\\Phi}\\left(z\\right)\\) we can write this as \\[ \\operatorname{\\Phi}\\left(z_\\beta\\right) = \\operatorname{\\Phi}\\left(\\frac{\\tau_M}{\\sigma\\lambda} - z_{\\frac{\\alpha}{2}}\\right), \\] where \\(\\tau_M\\) is our minimum detectable effect size. Because of the monotonicity of \\(\\operatorname{\\Phi}\\left(\\cdot\\right)\\), this becomes \\[\\begin{equation} \\begin{aligned} z_\\beta &amp; = \\frac{\\tau_M}{\\sigma\\lambda} - z_{\\frac{\\alpha}{2}} \\\\ z_\\beta + z_{\\frac{\\alpha}{2}} &amp; = \\frac{\\tau_M}{\\sigma\\lambda}. \\end{aligned} \\tag{2.3} \\end{equation}\\] Because we want to think about sample sizes, we rewrite this further. It is most common to perform trials with \\(n=m=N\\) participants in each group, in which case \\[ \\lambda\\left(n,m\\right) = \\sqrt{\\frac{2}{N}}\\] and Equation (2.3) rearranges to \\[\\begin{equation} N = \\frac{2\\sigma^2\\left(z_\\beta + z_{\\frac{\\alpha}{2}}\\right)^2}{\\tau_M^2}. \\tag{2.4} \\end{equation}\\] Example 2.1 (from Zhong 2009) A trial is being planned to test whether there is a difference in the efficacy of ACEII antagonist (a new drug) and ACE inhibitor (the standard drug) for the treatment of primary hypertension (high blood pressure). The primary outcome variable is change in sitting diastolic blood pressure (SDBP, mmHg) compared to a baseline measurement taken at the start of the trial. The trial should have a significance level of \\(\\alpha=0.05\\) and a power of \\(1-\\beta = 0.8\\), with the same number of participants in each group. The minimum clinically important difference is \\(\\tau_M = 3 \\text{ mmHg}\\) and the pooled standard deviation is \\(s = 8 \\text{ mmHg}\\). Therefore, using equation (2.4) the sample size should be at least \\[\\begin{align*} N &amp; = \\frac{2\\times{8}^2\\left(0.842 + 1.96\\right)^2}{3^2}\\\\ &amp; = 111.6, \\end{align*}\\] and therefore we need at least 112 participants in each trial arm. References Zhong, Baoliang. 2009. “How to Calculate Sample Size in Randomized Controlled Trial?” Journal of Thoracic Disease 1 (1): 51. "],["alloc.html", "3 Allocation 3.1 Bias 3.2 Allocation methods 3.3 Incorporating baseline measurements 3.4 Problems around allocation", " 3 Allocation Once we’ve decided how many participants we need in our trial, we need to determine how they will each be assigned to a trial arm. This is process is known as allocation (or sometimes as randomization). Before we think about methods for allocation, we are going to spend some time talking about bias. 3.1 Bias In statistics, bias is a systematic tendency for the results of our analysis to be different from the true value. We see this particularly when we are using sample data to estimate a parameter. We will revisit what we have learned in previous courses about bias before going on to see how it affects RCTs. Definition 3.1 (Bias of an estimate) Suppose that \\(T\\) is a statistic calculated to estimate a parameter \\(\\theta\\). The bias of \\(T\\) is \\[E\\left(T\\right) - \\theta.\\] If the bias of \\(T\\) is zero, we say that \\(T\\) is an unbiased estimator of \\(\\theta\\). An example you will have seen before is the standard deviation. If we have some data \\(x_1,\\,\\ldots,x_n\\) that are IID \\(N\\left(\\mu,\\,\\sigma^2\\right)\\), we can calculate the sample variance \\[ s^2 = \\frac{1}{n}\\sum\\limits_{i=1}^n\\left(x_i - \\bar{x}\\right)^2 .\\] In this case, \\(E\\left(s^2\\right) \\neq {\\sigma^2}\\) (you’ve probably seen this proved so we’re not going to prove it now), and \\(s^2\\) is a biased estimator of \\(\\sigma^2\\). However, we know that \\[E \\left(\\frac{n}{n-1}s^2\\right) = \\sigma^2,\\] and therefore we can apply this correction to the sample variance \\(s^2\\) to produce an unbiased estimate of the population variance \\(\\sigma^2\\). Here, the sample are representative of the population, but the size of the sample leads to some bias. Now, suppose our sample \\(x_1,\\ldots,x_n\\) were drawn from \\(N\\left(\\mu,\\sigma^2\\right)\\), but were not independent of one another. Then, neither our estimator \\(s^2\\), nor our bias-corrected estimator \\(\\frac{n}{n-1}s^2\\) would have expected value \\(\\sigma^2\\). Furthermore, we cannot use our sample \\(x_1,\\ldots,x_n\\) to produce an unbiased estimator of \\(\\sigma^2\\), or even of the mean \\(\\mu\\). This scenario is much closer to what we mean when we talk about bias in a clinical trial setting. Suppose we are testing some new treatment \\(T\\) against the standard \\(C\\). We measure some outcome \\(X\\) for each patient, and our hypothesis is that \\(X\\) behaves differently for those in the treatment group than for those in the control group. It is common practice to express this additively, \\[E\\left(X\\right) = \\mu + \\tau,\\] where \\(\\tau\\) is our treatment effect, which we can estimate using the difference in the groups’ means, \\(\\bar{X}_T - \\bar{X}_C\\). Our null hypothesis is that \\(\\tau = 0\\), and our alternative hypothesis is that \\(\\tau\\neq{0}\\), and therefore an estimate of \\(\\tau\\) from our data is very important! Put equivalently, it is important that there is no bias in our estimates of \\(\\bar{X}_C\\) and \\(\\bar{X}_T\\). Usually, what this comes down to is that the assumption that the data are independent, identically distributed random variables from the relevant distributions (which we have already relied on a lot for our sample size calculations) has been violated in some way. Example 3.1 Historically, women and the elderly are underrepresented in clinical trials (Cottingham and Fisher (2022)) and results are often translated from young or middle aged healthy men to these other groups (Vitale et al. (2017)). This isn’t reasonable, since women have very different hormonal activity from men, causing them to often react differently to drugs compared to men involved in the trial. The standard dose (based on trials with mostly male participants) can also be too high for many women. The complicated nature of women’s hormones is sometimes even given as a reason for not including them in the trial. Women and elderly people are also both more likely to have adverse effects to drugs in some fields. There are also ethical reasons behind the low numbers of women in trials, especially phase I and phase II trials. If a woman is possibly pregnant (and trials tend to be extremely cautious in deciding who might be pregnant!) then they are quite often excluded, in order to protect the (actual or hypothetical) fetus. Indeed, in 1977 the Food and Drug Administration (FDA) in the US recommended that women be excluded from phase I and II trials (Health (2023)) as a result of some severe cases of fetuses being harmed by drugs (especially Thalidamide) . This means that even some very mainstream drugs, for example antihistamines (Kar et al. (2012)), haven’t been tested for safety/efficacy during pregnancy, as well as some (for example HIV treatments) that would be of huge benefit to many many pregnant women. This article is an interesting read if you would like to know more. In the above example, the trial cohort is very deliberately (and arguably sometimes defensibly) not representative of the target population. However, bias can creep into trials in a number of ways. 3.1.1 Where does bias come from? Having established that bias is a serious issue in clinical trials, we will think about several sources of bias. Some of these we will elaborate on as we get to the relevant part of methodology. Most sources of bias creep in during the allocation or selection phase. Selection bias Selection bias occurs when certain patients or subjects are systematically more (or less) likely be entered into the trial because of the treatment they will receive. In a properly run trial this isn’t possible, because it is only after a participant has been recruited that their treatment is chosen. If a medical professional is not comfortable with a particular patient potentially receiving one of the possible treatments, then that patient should not be entered into the trial at all. If there are many such [technically eligible] patients, then this might cause the estimated treatment effect to be worryingly far from the true population treatment effect, since the recruited group of participants would not be very representative of the true population (this is not technically selection bias, but it comes from the same problem). It may happen that the doctor knows which treatment a patient would be given, for example if the allocation follows some deterministic pattern, or is fully known to the doctor in advance. Consciously or subconsciously this knowledge may influence the description they give to potential participants, and this in turn may affect which patients sign up, and the balance of the groups. In practice there should be various safeguards against this situation. Allocation bias Mathematically, allocation bias is similar to selection bias, but instead of coming from human ‘error’, it arises from the random process of allocation. Suppose a trial investigates a drug that turns out to have a much stronger effect on male patients than on female patients. The cohort of recruited participants are randomised into treatment and control groups, and it happens that there is a much smaller proportion of female patients in one group than the other. This will distort the estimated treatment effect. We will investigate various strategies for randomization designed to address this issue for known factors. Example 3.2 This example is framed in terms of selection bias, but applies equally to allocation bias Suppose we run a trial comparing a surgical (S) and a non-surgical (N) treatment for some condition. Patients who are eligible are given the opportunity to join the trial by a single doctor. The severity of the disease is graded as 1 (less serious) or 2 (more serious) for each patient. Across the full group of patients, proportion \\(\\lambda\\) have severity 1 and proportion \\(1-\\lambda\\) have severity 2. Our primary outcome is survival time, \\(X\\), which depends on the severity of disease: \\[\\begin{align*} E\\left(X\\mid{1}\\right) &amp; = \\mu_1\\\\ E\\left(X\\mid{2}\\right) &amp; = \\mu_2 \\end{align*}\\] and we assume \\(\\mu_1&gt;\\mu_2\\). For the overall trial group, for untreated patients we have \\[ E\\left(X\\right) = \\mu = \\lambda \\mu_1 + \\left(1-\\lambda\\right)\\mu_2.\\] Suppose that for treatment group \\(N\\), the expected survival time increase by \\(\\tau_N\\), and similarly for group \\(S\\), so that we have \\[\\begin{align*} E\\left(X\\mid{N,1}\\right) &amp; = \\mu_1 + \\tau_N\\\\ E\\left(X\\mid{N,2}\\right) &amp; = \\mu_2 + \\tau_N\\\\ E\\left(X\\mid{S,1}\\right) &amp; = \\mu_1 + \\tau_S\\\\ E\\left(X\\mid{S,2}\\right) &amp; = \\mu_2 + \\tau_S. \\end{align*}\\] If all patients were admitted with equal probability to the trial (ie. independent of the severity of their disease) then the expected survival time for group \\(N\\), \\(E\\left(X\\mid{N}\\right)\\), would be \\[\\begin{align*} E\\left(X\\mid{1,N}\\right)P\\left(1\\mid{N}\\right) + E\\left(X\\mid{2,N}\\right)P\\left(2\\mid{N}\\right)&amp; = \\left(\\mu_1 + \\tau_N\\right)\\lambda + \\left(\\mu_2+\\tau_N\\right)\\left(1-\\lambda\\right)\\\\ &amp; = \\mu + \\tau_N. \\end{align*}\\] Similarly, the expected survival time in group \\(S\\) would be \\(\\mu+\\tau_S\\), and the treatment effect difference between the two would be \\(\\tau = \\tau_N - \\tau_S\\) and the trial is unbiased. Suppose that although all eligible patients are willing to enter the trial, the doctor is reticent to subject patients with more severe disease (severity 2) to the surgical procedure. This is reflected in the way they explain the trial to each patient, particularly those with severity 2 whom the doctor knows will be assigned to group \\(S\\). In turn this leads to a reduced proportion \\(q = 1-p\\) of those with severity 2 assigned to surgery entering the trial (event \\(A\\)): \\[\\begin{align*} P\\left(A\\mid{N,1}\\right) = P\\left(A\\mid{S,1}\\right) = P\\left(A\\mid{N,2}\\right) &amp; = 1 \\\\ P\\left(A\\mid{S,2}\\right) &amp; = 1-p = q. \\end{align*}\\] Since our analysis is based only on those who enter the trial, our estimated treatment effect will be \\[E\\left(X\\mid{A, N}\\right) - E\\left(X\\mid{A, S}\\right). \\] We can split these according to disease severity, so that \\[E\\left(X\\mid{A,N}\\right) = E\\left(X\\mid{A,N,1}\\right)P\\left(1\\mid{A,N}\\right) + E\\left(X\\mid{A,N,2}\\right)P\\left(2\\mid{A,N}\\right) \\] and similarly for group \\(S\\). We can calculate \\(P\\left(1\\mid{A,N}\\right)\\) using Bayes’ theorem, \\[\\begin{align*} P\\left(1\\mid{A,N}\\right) &amp; = \\frac{P\\left(A\\mid{1,N}\\right)P\\left(1\\mid{N}\\right)}{P\\left(A\\mid{N}\\right)}\\\\ &amp; = \\frac{P\\left(A\\mid{1,N}\\right)P\\left(1\\mid{N}\\right)}{P\\left(A\\mid{N,1}\\right)P\\left(1\\mid{N}\\right) + P\\left(A\\mid{N,2}\\right)P\\left(2\\mid{N}\\right)} \\\\ &amp;= \\frac{1\\times{\\lambda}}{1\\times {\\lambda} + 1 \\times{\\left(1-\\lambda\\right)}}\\\\ &amp; = \\lambda. \\end{align*}\\] Therefore we also have \\(P\\left(2\\mid{A,N}\\right) = 1 -P\\left(1\\mid{A,N}\\right) = 1-\\lambda\\). Following the same process for group \\(S\\), we arrive at \\[\\begin{align*} P\\left(1\\mid{A,S}\\right) &amp; = \\frac{P\\left(A\\mid{1,S}\\right)P\\left(1\\mid{S}\\right)}{P\\left(A\\mid{S}\\right)}\\\\ &amp; = \\frac{P\\left(A\\mid{1,S}\\right)P\\left(1\\mid{S}\\right)}{P\\left(A\\mid{S,1}\\right)P\\left(1\\mid{S}\\right) + P\\left(A\\mid{S,2}\\right)P\\left(2\\mid{S}\\right)} \\\\ &amp; = \\frac{\\lambda}{\\lambda + q\\left(1-\\lambda\\right)}, \\end{align*}\\] which we will call \\(b\\). Notice that \\(P\\left(2\\mid{S}\\right)= 1-\\lambda\\), since it is not conditional on actually participating in the trial. Therefore, \\[\\begin{align*} E\\left(X\\mid{A,N}\\right) &amp; = E \\left(X\\mid{N,1}\\right)P\\left(1\\mid{A,N}\\right) + E \\left(X\\mid{N,2}\\right)P\\left(2\\mid{A,N}\\right) \\\\ &amp; = \\left(\\mu_1 + \\tau_N\\right)\\lambda + \\left(\\mu_2 + \\tau_N\\right)\\left(1-\\lambda\\right) \\\\ &amp; = \\lambda\\mu_1 + \\left(1-\\lambda\\right)\\mu_2 + \\tau_N \\end{align*}\\] and \\[\\begin{align*} E\\left(X\\mid{A,S}\\right) &amp; = E \\left(X\\mid{S,1}\\right)P\\left(1\\mid{A,S}\\right) + E \\left(X\\mid{S,2}\\right)P\\left(2\\mid{A,S}\\right) \\\\ &amp; = \\left(\\mu_1 + \\tau_S\\right)b + \\left(\\mu_2 + \\tau_S\\right)\\left(1-b\\right) \\\\ &amp; = b\\mu_1 + \\left(1-b\\right)\\mu_2 + \\tau_S. \\end{align*}\\] From here, we can calculate the expected value of the estimated treatment effect \\(\\hat\\tau\\) as (substituting our equation for \\(b\\) and rearranging): \\[\\begin{align*} E\\left(X\\mid{A,N}\\right) - E\\left(X\\mid{A,S}\\right) &amp; = \\tau_N - \\tau_S + \\left(\\lambda - b\\right)\\left(\\mu_1 - \\mu_2\\right) \\\\ &amp; = \\tau_N - \\tau_S - \\frac{p\\lambda\\left(1-\\lambda\\right)\\left(\\mu_1 - \\mu_2\\right)}{\\lambda + q\\left(1-\\lambda\\right)}, \\end{align*}\\] where the third term represents the bias. Notice that if \\(q=1-p = 1\\), then there is no bias. There is also no bias if \\(\\mu_1 = \\mu_2\\), ie. if there is no difference between the disease severity groups in terms of survival time. Assuming \\(\\mu_1 - \\mu_2 &gt;0\\), then the bias term is positive and \\[E\\left(\\hat\\tau\\right) = E\\left(X\\mid{A,N}\\right)- E\\left(X\\mid{A,S}\\right) &lt; \\tau_N - \\tau_S.\\] If \\(N\\) is the better treatment, then \\(\\tau_N - \\tau_S&gt;0\\) and the bias will cause the trial to underplay the treatment effect. Conversely, if \\(S\\) is better, then \\(\\tau_N-\\tau_S&lt;0\\) and the trial will exaggerate the treatment effect. Essentially, this is because more severely ill patients have been assigned to \\(N\\) than to \\(S\\), which reduces the average survival time for those in group \\(N\\). Assessment bias Measurements are made on participants throughout the trial. These measurements will often be objective, for example the patients’ weight, or concentration of blood sugar. However, some types of measurement are much more subject to the individual practitioner assessing the patient. For example, many skin conditions are assessed visually, for example estimating the proportion of the body affected. Measuring quantities such as quality of life or psychological well-being involve many subjective judgements on the part of both patient and clinician. Clearly it is ideal for both the patient and the clinician not to know which arm of the trial the patient was part of (this is known as a double blind trial). For treatments involving drugs, this is usually straightforward. However, for surgical interventions it is often impossible to keep a trial ‘blind’, and for interventions involving therapy (for example cognitive behavioural therapy) it is impossible for the patient to be unaware. In this situation, it is possible for the patient or clinician’s judgement to be affected by their knowledge of the allocation, thus affecting the estimated treatment effect. Slight aside: publication bias In most areas of science, including clinical trials, the ultimate aim is to affect practice. This is usually done by publishing a write-up of the trial, including its design, methods, analysis and results, and publishing that in a [medical] journal. These are peer-reviewed, which means that experts from the relevant field are asked to review submitted papers, and either reject or accept them (usually conditional on some revision). These reviewers advise the editor of the journal, who ultimately decides whether or not the paper will be published. There is compelling evidence that papers reporting positive / conclusive results are more likely to be published than papers about [viable] trials that ultimately fail to reject the null hypothesis. As we know, in most cases if the null hypothesis is rejected this is indicative that there is a true treatment difference. However, sometimes by random chance a trial will detect a difference even when there isn’t one (approximately 5% of the time if \\(\\alpha=0.05\\)). If these papers are disproportionately likely to be published, the body of literature will not reflect the truth, and there may be serious implications for practice. This is a huge issue in the pharmaceutical industry, and one from which we can’t escape. If you’d like to learn more about it the book ‘Bad Pharma’ by Goldacre (2012) is an excellent source. Measures are being taken to prevent this: for example, leading medical journal The Lancet insists that any clinical trial related paper is registered with them before the first participant has been recruited, with details of the design and statistical analysis plan. This is then reviewed before the trial begins. 3.1.2 Implications for allocation Historically (and probably still, to an extent), clinical trials have not necessarily used random allocation to assign participants to groups. Altman and Bland (1999) gives an overview of why this has led to bias, and gives some examples. Sometimes analyses compare groups in serial, so that \\(N_A\\) patients one year (say) form the control group, and \\(N_B\\) patients in a subsequent year, who are given treatment \\(B\\), form the intervention group. In this scenario it is impossible to control for all other changes that have occurred with time, and this leads to a systematic bias, usually in favour of treatment \\(B\\). Given the need for contemporary control participants, the question becomes how to assign participants to each group. If the clinician is able to choose who receives which treatment, or if each patient is allowed to choose or refuse certain treatments, this is almost certain to introduce bias. This is avoided by using random allocation. There are two important aspects to the allocation being random that we will draw attention to. Every patient should have the same probability of being assigned to each treatment group. The treatment group for a particular patient should not be able to be predicted. Point 1 is important because, as we have already mentioned, the statistical theory we use to plan and analyse the trial is based on the groups being random samples from the population. Point 2 is important to avoid biases that come through the assignment of a particular patient being known either in advance or after the fact. There are some approaches that ‘pass’ the first point, but fail at the second. As well as strict alternation (\\(ABABAB\\ldots\\)), some such methods use patient characteristics such as date of birth or first letter of surname, which is not related to the trial outcome, but which enables allocations to be predicted. We will now explore some commonly used methods of allocation. We will usually assume two equally sized groups, \\(A\\) and \\(B\\), but it is simple to generalize to three or more groups, or to unequal allocation. 3.2 Allocation methods In this chapter, and in the computer practical on allocation, we will study the behaviour of various allocation methods by implementing them many times. This gives us an idea of how variable the results are, especially in terms of being likely to introduce bias or reduce power. In almost all real trials, patients are allocated and begin treatment as they arrive, and this may happen over the course of weeks or months. The allocation that is generated is the final allocation. It is therefore important to minise the probability of it being a poor one! 3.2.1 Simple random allocation Perhaps intuitively the most simple method is a ‘toin coss’, where each participant has a probability 0.5 of being placed in each group. As participants arrive, assignment \\(C\\) or \\(T\\) is generated (with equal probability). Statistically, this scheme is ideal, since it generates the random sample we need, and the assignment of each participant is statistically independent of that of all other participants. It also doesn’t require a ‘master’ randomisation; several clinicians can individually assign participants to treatment groups in parallel and the statistical properties are maintained. This method is used effectively in many large trials, but for small trials it can be statistically problematic. The main reason for this is chance imbalance of group sizes. Suppose we have two groups, \\(T\\) of size \\(N_T\\) and \\(C\\) of size \\(N_C\\), with \\(N_T + N_C = 2n\\). Patients are allocated independently with equal probability, which means \\[N_C \\sim \\operatorname{Bi}\\left(2n,\\frac{1}{2}\\right), \\] and similar for \\(N_T\\). If the two groups are of unequal size, the larger will be of some size \\(N_{max}\\) between \\(n\\) and \\(2n\\), such that for \\(r = n+1,\\,\\ldots,\\,2n,\\) \\[\\begin{align*} P\\left(N_{max} = r\\right) &amp; = P\\left(N_C = r\\right) + P\\left(N_T = r\\right) \\\\ &amp; = 2\\binom{2n}{r}\\left(\\frac{1}{2}\\right)^{2n}. \\end{align*}\\] The probability that \\(N_C = N_T = n\\) is \\[ P\\left(N_T = N_C = n\\right)= \\binom{2n}{n}\\left(\\frac{1}{2}\\right)^{2n}. \\] These probabilities are shown in Figure 3.1. We can see that this method leads to very unequal groups relatively easily; with \\(n=15\\), \\(P\\left(N_{max}\\geq 20\\right) = 0.099\\), so there is around a one in ten chance that one group will be double or more the size of the other. Figure 3.1: The probability distribution of largest group size for n=15. As we have seen when thinking about sample sizes in Section 2.4, this will reduce the power \\(\\Psi\\) of the trial, since it depends on \\(\\lambda\\left(N_C,\\,N_T\\right) = \\sqrt{\\frac{1}{N_C} + \\frac{1}{N_T}}\\). For larger trials, this imbalance will be less pronounced, for example Figure 3.2 shows the same for \\(n=200\\). Figure 3.2: The probability distribution of largest group size for n=200. In this case \\(P\\left(N_{max} \\geq 220\\right)=0.051\\), so the chance of highly imbalanced groups is much lower. However, we may want to achieve balance on some factor thought to be important, for example sex, age group or disease state, and in this case there may be small numbers even in a large trial. We saw in the sample size section that the greatest power is achieved when group sizes are equal, since this minimises the function \\[\\lambda\\left(n,m\\right) = \\sqrt{\\frac{1}{n}+\\frac{1}{m}}.\\] However, with simple random sampling we can’t guarantee equal group sizes. Example 3.3 Suppose we are designing a trial to have \\(\\alpha=0.05\\), and our minimum detectable effect size is such that \\(\\frac{\\tau_M}{\\sigma}=1\\). If 30 participants are recruited, then we can calculate the power of the study using methods from Chapter 2: \\[1-\\beta = \\Phi\\left(\\sqrt{\\frac{n_T\\,n_C}{30}} - 1.96\\right). \\] The first term in the standard normal CDF comes from the fact that \\[\\left[\\lambda\\left(n,m\\right)\\right]^{-1} = \\sqrt{\\frac{nm}{n+m}} .\\] If we have equal group sizes \\(n_T=n_C=15\\), then the power achieved is 78%. If the group sizes are 10 and 20, we have a power of 73%. If the group sizes are 6 and 24, the power goes down to 59%. So, as we saw when looking at power, we don’t lose too much if the group sizes are 2:1, but a more pronounced imbalace has resulted in a much more noticeable loss. There may be other disadvantages to having such imbalance, for example increased costs, or a reduction in the amount of information gained about side effects. If this imbalance can be avoided, it should be. 3.2.2 Random permuted blocks One commonly used method to randomly allocate participants while avoiding imbalance is to use random permuted blocks (RPBs). With RPBs we randomly generate the allocation in blocks of some predetermined size. If the blocks have size \\(2m\\), and there are two groups then there are \\[\\binom{2m}{m},\\] but this method can be adapted to more than two groups and to unequal group size. Example 3.4 If we have two groups, \\(A\\) and \\(B\\), then there are six blocks of length 4 containing two \\(A\\)s and two \\(B\\)s \\[ \\begin{aligned} 1.&amp; AABB\\\\ 2.&amp; ABAB\\\\ 3.&amp; ABBA\\\\ 4.&amp; BAAB\\\\ 5.&amp; BABA\\\\ 6.&amp; BBAA. \\end{aligned} \\] We can also randomly generate a sequence of numbers from \\(\\left\\lbrace 1, 2, 3, 4, 5, 6 \\right\\rbrace\\), where each number has equal probability. This sequence will correspond to a sequence in \\(A\\) and \\(B\\) with four times the length. In this method, each patient is equally likely to receive \\(A\\) and \\(B\\), but there will never be a difference of more than two between the size of the two groups. For example, suppose the sequence begins \\(2,1,3,6,\\ldots\\). Replacing each number by its block, we have \\(ABAB\\;AABB\\;ABBA\\;BBAA\\;\\ldots\\). One serious disadvantage of this method is that if the block size is fixed, and the doctors involved in the trial know which participants have received which treatments (which is unavoidable in cases such as surgery), then the allocation for some patients can be perfectly predicted. For blocks of size four this is true for the fourth in every block, and for the third and fourth if the first two were the same. This means that selection bias may be a problem in more than 25% of participants, which is deemed unacceptable; indeed, it fails our second point about randomization. 3.2.2.1 RPBs with random block length The issue above can be circumvented by not only randomly choosing from a selection of blocks, but also randomly choosing the length of the block. For example, there are \\[ \\binom{6}{3} = 20\\] possible blocks of size 6. Instead of always selecting from the six possible 4-blocks, a sampling scheme can be as follows. A random number \\(X\\) is drawn from \\(\\left\\lbrace 4,6\\right\\rbrace\\) to select the block length. A second random number \\(Y\\) is drawn from 1 to 6 (if the block length is four) or 1 to 20 (if the block length is 6). The block corresponding to \\(Y\\) is chosen and participants assigned accordingly. If more participants are needed, go back to step 1. As well as ensuring that patients are equally likely to receive treatments \\(A\\) and \\(B\\), and that \\(N_A\\) and \\(N_B\\) can never differ by more than three, this method hugely reduces the possibility of enabling selection bias. The assignment of a patient can only be perfectly predicted if the difference is three, and this happens only for two of the twenty blocks of length six. 3.2.3 Biased coin designs and urn schemes It may be that we prefer a method that achieves balance while retaining the pure stochasticity of simple random sampling. An advantage of RPBs was that once the sequence was generated, no computing power was needed. However, it is safe now to assume that any hospital pharmacy, nurse’s station, GP office or other medical facility will have a computer with access to the internet (or some internal database), and therefore more sophisticated methods are available. It is also very likely that all trial data may be stored on some central database, and so methods that rely on knowing the allocation so far (albeit in some encrypted form) should be possible even if there are multiple clinicians and sites involved. Biased coin designs and urn schemes both work by adjusting the probabilities of allocation according to the balance of the design so far, such that a participant is less likely to be assigned to an over-represented group. 3.2.3.1 Biased coin designs The biased coin design was introduced by Efron (1971), with the aim of ensuring balance whilst not becoming vulnerable to various forms of experimental bias. Efron (1971) suggested the biased coin design be used to achieve balance in terms of patient characteristics (eg. age group, sex, disease state etc.), but in this section we will think about the whole cohort (the maths for a subgroup would be the same). Suppose we are using a biased coin design for a trial to compare two treatments, \\(T\\) and \\(C\\). At the point where some number \\(n\\) (not the total trial cohort) have been allocated, we can use the notation \\(N_T\\left(n\\right)\\) for the number of participants allocated to treatment \\(T\\), and \\(N_C\\left(n\\right)\\) for the number of participants allocated to treatment \\(C\\). Using these, we can denote the imbalance in treatment numbers by \\[ D\\left(n\\right) = N_C\\left(n\\right) - N_T\\left(n\\right) = 2N_C\\left(n\\right) - n.\\] We use the imbalance \\(D\\left(n\\right)\\) to alter the probability of allocation to each treatment in order to restore (or maintain) balance in the following way: If \\(D\\left(n\\right)=0\\), allocate patient \\(n+1\\) to treatment \\(T\\) with probability \\(\\frac{1}{2}\\). If \\(D\\left(n\\right)&gt;0\\), allocate patient \\(n+1\\) to treatment \\(T\\) with probability \\(P\\). If \\(D\\left(n\\right)&lt;0\\), allocate patient \\(n+1\\) to treatment \\(T\\) with probability \\(1-P\\). where \\(P\\in\\left(\\frac{1}{2}, 1\\right)\\). Question: What would happen if \\(P=\\frac{1}{2}\\) or \\(P=1\\)? If, at some point in the trial, we have \\(\\lvert D\\left(n\\right)\\rvert = j\\), for some \\(j&gt;0\\), then we must have either \\[ \\lvert D\\left(n+1\\right)\\rvert = j+1 \\] or \\[ \\lvert D\\left(n+1\\right)\\rvert = j-1 .\\] Because of the way we have set up the scheme, \\[ p\\big(\\lvert D\\left(n+1\\right)\\rvert = j+1\\big) = 1-P \\] and \\[ p\\big(\\lvert D\\left(n+1\\right)\\rvert = j-1\\big) = P.\\] If \\(\\lvert D\\left(n\\right) \\rvert = 0\\), ie. the scheme is in exact balance after \\(n\\) allocations, then we must have \\(\\lvert D\\left(n+1\\right)\\rvert = 1\\). The absolute imbalances therefore form a simple random walk on the non-negative integers, with transition probabilities \\[ \\begin{aligned} P\\bigg(\\lvert D\\left(n+1\\right) \\rvert = 1 \\; \\bigg| \\;\\lvert D\\left(n\\right)\\rvert=0\\bigg)&amp; = 1\\\\ P\\bigg(\\lvert D\\left(n+1\\right) \\rvert = j+ 1 \\; \\bigg| \\; \\lvert D\\left(n\\right)\\rvert=j\\bigg)&amp; = 1 - P\\\\ P\\bigg(\\lvert D\\left(n+1\\right) \\rvert = j-1 \\; \\bigg| \\; \\lvert D\\left(n\\right)\\rvert=j\\bigg)&amp; = P \\end{aligned} \\] Figure 3.3 shows four realisations of this random walk with \\(P=0.667\\) (Efron’s preferred value). We see that sometimes the imbalance gets quite high, but in general it isn’t too far from 0. Figure 3.3: Absolute imbalance for a biased-coin scheme with \\(P=0.667\\). Figure 3.4 shows four realisations of the random walk with \\(P=0.55\\). Here, the imbalance is able to get very high (note the change in \\(y\\)-axis); for example in the first plot, if we stopped the trial at \\(n=50\\) we would have 34 participants in one arm and only 16 in the other. Figure 3.4: Absolute imbalance for a biased-coin scheme with \\(P = 0.55\\). By contrast, with \\(P=0.9\\) as in Figure 3.5, there is much less imbalance. However, this brings with it greater predictability. Although allocation is always random, given some degree of imbalance (likely to be known about by those executing the trial), the probability of guessing the next allocation correctly is high (0.9). This invites the biases we have been trying to avoid, albeit in an imperfect form. Figure 3.5: Absolute imbalance for a biased-coin scheme with \\(P = 0.9\\). Efron’s suggestion for implementation was that each clinician would received an unordered stack of envelopes. Each would contain three more envelopes, each with instructions covering one of the three possible cases (\\(\\lvert D\\left(n\\right)\\rvert&lt;0,\\,\\lvert D\\left(n\\right)\\rvert=0\\) and \\(\\lvert D\\left(n\\right)\\rvert&gt;0\\)). The clinician would open the appropriate envelope and implement the instruction. Remember this was 1971! A big disadvantage to the biased coin scheme is that the same probability is used regardless of the size of the imbalance (assuming it isn’t zero). In the next section, we introduce a method where the probability of allocating the next patient to the underrepresented treatment gets larger as the imbalance grows. 3.2.3.2 Urn models Urn models for treatment allocation use urns in the way that you might well remember from school probability (or indeed often we had drawers of socks). They were first applied to clinical trials by Wei (1978). In this setting, the urn starts off with a ball for each treatment, and a ball is added to the urn each time a participant is allocated. The ball is labelled according to the treatment allocation that participant did not receive. To allocate the next participant, a ball is drawn from the urn. If the allocations at this point are balanced, then the participant has equal probability of being allocated to each treatment. If there is imbalance, there will be more balls labelled by the underrepresented treatment, and so the participant is more likely to be allocated to that one. The greater the imbalance, the higher the probability of reducing it. The process described so far is a \\(UD\\left(1,1\\right)\\); there is one ball for each treatment to start with, and one ball is added to the urn after each allocation. To be more general, we can assume a \\(UD\\left(r,s\\right)\\) scheme. Now, there are \\(r\\) balls for each treatment in the urn to begin with, and \\(s\\) are added after each allocation. Near the start of the allocation, the probabilities are likely to change a lot to address imbalance, but once a ‘reasonable number’ of allocations have been made it is likely to settle into simple random sampling (or very close). Once again, we can find the transition probabilities by considering the absolute imbalance \\(\\lvert D\\left(n\\right) \\rvert\\). Suppose that after participant \\(n\\), \\(N_T\\left(n\\right)\\) participants have been allocated to group \\(T\\), and \\(N_C\\left(n\\right) = n - N_T\\left(n\\right)\\) to group \\(C\\). The imbalance is therefore \\[D\\left(n\\right) = N_C\\left(n\\right) - N_T\\left(n\\right) = 2N_T\\left(n\\right) - n.\\] After \\(n\\) allocations there will be \\(2r + ns\\) balls in the urn: \\(r\\) for each treatment at the start, and \\(s\\) added after each allocation. Of these, \\(r + N_C\\left(n\\right)s\\) will be labelled by treatment \\(T\\) and \\(r + N_T\\left(n\\right)s\\) by treatment \\(C\\). To think about the probabilities for the absolute imbalance \\(\\lvert D\\left(n\\right)\\rvert\\), we have to be careful now about which direction it is in. If the trial currently (after allocation \\(n\\)) has an imbalance of participants in favour of treatment \\(C\\), then the probability that it becomes less imbalanced at the next allocation is the probability of the next allocation being to treatment \\(T\\), which is \\[ \\begin{aligned} p\\left(\\lvert D\\left(n+1\\right)\\rvert = j-1 \\mid D\\left(n\\right)=j, j&gt;0\\right) &amp; = \\frac{r + N_C\\left(n\\right)s}{2r + ns} \\\\ &amp; = \\frac{r + \\frac{1}{2}\\left(n + D\\left(n\\right)\\right)s}{2r + ns} \\\\ &amp; = \\frac{1}{2} + \\frac{D\\left(n\\right)s}{2\\left(2r + ns\\right)} \\\\ &amp; = \\frac{1}{2} + \\frac{\\lvert D\\left(n\\right)\\rvert s}{2\\left(2r + ns\\right)}. \\end{aligned} \\] Similarly, if there is currently an excess of patients allocated to treatment \\(T\\), then the imbalance will be reduced if the next allocation is to treatment \\(C\\), and so the conditional probability is \\[ \\begin{aligned} p\\left(\\lvert D\\left(n+1\\right)\\rvert = j-1 \\mid D\\left(n\\right)=j, j&lt;0\\right) &amp; = \\frac{r + N_T\\left(n\\right)s}{2r + ns} \\\\ &amp; = \\frac{r + \\frac{1}{2}\\left(n - D\\left(n\\right)\\right)s}{2r + ns} \\\\ &amp; = \\frac{1}{2} - \\frac{D\\left(n\\right)s}{2\\left(2r + ns\\right)}\\\\ &amp; = \\frac{1}{2} + \\frac{\\lvert D\\left(n\\right)\\rvert s}{2\\left(2r + ns\\right)}. \\end{aligned} \\] Because the process is symmetrical, an imbalance of a given magnitude (say \\(\\lvert D\\left(n\\right)\\rvert=j\\)) is equally likely to be in either direction. That is \\[p\\big(D\\left(n\\right) &lt; 0 \\mid \\lvert D\\left(n\\right)\\rvert =j \\big)= p\\big(D\\left(n\\right) &gt; 0 \\mid \\lvert D\\left(n\\right)\\rvert =j \\big) = \\frac{1}{2}.\\] Therefore we can use the law of total probability (or partition theorem) to find that \\[ p\\big(\\lvert D\\left(n+1\\right) \\rvert = j-1 \\mid \\lvert D\\left(n\\right) \\rvert = j \\big) = \\frac{1}{2} + \\frac{\\lvert D\\left(n\\right)\\rvert s}{2\\left(2r + ns\\right)}. \\] Since the two probabilities are equal this is trivial. Since the only other possibility is that the imbalance is increased by one, we also have \\[p\\big(\\lvert D\\left(n+1\\right) \\rvert = j+1 \\mid \\lvert D\\left(n\\right) \\rvert = j \\big) = \\frac{1}{2} - \\frac{\\lvert D\\left(n\\right)\\rvert s}{2\\left(2r + ns\\right)}. \\] As with the biased coin design, we also have the possibility that the imbalance after \\(n\\) allocations is zero, in which case the absolute imbalance after the next allocation will definitely be one. This gives us another simple random walk, with \\[ \\begin{aligned} P\\big(\\lvert D\\left(n+1\\right) \\rvert = 1 \\mid \\lvert D\\left(n\\right)\\rvert=0\\big)&amp; = 1\\\\ P\\big(\\lvert D\\left(n+1\\right) \\rvert = j+ 1 \\mid \\lvert D\\left(n\\right)\\rvert=j&gt;0\\big)&amp; = \\frac{1}{2} - \\frac{\\lvert D\\left(n\\right)\\rvert s}{2\\left(2r + ns\\right)}\\\\ P\\big(\\lvert D\\left(n+1\\right) \\rvert = j-1 \\mid \\lvert D\\left(n\\right)\\rvert=j&gt;0\\big)&amp; = \\frac{1}{2} + \\frac{\\lvert D\\left(n\\right)\\rvert s}{2\\left(2r + ns\\right)} \\end{aligned} \\] Figure 3.6: Four realisations of absolute imbalance for r=1, s=1, N=50. Figure 3.7: Four realisations of absolute imbalance for r=1, s=8, N=50. Figure 3.8: Four realisations of absolute imbalance for r=8, s=1, N=50. We see that imbalance is reduced, particularly for small \\(n\\). A small \\(r\\) and large \\(s\\) enhance this, since the large number (\\(s\\)) of balls added to the urn with each allocation weight the probabilities more heavily, as in Figure 3.7. By contrast, if \\(r\\) is large and \\(s\\) is small, as in Figure 3.8, the probabilities stay closer to \\(\\left(\\frac{1}{2}, \\frac{1}{2}\\right)\\) and so more imbalance occurs early on. 3.3 Incorporating baseline measurements At the start of the trial (ideally before allocation) various baseline measurements are usually taken. If the primary outcome variable is a continuous measurement (eg. blood pressure, weight,…) this same quantity will often be included, so that there is some measure of each participant’s condition/symptoms at the start of the trial. Factors such as age, sex, level of symptoms, things to do with treatment history and many others are included. Essentially, we include any variable we can that may lead to bias if not properly dealt with. The crucial thing is that none of these measurements (taken when they are) should be affected by the trial. Such baseline measurements can be used in allocation. 3.3.1 Stratified sampling The usual method of achieving balance with respect to prognostic factors is to divide each factor into several levels and to consider treatment assignment separately for patients having each particular combination of such factor levels. Such groups of patients are commonly referred to as randomization groups or strata. Treatment assignment is performed entirely separately for each stratum, a permuted block design of the type mentioned above often being used. In fact, using purely random treatment assignment for each stratum is equivalent to simple random assignment, so that some equalization of treatment numbers within each stratum is essential. Both the biased coin design and the urn design were intended for use in this way, adjusting in relation to imbalance within each stratum independently. This whole procedure is analogous to performing a factorial experiment, without being able to control the factor levels of the experimental units. Example 3.5 Suppose we are planning a trial involving people over the age of 50, and we anticipate that age and sex might both play an important role in how participants respond to the treatment. For sex, we use the levels ‘male’ and ‘female’, and for age we split the range into 50-65, 66-80 and 81 or over. We therefore have six strata, and we use an allocation strategy independently in each stratum. For example, below we have used randomly permuted blocks of length four. Male Female 50-65 ABAB BBAA … ABBA BBAA … 66-80 BAAB AABB … BABA BAAB … 81 and over ABAB ABBA … ABBA BAAB … Each time a new participant arrives, we follow the randomization pattern for their stratum. We could use another allocation scheme within each stratum, for example an urn model or a biased coin. It is important that we use one that aims to conserve balance, or else the benefits of stratification are lost. A difficulty with stratified sampling is that the number of strata can quickly become large as the number of factors (or the number of levels within some factors) increases. For example, if we have four prognostic factors each with three levels, there are \\(3^4=81\\) strata. This creates a situation that is at best unwieldy, and at worst completely unworkable; in a small trial (with say 100 patients in each arm) there may be some strata with no patients in (this is actually not a problem), and probably many more with only one (this is much more problematic). 3.3.2 Minimization Minimization was first proposed by Taves (1974), then shortly after by Pocock and Simon (1975) and Freedman and White (1976). The aim of minimization is to minimize the difference between the two groups. It was developed for use with strata, as an alternative to randomly permuted blocks. Although the method was developed in the seventies, it has only gained popularity relatively recently, mainly as computers have become widely available. To form the strata, the people running the trial must first specify all of the factors they would like to be balanced between the two groups. These should be any variables that are thought to possibly affect the outcome. Example 3.6 The study by Kallis et al. (1994) compares the effect of giving aspirin to patients before coronary artery surgery with giving them a placebo. Interestingly, the effects of aspirin were found to be both positive (decreases platelet aggregation to arachidonic acid and to collagen) and negative (increased likelihood of post-operative excessive blood loss). For their prognostic factors, Kallis et al. (1994) chose age (\\(\\leq{50}\\) or \\(&gt;50\\)), sex (M or F), operating surgeon (3 possibilities) and number of coronary arteries affected (1 or 2). This creates 24 strata. The trial had 100 participants, meaning an average of 4.17 in each stratum. When a patient enters the trial, their level of each factor is listed. The patient is then allocated in such a way as to minimise any difference in these factors between the two groups. The minimization method has evolved since its conception, and exists in several forms. Two areas in which methods vary are Whether continuous variables have to be binned Whether there is any randomness It is generally agreed that if the risk of selection bias cannot be avoided, there should be an element of randomness. It is also usually accepted that if a variable is included in the minimization, it should also be included in the statistical analysis. 3.3.2.1 Minimization algorithm Suppose we have a trial in which patients are recruited sequentially and need to be allocated to a trial arm (of which there are two). Pocock and Simon (1975) give an algorithm in the general case of \\(N\\) treatment arms, but we will not do that here. Suppose there are several prognostic factors over which we require balance, and that these factors have \\(I, J, K, ...\\) levels. In our example above, there would be \\(I=2,\\; J=2,\\; K=3,\\; L=2\\). Note that this equates to 24 strata. At some point in the trial, suppose we have recruited \\(n_{ijkl}\\) patients with levels \\(i,\\,j,\\,k,\\,l\\) of the factors. For example, this may be males, aged over 50, assigned to the second surgeon, with both coronary arteries affected. Within these, \\(n^A_{ijkl}\\) have been assigned to treatment arm \\(A\\), and \\(n^B_{ijkl}\\) to arm \\(B\\). So we have \\[ n^A_{ijkl} + n^B_{ijkl} = n_{ijkl} .\\] If we were to use random permuted blocks within each stratum, then we would be assured that \\[\\lvert n^A_{ijkl} - n^B_{ijkl} \\rvert \\leq{\\frac{1}{2}b},\\] where \\(b\\) is the block length. However, there are two issues with this: There may be very few patients in some strata, in which case RPBs will fail to provide adequate balance. It is unlikely that we actually need this level of balance. The first point is a pragmatic one - the method usually guaranteed to achieve good balance is likely to fail, at least for some strata. The second is more theoretical. In general, we require that groups be balanced according to each individual prognostic factor, but not to interactions. For example, it is often believed that younger patients would have generally better outcomes, but that other factors do not systematically affect this difference. Therefore, it is enough to make sure that the following are all small: \\[ \\begin{aligned} \\lvert n^A_{i+++} - n^B_{i+++} \\rvert&amp;\\text{ for each }i=1,\\ldots,I\\\\ \\lvert n^A_{+j++} - n^B_{+j++} \\rvert&amp;\\text{ for each }j=1,\\ldots,J\\\\ \\ldots&amp; \\end{aligned} \\] where \\(+\\) represents summation over the other factors, so that for example \\[n^A_{++k+} = \\sum\\limits_{i,j,l}{n^A_{ijkl}}\\] is the total number of patients with level \\(k\\) of that factor assigned to treatment arm \\(A\\). Therefore, instead of having \\(IJKL\\) constraints constraints, as we would with using randomly permuted blocks (or some other randomization method) within each stratum, we have \\(I+J+K+L\\) constraints, one for each level of each factor. In our example this is 9 constraints rather than 24. In order to implement minimisation, we follow these steps: Allocate the first patient by simple randomisation. Suppose that at some point in the trial we have recruited \\(n_{ijkl}\\) patients with prognostic factors \\(i,\\,j,\\,k,\\,l\\). Of these \\(n^A_{ijkl}\\) are allocated to treatment arm \\(A\\) and \\(n^B_{ijkl}\\) to arm \\(B\\). A new patient enters the trial. They have prognostic factors at levels \\(w,\\,x,\\,y,\\,z\\). We form the sum \\[\\begin{equation} \\left(n^A_{w+++} - n^B_{w+++}\\right) + \\left(n^A_{+x++} - n^B_{+x++}\\right) + \\left(n^A_{++y+} - n^B_{++y+}\\right) + \\left(n^A_{+++z} - n^B_{+++z}\\right). \\tag{3.1} \\end{equation}\\] If the sum from step 4 is negative (that is, allocation to arm \\(B\\) has dominated up to now) then we allocate the new patient to arm \\(A\\) with probability \\(P\\), with \\(P&gt;0.5\\). If the sum is positive, they are allocated to arm \\(B\\) with probability \\(P\\). If the sum is zero, they are allocated to arm \\(A\\) with probability \\(\\frac{1}{2}\\). Some people set \\(P=1\\), whereas others would set \\(\\frac{1}{2}&lt;P&lt;1\\) to retain some randomness. Although setting \\(P=1\\) makes the system deterministic, to predict the next allocation a doctor (or whoever) would need to know \\(n^A_{i+++}\\) and so on. This is very unlikely unless they are deliberately seeking to disrupt the trial. However, generally the accepted approach is becoming to set \\(P&lt;1\\). Example 3.7 From Altman (1990) (citing Fentiman, Rubens, and Hayward (1983)). In this trial, 46 patients with breast cancer were allocated to receive either Mustine (arm A) or Talc (arm B) as treatment for pleural effusions (fluid between the walls of the lung). They used four prognostic factors: age (\\(\\leq{50}\\) or \\(&gt;50\\)), stage of disease (I or II, III or IV), time in months between diagnosis of breast cancer and diagnosis of pleural effusions (\\(\\leq{30}\\) or \\(&gt;30\\)) and menopausal status (Pre or post). Let’s suppose that 15 patients have already been allocated. The totals of patients in each treatment arm in terms of each level of each prognostic factor are shown in the table below. factor level Mustine (A) Talc (B) Age 1. 50 or younger 3 4 Age 2. &gt;50 4 4 Stage 1. I or II 1 2 Stage 2. III or IV 6 6 Time interval 1. 30 months or less 4 2 Time interval 2. &gt;30 months 4 5 Menopausal status 1. Pre 4 3 Menopausal status 2. Post 5 3 Suppose our sixteenth patient is under 50, has disease at stage III, has less than 30 months between diagnoses and is pre-menopausal. Our calculation from step 4 of the minimisation algorithm is therefore \\[ \\begin{aligned} \\left(n^A_{1+++} - n^B_{1+++}\\right) + \\left(n^A_{+2++} - n^B_{+2++}\\right) + \\left(n^A_{++1+} \\right.&amp; \\left.- n^B_{++1+}\\right) + \\left(n^A_{+++1} - n^B_{+++1}\\right) \\\\ &amp; = \\left(3-4\\right) + \\left(6-6\\right) + \\left(4-2\\right) + \\left(4-3\\right) \\\\ &amp; = -1 + 0 + 2 + 1\\\\ &amp; = 2 . \\end{aligned} \\] Since our sum is greater than zero, we allocate the new patient to arm B (talc) with some probability \\(P\\in\\left(0.5,1\\right)\\) and update the table before allocating patient 17. One shortcoming of minimisation is that the factors are equally weighted in the algorithm, regardless of the number of patients with that particular factor level. For example, suppose at some later stage of allocation in our Mustine example, only four patients with stage I or II disease had been recruited, and that one of these had been allocated to group \\(A\\) and three to group \\(B\\). At the same point, 18 of the recruited number were post-menopausal, and of these 10 had been allocated to group \\(A\\) and 8 to group \\(B\\). The values contributed to the sum in Equation (3.1) are \\(+2\\) and \\(-2\\), so these imbalances effectively cancel one another out, but intuitively it would feel sensible to prioritise equal distribution within the stage I or II women, since proportionally this stratum is less balanced. Wei (1978) proposed an extension of the Urn Design that does exactly this, but we won’t cover this method in our course. 3.4 Problems around allocation In clinical trials papers, the allocation groups are usually summarised in tables giving summary statistics (eg. mean and SD) of each characteristic for the control group and the intervention group. The aim of these is to show that the groups are similar enough for any difference in outcome to be attributed to the intervention itself. Figure 3.9 shows an example, taken from Ruetzler et al. (2013). Figure 3.9: Summary statistics for an RCT comparing a licorice gargle (the intervention) to a sugar-water gargle (the standard). From Ruetzler et al. (2013) The problem here is that only the marginal distributions are compared for similarity. Consider the following (somewhat extreme and minimalistic) scenario. A study aims to investigate the effect of some treatment, and to balance for gender and age in their allocation, resulting in the following summary table. Male Female Control 57.51 (7.09) 40.31 (5.83) Intervention 44.19 (5.96) 60.03 (5.27) This appears to be a reasonably balanced design. However, if we look at the joint distribution, we see that there are problems. If the intervention is particularly effective in older men, our trial will not notice. Likewise, if older women generally have a more positive outcome than older men, our trial may erroneously find the intervention to be effective. Although this example is highly manufactured and [hopefully!] unlikely to take place in real life, for clinical trials there are often many demographic variables and prognostic factors being taken into account. Achieving joint balance across all them is very difficult, and extremely unlikely to happen if it isn’t aimed for. Treasure and MacRae (1998) give an example in relation to a hypothetical study on heart disease Supposing one group has more elderly women with diabetes and symptoms of heart failure. It would then be impossible to attribute a better outcome in the other group to the beneficial effects of treatment since poor left ventricular function and age at outset are major determinants of survival in any longitudinal study of heart disease, and women with diabetes, as a group, are likely to do worse. At this point the primary objective of randomisation—exclusion of confounding factors—has failed. … If a very big trial fails, because, for example, the play of chance put more hypertensive smokers in one group than the other, the tragedy for the trialists, and all involved, is even greater. However, this issue is rarely addressed in clinical trials: a lot of faith is placed (with reasonable justification) in the likely balance achieved by random sampling, whatever method is used. We will also see in the next Chapter that we can account for some degree of imbalance at the analysis stage. References Altman, Douglas G. 1990. Practical Statistics for Medical Research. CRC press. Altman, Douglas G, and J Martin Bland. 1999. “Treatment Allocation in Controlled Trials: Why Randomise?” Bmj 318 (7192): 1209–9. Cottingham, Marci D, and Jill A Fisher. 2022. “Gendered Logics of Biomedical Research: Women in US Phase i Clinical Trials.” Social Problems 69 (2): 492–509. Efron, Bradley. 1971. “Forcing a Sequential Experiment to Be Balanced.” Biometrika 58 (3): 403–17. Fentiman, Ian S, Robert D Rubens, and John L Hayward. 1983. “Control of Pleural Effusions in Patients with Breast Cancer a Randomized Trial.” Cancer 52 (4): 737–39. Freedman, LS, and Susan J White. 1976. “On the Use of Pocock and Simon’s Method for Balancing Treatment Numbers over Prognostic Factors in the Controlled Clinical Trial.” Biometrics, 691–94. Goldacre, B. 2012. Bad Pharma: How Medicine Is Broken, and How We Can Fix It. HarperCollins Publishers. https://books.google.co.uk/books?id=4amY1Q6Id4QC. Health, National Institute of. 2023. “History of Women’s Participation in Clinical Research.” Office of Research on Women’s Health. https://orwh. od. nih. gov/toolkit …. https://orwh.od.nih.gov/toolkit/recruitment/history. Kallis, P, JA Tooze, S Talbot, D Cowans, DH Bevan, and T Treasure. 1994. “Pre-Operative Aspirin Decreases Platelet Aggregation and Increases Post-Operative Blood Loss–a Prospective, Randomised, Placebo Controlled, Double-Blind Clinical Trial in 100 Patients with Chronic Stable Angina.” European Journal of Cardio-Thoracic Surgery: Official Journal of the European Association for Cardio-Thoracic Surgery 8 (8): 404–9. Kar, Sumit, Ajay Krishnan, Preetha K, and Atul Mohankar. 2012. “A Review of Antihistamines Used During Pregnancy.” Journal of Pharmacology and Pharmacotherapeutics 3 (2): 105–8. Pocock, Stuart J, and Richard Simon. 1975. “Sequential Treatment Assignment with Balancing for Prognostic Factors in the Controlled Clinical Trial.” Biometrics, 103–15. Ruetzler, Kurt, Michael Fleck, Sabine Nabecker, Kristina Pinter, Gordian Landskron, Andrea Lassnigg, Jing You, and Daniel I Sessler. 2013. “A Randomized, Double-Blind Comparison of Licorice Versus Sugar-Water Gargle for Prevention of Postoperative Sore Throat and Postextubation Coughing.” Anesthesia &amp; Analgesia 117 (3): 614–21. Taves, Donald R. 1974. “Minimization: A New Method of Assigning Patients to Treatment and Control Groups.” Clinical Pharmacology &amp; Therapeutics 15 (5): 443–53. Treasure, Tom, and Kenneth D MacRae. 1998. “Minimisation: The Platinum Standard for Trials?: Randomisation Doesn’t Guarantee Similarity of Groups; Minimisation Does.” Bmj. British Medical Journal Publishing Group. Vitale, Cristiana, Massimo Fini, Ilaria Spoletini, Mitja Lainscak, Petar Seferovic, and Giuseppe MC Rosano. 2017. “Under-Representation of Elderly and Women in Clinical Trials.” International Journal of Cardiology 232: 216–21. Wei, LJ. 1978. “An Application of an Urn Model to the Design of Sequential Controlled Clinical Trials.” Journal of the American Statistical Association 73 (363): 559–63. "],["rct-analysis.html", "4 Analyzing RCT data 4.1 Confidence intervals and P-values 4.2 Using baseline values 4.3 Analysis of covariance (ANCOVA) 4.4 Some follow-up questions….", " 4 Analyzing RCT data We’re now in the post-trial stage. The trial has been run, and we have lots of data to analyze to try to assess what effect the treatment or intervention has had. In general we will use the notation \\(\\tau\\) to denote the treatment effect. In this chapter we’ll keep our focus on the scenario where the trial outcome is measured on a continuous scale, but in later weeks we’ll go on to look at other types of data. Example 4.1 To illustrate the theory and methods, we’ll use an example dataset from Hommel et al. (1986) (this example is also used by Matthews (2006)). The data involves a trial of 16 diabetes patients, and focusses on a drug (Captopril) that may reduce blood pressure. This is important, since for those with diabetes, high blood pressure can exacerbate kidney disease (specifically diabetic nephropathy, a complication of diabetes). To participate in the trial, people had to be insulin-dependent and already affected by diabetic nephropathy. In the trial, systolic blood pressure was measured before participants were allocated to each trial arm, and then measured again after one week on treatment. A placebo was given to the control group, so that all participants were blinded. The baseline and outcome blood pressure measurements (in mmHg) are shown in Table 4.1. We see that nine participants were assigned to the treatment arm (Captopril) and the remaining seven to the placebo group. Hommel et al. (1986) say that the patients were ‘randomly allocated’ to their group. Table 4.1: Data for the Captopril trial from Hommel et al. (1986). Patient (ID) Baseline (B) Outcome at 1 week (X) Trial Arm 1 147 137 Captopril 2 129 120 Captopril 3 158 141 Captopril 4 164 137 Captopril 5 134 140 Captopril 6 155 144 Captopril 7 151 134 Captopril 8 141 123 Captopril 9 153 142 Captopril 1 133 139 Placebo 2 129 134 Placebo 3 152 136 Placebo 4 161 151 Placebo 5 154 147 Placebo 6 141 137 Placebo 7 156 149 Placebo This is very small dataset, and so in that respect it is quite unusual, but its structure is similar to many other trials. We will build up from the simplest type of analysis to some more complicated / sophisticated approaches. 4.1 Confidence intervals and P-values Because the randomization process should produce groups that are comparable, we should in principle be able to compare the primary outcome (which we’ll still denote \\(X\\)) between the groups. Example 4.2 Summary statistics of the outcome for each group are shown in Table 4.2. Table 4.2: Summary statistics for each group. Sample Size Mean (mmHg) SD (mmHg) SE of mean (mmHg) Captopril 9 135.33 8.43 2.81 Placebo 7 141.86 6.94 2.62 We see that the difference in mean outcome (systolic blood pressure) between the two groups is \\(141.86 - 135.33 = 6.53 \\text{mmHg}\\). Clearly overall there has been some reduction in systolic blood pressure for those in the Captopril arm, but how statistically sound is this as evidence? It could be that really (for the wider population) there is no reduction, and we have just been ‘lucky’. The variances within the two groups are fairly close, so we can use the pooled estimate of standard deviation: \\[ s_p = \\sqrt{\\frac{\\sum\\limits_{i=1}^N\\left(n_i-1\\right)s_i^2}{\\sum\\limits_{i-1}^N\\left(n_i-1\\right)}}. \\] In our case \\[ \\begin{aligned} s_p&amp;= \\sqrt{\\frac{8\\times{8.43^2} + 6 \\times{6.94^2}}{8+6}}\\\\ &amp; = 7.82\\text{ mmHg.} \\end{aligned} \\] This enables us to do an independent two-sample \\(t\\)-test, by computing the standardized measure of effect size we found in Section 2.3. We find the \\(t\\) statistic \\[ \\begin{aligned} t &amp; = \\frac{\\bar{X_C} - \\bar{X_T}}{s_p\\sqrt{\\frac{1}{n_C} + \\frac{1}{n_T}}}\\\\ &amp; = \\frac{6.53}{7.82\\sqrt{\\frac{1}{7} + \\frac{1}{9}}} \\\\ &amp; = 1.65. \\end{aligned} \\] Note that here the placebo group is group \\(C\\), and the Captopril group is group \\(T\\). Under the null hypothesis that the mean systolic blood pressure at the end of the week of treatment/placebo is the same in both groups, this value should have a \\(t\\) distribution with 14 degrees of freedom (\\(n_i-1\\) for each group). Figure 4.1: The distribution \\(t_{14}\\), with \\(t=1.65\\) shown by the dashed line and the ‘more extreme’ areas shaded. The dashed line in Figure 4.1 is at \\(t=1.65\\), and the red shaded areas show anywhere ‘at least as extreme’. We can find the area (ie. the probability of anything at least as extreme as our found value) in R by 2*(1-pt(1.65, df=14)) ## [1] 0.1211902 This is the value we know as ‘the P value’. We see that in this case our results are not statistically significant (even at the 0.10 level), under this model. 4.1.1 What do we do with this outcome? The outcome of this Captopril study is in some ways the worst case scenario. The difference in means is large enough to be compelling, but our dataset is too small for it to be statistically significant, and so we can’t confidently conclude that Captopril has any effect on blood pressure. However, we also can’t say that there is no effect. This is exactly the sort of scenario we hoped to avoid when planning our study. One way to reframe the question is to consider the range of treatment effects that are compatible with our trial data. That is, we find the set \\[\\left\\lbrace \\tau \\mid \\frac{\\lvert \\bar{x}_C - \\bar{x}_T - \\tau \\rvert}{s\\sqrt{n_C^{-1} + n_T^{-1}}} \\leq t_{n_C+n_T-2;\\,0.975} \\right\\rbrace, \\] which contains all possible values of treatment effect \\(\\tau\\) that are compatible with our data. That is, suppose the true treatment effect is \\(\\tau^*\\), and we test the hypothesis that \\(\\tau = \\tau^*\\). For all values of \\(\\tau^*\\) inside this range, our data are not sufficiently unlikely to reject the hypothesis at the 0.05 level. However, for all values of \\(\\tau^*\\) outside this range, our data are sufficiently unlikely to reject that hypothesis. We can rearrange this to give a 95% confidence interval for \\(\\tau\\), \\[\\left\\lbrace \\tau \\mid \\bar{x}_C - \\bar{x}_T - t_{n_C+n_T-2;\\,0.975}\\,s\\sqrt{n_C^{-1} + n_T^{-1}} \\leq \\tau \\leq \\bar{x}_C - \\bar{x}_T + t_{n_C+n_T-2;\\,0.975}\\,s\\sqrt{n_C^{-1} + n_T^{-1}} \\right\\rbrace \\] Example 4.3 Continuing our example, we have \\[\\left\\lbrace \\tau \\mid \\frac{\\lvert 6.53 - \\tau \\rvert}{7.82\\sqrt{\\frac{1}{7} + \\frac{1}{9}}} \\leq t_{14;0.975} = 2.145 \\right\\rbrace \\] Here, \\(t_{14;0.975} = 2.145\\) is the \\(t\\)-value for a significance level of \\(0.05\\), so if we were working to a different significance level we would change this. Rearranging as above, this works out to be the interval \\[ -1.92 \\leq \\tau \\leq 14.98. \\] Notice that zero is in this interval, consistent with the fact that we failed to reject the null hypothesis. Some things to note We can compute this confidence interval whether or not we failed to reject the null hypothesis that \\(\\tau=0\\), and for significance levels other than 0.05. In most cases, reporting the confidence interval is much more informative than simply reporting the \\(P\\)-value. In our Captopril example, we found that a negative treatment effect (ie. Captopril reducing blood pressure less than the placebo) of more than 2 mmHg was very unlikely, whereas a positive effective (Captopril reducing blood pressure) of up to 15 mmHg was plausible. If Captopril were inexpensive and had very limited side effects (sadly neither of which is true) it may still be an attractive drug. These confidence intervals are exactly the same as you have learned before, but we emphasise them because they are very informative in randomised controlled trials (but not so often used!). At the post trial stage, when we have data, the confidence interval is the most useful link to the concept of power, which we thought about at the planning stage. Remember that the power function is defined as \\[\\psi \\left(\\tau\\right) = P\\left(\\text{Reject }H_0\\mid \\tau\\neq 0\\right),\\] that is, the probability that we successfully reject \\(H_0\\) (that \\(\\tau=0\\)) given that there is a non-zero treatment effect \\(\\tau\\neq 0\\). This was calculated in terms of the theoretical model of the trial, and in terms of some minimum detectable effect size \\(\\tau_M\\) that we wanted to be able to correctly detect with probability \\(1-\\beta\\) (the power). It is difficult to say anything meaningful about the power of a trial after the fact. That said, if we failed to reject \\(H_0\\) and \\(\\tau_M\\) is in the confidence interval for \\(\\tau\\), then that is a good indication that our trial was indeed underpowered. 4.2 Using baseline values In the example above, our primary outcome variable \\(X\\) was the systolic blood pressure of each participant at the end of the intervention period. However, we see in Table 4.1 that we also have baseline measurements: measurements of systolic blood pressure for each patient from before allocation (and therefore before they received the treatment or placebo). Baseline measurements are useful primarily for two reasons: They can be used to assess the balance of the design. They can be used in the analysis. We will demonstrate these by returning to our Captopril example. Example 4.4 Firstly, we use the baseline systolic blood pressure to assess balance. The placebo group has a mean of 146.6 mmHg and an SD of 12.3 mmHg, whereas the Captopril group has mean 148.0 mmHg, SD 11.4 mmHg. While these aren’t identical, they are sufficiently similar not to suspect any systematic imbalance. In a study this small there is likely to be some difference. Secondly, since we are interested in whether the use of Captopril has reduced blood pressure for each individual, and these individuals had different baseline values, it makes sense to compare not just the outcome but the difference from baseline to outcome for each individual. We can see individual data in Table 4.3 and summary statistics in Table 4.4. Table 4.3: Data for the Captopril trial, with differences shown. Patient (ID) Baseline (B) Outcome at 1 week (X) Trial Arm Difference 1 147 137 Captopril -10 2 129 120 Captopril -9 3 158 141 Captopril -17 4 164 137 Captopril -27 5 134 140 Captopril 6 6 155 144 Captopril -11 7 151 134 Captopril -17 8 141 123 Captopril -18 9 153 142 Captopril -11 1 133 139 Placebo 6 2 129 134 Placebo 5 3 152 136 Placebo -16 4 161 151 Placebo -10 5 154 147 Placebo -7 6 141 137 Placebo -4 7 156 149 Placebo -7 Table 4.4: Summary statistics of the differences for each group. Sample Size Mean (mmHg) SD (mmHg) SE of mean (mmHg) Captopril 9 -12.67 8.99 3.00 Placebo 7 -4.71 7.91 2.99 We can perform an unpaired t-test as before, in which case we find \\[ t = \\frac{-4.71 - (-12.67)}{8.54\\sqrt{\\frac{1}{7}+\\frac{1}{9}}} = 1.850 \\] where 8.54 is the pooled standard deviation. Under the null hypothesis of no difference, this test statistic has a \\(t\\)-distribution with 14 degrees of freedom, and so we have a \\(P\\)-value of 0.086. 2*(1-pt(1.85, df=14)) ## [1] 0.08553164 Our 0.95 confidence interval is \\[\\underbrace{-4.71 - (-12.67)}_{\\text{Mean difference}} \\pm t_{14;\\,0.975}\\times \\underbrace{8.54\\sqrt{\\frac{1}{7}+\\frac{1}{9}}}_{\\text{Pooled SD}} = \\left[-1.3,\\,17.2\\right].\\] We see that taking into account the baseline values in this way has slightly reduced the \\(P\\)-value and shifted the confidence interval slightly higher. Though at the \\(\\alpha = 0.05\\) level we still don’t have significance. We will now look into why the confidence interval and \\(P\\)-value changed in this way, before going on to another way of taking into account the baseline value. 4.2.1 The variance of the treatment effect estimate Let’s label the baseline measurement for each group \\(B_C\\) and \\(B_T\\), and the outcome measurements \\(X_C,\\,X_T\\), where we will take group \\(C\\) to be the placebo/control group and group \\(T\\) to be the treatment group. Because all participants have been randomised from the same population, we have \\[\\operatorname{E}\\left(B_C\\right) = \\operatorname{E}\\left(B_T\\right) = \\mu_B.\\] Assuming some treatment effect \\(\\tau\\) (which could still be zero) we have \\[ \\begin{aligned} \\operatorname{E}\\left(X_C\\right) &amp; = \\mu\\\\ \\operatorname{E}\\left(X_T\\right) &amp; = \\mu + \\tau. \\end{aligned} \\] Usually we will assume that \\[\\operatorname{Var}\\left(X_C\\right) = \\operatorname{Var}\\left(X_T\\right) = \\operatorname{Var}\\left(B_C\\right) = \\operatorname{Var}\\left(B_T\\right) = \\sigma^2,\\] and this is generally fairly reasonable in practice. Notice that for the two analyses we have performed so far (comparing outcomes and comparing outcome-baseline differences) we have \\[ \\begin{aligned} \\operatorname{E}\\left(X_T\\right) - \\operatorname{E}\\left(X_C\\right) &amp; = \\left(\\mu + \\tau\\right) - \\mu = \\tau\\\\ \\operatorname{E}\\left(X_T - B_T\\right) - \\operatorname{E}\\left(X_C - B_C\\right) &amp; = \\left(\\mu - \\mu_B + \\tau\\right) - \\left(\\mu - \\mu_B\\right) = \\tau, \\end{aligned} \\] that is, both are unbiased estimators of \\(\\tau\\). However, whereas the first is based on data with variance \\(\\sigma^2\\), the second has \\[ \\begin{aligned} \\operatorname{Var}\\left(X_T-B_T\\right) &amp; = \\operatorname{Var}\\left(X_T\\right) + \\operatorname{Var}\\left(B_T\\right) - 2\\operatorname{cov}\\left(X_T,B_T\\right)\\\\ &amp; = \\sigma^2 + \\sigma^2 - 2\\rho\\sigma^2 \\\\ &amp; = 2\\sigma^2\\left(1-\\rho\\right), \\end{aligned} \\] where \\(\\rho\\) is the true correlation between \\(X\\) and \\(B\\), and is assumed to be the same in either group. Similarly, \\[\\operatorname{var}\\left(X_C-B_C\\right) = 2\\sigma^2\\left(1-\\rho\\right).\\] Using this to work out the variance of the estimator \\(\\hat{\\tau}\\) we find that for comparing means, assuming two equally sized groups of size \\(N\\), we have \\[\\operatorname{var}\\left(\\hat{\\tau}\\right) = \\operatorname{var}\\left(\\bar{x}_T - \\bar{x}_C\\right) = \\frac{2\\sigma^2}{N}.\\] whereas for comparing differences from baseline \\[\\operatorname{var}\\left(\\hat{\\tau}\\right) = \\operatorname{var}\\left[\\left(\\overline{X_T-B_T}\\right) - \\left(\\overline{X_C - B_C}\\right)\\right] = 2\\left(1-\\rho\\right)\\left(\\frac{2\\sigma^2}{N}\\right).\\] Therefore, if \\(\\frac{1}{2}&lt;\\rho\\leq 1\\) there will be a smaller variance when comparing differences than when using just the outcome variable. However, if \\(\\rho&lt;\\frac{1}{2}\\), the variance will be smaller when comparing outcome variables. Intuitively, this seems reasonable: if the correlation between baseline and outcome measurements is very strong, then we can remove some of the variability between participants by taking into account their baseline measurement. However, if the correlation is weak, then by including the baseline in the analysis we are essentially just introducing noise. If the correlation is negative (very unusual in practice) then the \\(\\operatorname{var}\\left(\\hat\\tau\\right)\\) is much larger - this also makes sense intuitively because we aren’t allowing the coefficient of the baseline measurement to reflect the relationship between baseline and outcome. Example 4.5 For our Captopril example, the sample correlation between baseline and outcome is 0.63 in the Captopril group and 0.80 in the Placebo group. This fits with the \\(P\\)-value having reduced slightly. 4.2.2 A dodgy way to use baseline variables Sometimes the analysis performed on a dataset is rather spurious, but it isn’t always immediately obvious why. We’ll look at one example now, because it is done sometimes. This approach involves looking at each group separately, and determining whether there has been a significant change in the outcome variable (note that this only ‘works’ if \\(\\mu_B = \\mu\\), ie. if you wouldn’t expect a change in \\(X\\) over time anyway). One thing that makes this analysis attractive is that it lends itself to a paired \\(t\\)-test, which is more poweful (in general). Example 4.6 Returning to our Captopril data, we could perform a paired \\(t\\)-test on the difference between baseline \\(B\\) and outcome \\(X\\) for each patient, for each group. If we do this, we find the summary statistics in Table 4.5. Table 4.5: Summary statistics for the dodgy analysis T statistic Deg of freedom P-value Captopril 4.23 8 0.003 Placebo 1.58 6 0.170 From this we see that there is strong evidence for a change in blood pressure for the Captopril patients (group \\(T\\)), which isn’t surprising, and no such evidence for the placebo patients. Can we therefore conclude that Captopril is significantly better than the placebo? No! The analysis is flawed: The \\(p\\)-value of 0.17 in the control group doesn’t show that the null hypothesis (no treatment effect for the control group) is true, just that we can’t reject the null hypothesis. It is quite possible that there is a difference in the control group, and that numerically it could even be comparable to that in the treatment group, so although we can say that there is a significant reduction in blood pressure for the captopril group, we can’t conclude that Captopril is better than the placebo. Having set up the experiment as a randomised controlled trial, with a view to comparing the two groups, it seems strange to then deal with them separately. 4.3 Analysis of covariance (ANCOVA) In the previous section we based our analysis on the baseline values being statistically identical draws from the underlying distribution, and therefore having the same expectation and variance. However, although this is theoretically true, in real life trials there will be some imbalance in the baseline measurements for the different treatment arms. We can see this in our Captopril example, in Figure 4.2. Figure 4.2: Baseline measurements from the Captopril trial. The baseline measurements are not identical in each group. Indeed, we saw earlier that the means differ by 1.4 mmHg. Although this isn’t a clinically significant difference, or a large enough difference to make us doubt the randomisation procedure, it is still a difference. The basic principle of ANCOVA is that if there is some correlation between the baseline and outcome measurements, then if the baseline measurements differ, one would expect the outcome measurements to differ, even if there is no treatment effect (ie. if \\(\\tau=0\\)). Indeed, how do we decide how much of the difference in outcome is down to the treatment itself, and how much is simply the difference arising from different samples? This issue arises in many trials, particularly where there is a strong correlation between baseline and outcome measurements. 4.3.1 The theory Suppose the outcome for a clinical trial is \\(X\\), which has mean \\(\\mu\\) in the control group (C) and mean \\(\\mu+\\tau\\) in the test group (T), and as usual our aim is to determine the extent of \\(\\tau\\), the treatment effect. We suppose also that \\(X\\) has variance \\(\\sigma^2\\) in both groups. The same quantity is measured at the start of the trial, and this is the baseline \\(B\\), which we can assume to have true mean \\(\\mu_B\\) in both groups (because of randomisation) and variance \\(\\sigma^2\\). We also assume that the true correlation between \\(B\\) and \\(X\\) is \\(\\rho\\) in each group. Finally, we assume that both treatment groups are of size \\(N\\). We therefore have \\(2N\\) patients, and so we observe baseline measurements \\(b_1,\\,b_2,\\ldots,b_{2N}\\). Our first step is to find an estimator for \\(\\tau\\) that uses this information. 4.3.1.1 Finding an estimator Let’s suppose that random variables \\(Z\\) and \\(Y\\) are jointly normally distributed with correlation \\(\\rho\\) \\[\\begin{equation} \\begin{pmatrix} Z\\\\ Y \\end{pmatrix} \\sim N\\left( \\begin{pmatrix} \\mu_Z\\\\ \\mu_Y \\end{pmatrix},\\; \\begin{pmatrix} \\sigma^2_Z &amp; \\rho\\sigma_Z\\sigma_Y \\\\ \\rho\\sigma_Z\\sigma_Y &amp; \\sigma^2_Y \\end{pmatrix} \\right). \\tag{4.1} \\end{equation}\\] From Equation (4.1), we know that \\(\\operatorname{E}\\left(Y\\right) = \\mu_Y\\). But, if we have observed \\(Z=z\\), this gives us some information about likely values of \\(Y\\): if \\(\\rho&gt;0\\) then a lower value of \\(z\\) should lead us to expect a lower value of \\(Y\\), for example. Figure 4.3 shows \\[\\begin{equation} \\begin{pmatrix} Z\\\\ Y \\end{pmatrix} \\sim N\\left( \\begin{pmatrix} 0\\\\ 0 \\end{pmatrix},\\; \\begin{pmatrix} 2 &amp; 1.5 \\\\ 1.5 &amp; 3 \\end{pmatrix} \\right). \\tag{4.2} \\end{equation}\\] Figure 4.3: A bivariate normal density. The higher the value of \\(\\rho\\) (in magnitude), the more the conditional distribution of \\(Y\\) given an observed value of \\(z\\) deviates from the marginal distribution of \\(Y\\) (in our example, \\(N\\left(0,\\;3\\right)\\)). In particular, \\[\\operatorname{E}\\left(Y\\mid{Z=z}\\right)\\neq{\\operatorname{E}\\left(Y\\right)}.\\] If we have another random variable, \\(W\\), that is independent of \\(Y\\) (and note that if two normally distributed variables are uncorrelated, they are also independent), then observing \\(W=w\\) doesn’t give us any information about the distribution of \\(Y\\), so we have \\[ \\operatorname{E}\\left(Y\\mid{W=w}\\right)={\\operatorname{E}\\left(Y\\right)}. \\] We can combine this information to work out \\(\\operatorname{E}\\left(Y\\mid{Z=z}\\right)\\). Firstly, we’ll calculate the covariance of \\(Z\\) and \\(Y-kZ\\), for some constant \\(k\\). We can find this by \\[ \\begin{aligned} \\operatorname{cov}\\left(Z,\\,Y-kZ\\right) &amp;= \\operatorname{E}\\big[\\left(Z-\\mu_Z\\right)\\left(Y-kZ - \\mu_Y + k\\mu_Z\\right)\\big]\\\\ &amp;\\text{ (using that }\\operatorname{cov\\left(Z,Y\\right)=\\operatorname{E}\\left[\\left(Z-\\operatorname{E}\\left(Z\\right)\\right)\\left(Y-\\operatorname{E}\\left(Y\\right)\\right)\\right]}\\\\ &amp; = \\operatorname{E}\\left[\\left(Z-\\mu_Z\\right)\\left(Y-\\mu_Y\\right) - k\\left(Z-\\mu_Z\\right)^2\\right]\\\\ &amp; = \\rho \\sigma_Z\\sigma_Y - k\\sigma_Z^2. \\end{aligned} \\] If we set \\[k = \\beta = \\frac{\\rho\\sigma_Y}{\\sigma_Z} \\] then \\(\\operatorname{cov}\\left(Z,\\,Y-\\beta Z\\right)=0\\), and since \\(Y-\\beta Z\\) is also normally distributed, this means that \\(Z\\) and \\(Y-\\beta Z\\) are independent. Therefore we have \\[\\operatorname{E}\\left(Y-\\beta Z \\mid Z=z\\right) = \\operatorname{E}\\left(Y-\\beta Z\\right) = \\mu_Y - \\beta \\mu_Z.\\] However, since we’re conditioning on an observed value of \\(Z=z\\) we can take \\(Z\\) to be fixed at this value, and so \\(\\operatorname{E}\\left(\\beta Z\\mid{Z=z}\\right) = \\beta z\\). Finally, this allows us to calculate \\[ \\begin{aligned} \\operatorname{E}\\left(Y\\mid{Z=z}\\right) &amp; = \\operatorname{E}\\left(\\beta Z\\mid{Z=z}\\right) + \\mu_Y - \\beta\\mu_Z\\\\ &amp; = \\mu_Y + \\beta\\left(z - \\mu_Z\\right). \\end{aligned} \\] We can apply this to our clinical trials setting to find \\[ \\begin{aligned} \\operatorname{E}\\left(X_i\\mid{b_i}\\right) &amp;= \\mu + \\rho\\left(b_i - \\mu_B\\right)\\text{ in the control group}\\\\ \\operatorname{E}\\left(X_i\\mid{b_i}\\right) &amp;= \\mu +\\tau + \\rho\\left(b_i - \\mu_B\\right)\\text{ in the test group.} \\end{aligned} \\] Notice that because in our model \\(\\sigma_T = \\sigma_C = \\sigma\\), we have \\(\\beta = \\rho\\). From this, we find that \\[\\begin{equation} \\operatorname{E}\\left(\\bar{X}_T - \\bar{X}_C\\mid{\\bar{b}_T,\\,\\bar{b}_C}\\right) = \\tau + \\rho\\left(\\bar{b}_T - \\bar{b}_C\\right). \\tag{4.3} \\end{equation}\\] That is, if there is a difference in the baseline mean between the control and test groups, then the difference in outcome means is not an unbiased estimator of the treatment effect \\(\\tau\\). Assuming \\(\\rho&gt;0\\) (which is almost always the case) then if \\(\\bar{b}_T&gt;\\bar{b}_C\\) the difference in outcome means overestimates \\(\\tau\\). Conversely, if \\(\\bar{b}_T&lt;\\bar{b}_C\\), the difference in outcome means underestimates \\(\\tau\\). The only situation in which the difference in outcome means is an unbiased estimator is when \\(\\rho=0\\), however this is not common in practice. Comparing the difference between outcome and baseline, as we did in 4.2, does not solve this problem, since we have \\[\\operatorname{E}\\left[\\left(\\bar{X}_T - \\bar{b}_T\\right) - \\left(\\bar{X}_C - \\bar{b}_C\\right)\\mid{\\bar{b}_T,\\,\\bar{b}_C}\\right] = \\tau + \\left(\\rho-1\\right)\\left(\\bar{b}_T - \\bar{b}_C\\right),\\] which is similarly biased (unless \\(\\rho=1\\), which is never the case). Notice, however, that if we use as our estimator \\[\\begin{equation} \\hat{\\tau} = \\left(\\bar{X}_T - \\bar{X}_C\\right) - \\rho \\left(\\bar{b}_T - \\bar{b}_C\\right) \\tag{4.4} \\end{equation}\\] then, following from Equation (4.3) we have \\[ \\operatorname{E}\\left[\\left(\\bar{X}_T - \\bar{X}_C\\right) - \\rho \\left(\\bar{b}_T - \\bar{b}_C\\right)\\mid{\\bar{b}_T,\\,\\bar{b}_C}\\right] = \\tau + \\rho\\left(\\bar{b}_T - \\bar{b}_C\\right)- \\rho\\left(\\bar{b}_T - \\bar{b}_C\\right) = \\tau. \\] We now return to our general bivariate normal variables \\(Y\\) and \\(Z\\) to find the variance of this estimator. 4.3.1.2 What’s the variance of this estimator? To work out the variance of \\(\\hat{\\tau}\\) in Equation (4.4) we need to go back to our bivariate normal variables. Recall that \\(\\operatorname{var}\\left(Y\\right) = \\operatorname{E}\\left[Y^2\\right] - \\left[\\operatorname{E}\\left(Y\\right)\\right]^2\\), and so \\[\\begin{equation} \\operatorname{var}\\left(Y\\mid{Z=z}\\right) = \\operatorname{E}\\left(Y^2\\mid{Z=z}\\right) - \\left[\\operatorname{E}\\left(Y\\mid{Z=z}\\right)\\right]^2. \\tag{4.5} \\end{equation}\\] We already know the second term, and we can find the first term using the same idea as before, this time noting that \\(Z\\) and \\(\\left(Y-\\beta Z\\right)^2\\) are independent. From this, and using the fact that (for example) \\[ \\begin{aligned} \\operatorname{var}\\left(Z\\right) &amp; = \\operatorname{E}\\left(Z^2\\right) - \\left[\\operatorname{E}\\left(Z\\right)\\right]^2\\\\ \\text{and therefore} &amp; \\\\ \\operatorname{E}\\left(Z^2\\right) &amp;= \\sigma_Z^2 + \\mu_Z^2, \\end{aligned} \\] we find that \\[\\begin{equation} \\operatorname{E}\\left[\\left(Y-\\beta Z\\right)^2 \\mid Z=z\\right] = \\operatorname{E}\\left[\\left(Y-\\beta Z\\right)^2\\right] =S^2 + \\left(\\mu_Y - \\beta \\mu_Z\\right)^2, \\tag{4.6} \\end{equation}\\] where \\(S^2 = \\sigma^2_Y + \\beta^2\\sigma^2_Z - 2\\beta\\rho\\sigma_Z\\sigma_Y = \\sigma^2_Y\\left(1-\\rho^2\\right)\\) (by plugging in \\(\\beta = \\frac{\\rho\\sigma_Y}{\\sigma_Z}\\)). If we multiply out the left-hand side of Equation (4.6), we find that this is the same as \\[\\operatorname{E}\\left[Y^2\\mid{Z=z}\\right] - 2\\beta\\operatorname{E}\\left(Y\\mid{Z=z}\\right) + \\beta^2 z^2 = \\operatorname{E}\\left[Y^2\\mid{Z=z}\\right] - 2\\beta z\\left(\\mu_Y - \\beta\\mu_Z\\right) - \\beta^2 z^2.\\] Equating this with Equation (4.6) and rearranging, we find \\[\\operatorname{E}\\left[Y^2\\mid{Z=z}\\right] = S^2 + \\left(\\mu_Y - \\beta\\mu_Z\\right)^2 + 2\\beta z\\left(\\mu_Y - \\beta \\mu_Z\\right) + \\beta^2z^2.\\] Now we can expand out \\[\\operatorname{E}\\left(Y\\mid{Z=z}\\right) = \\mu_Y + \\beta\\left(z-\\mu_Z\\right) = \\left(\\mu_Y - \\beta\\mu_Z\\right) +\\beta z\\] to find \\[\\left[\\operatorname{E}\\left(Y\\mid{Z=z}\\right) \\right]^2 = \\left(\\mu_Y - \\beta\\mu_Z\\right)^2 + 2\\beta z \\left(\\mu_Y - \\beta\\mu_Z\\right) + \\beta^2 z^2.\\] Finally (!) we can use these two expressions to find \\[ \\begin{aligned} \\operatorname{var}\\left(Y\\mid{Z=z}\\right) &amp; = \\operatorname{E}\\left[Y^2\\mid{Z=z}\\right] - \\left[\\operatorname{E}\\left(Y\\mid{Z=z}\\right)\\right]^2\\\\ &amp; = S^2 \\\\ &amp; = \\sigma_Y^2\\left(1-\\rho^2\\right). \\end{aligned} \\] One thing to notice is that this conditional variance of \\(Y\\) doesn’t depend on the observed value of \\(Z=z\\). It can also never exceed \\(\\sigma^2_Y\\), and is only equal to \\(\\sigma^2_Y\\) if \\(Z\\) and \\(Y\\) are uncorrelated. Back to our estimator! Recall that in ANCOVA our estimator of the treatment effect \\(\\tau\\) is \\[ \\hat{\\tau} = \\left(\\bar{X}_T - \\bar{X}_C\\right) - \\rho\\left(\\bar{b}_T - \\bar{b}_C\\right)\\] and that we have \\[\\operatorname{cor}\\left(\\bar{X}_T - \\bar{X}_C,\\; \\bar{b}_T - \\bar{b}_C\\right) = \\rho.\\] Therefore, using the result we just found, \\[ \\begin{aligned} \\operatorname{var}\\left(\\hat{\\tau}\\right) = \\operatorname{var}\\left[\\left(\\bar{X}_T - \\bar{X}_C\\right) - \\rho\\left(\\bar{b}_T - \\bar{b}_C\\right)\\mid{\\bar{b}_T,\\bar{b}_C}\\right] &amp;= \\operatorname{var}\\left[\\left(\\bar{X}_T - \\bar{X}_C\\right) \\mid{\\bar{b}_T,\\bar{b}_C}\\right]\\\\ &amp; = \\operatorname{var}\\left(\\bar{X}_T - \\bar{X}_C\\right)\\left(1-\\rho^2\\right)\\\\ &amp; = \\frac{2\\sigma^2}{N}\\left(1-\\rho^2\\right). \\end{aligned} \\] Notice that unlike our first estimator that used baseline values, in Section 4.2, the variance of the ANCOVA estimate can never exceed \\(\\frac{2\\sigma^2}{N}\\); if the baseline and outcome are uncorrelated, ANCOVA will perform as well as the \\(t\\)-tests we covered in Sections 4.1 and 4.2. Borm, Fransen, and Lemmens (2007) discuss how this reduction in \\(\\operatorname{var\\left(\\hat{\\tau}\\right)}\\) can impact our sample size calculations. 4.3.2 The practice In the previous section we established an unbiased estimate of the treatment effect that takes into account the baseline measurements. However, we can’t use it as a model, because there are a few practical barriers: Our estimate for \\(\\tau\\) relies on the correlation \\(\\rho\\), which is unknown In real life, the groups are unlikely to have equal size and variance, so ideally we’d lose these constraints We can solve both of these by fitting the following statistical model to the observed outcomes \\(x_i\\): \\[ \\begin{aligned} x_i &amp; = a_0 + \\gamma b_i + \\epsilon_i &amp; \\text{ in group C}\\\\ x_i &amp; = a_0 + \\tau + \\gamma b_i + \\epsilon_i &amp; \\text{ in group T}&amp;. \\end{aligned} \\] Here, the \\(\\epsilon_i\\) are independent errors with distribution \\(N\\left(0,\\,\\sigma^2_\\epsilon\\right)\\), the \\(b_i\\) are the baseline measurements for \\(i=1,\\ldots,N_T+N_C\\), for groups \\(T\\) and \\(C\\) with sizes \\(N_T\\) and \\(N_C\\) respectively, and \\(a_0,\\,\\gamma\\) are coefficients. Sometimes this is written instead in the form \\[ x_i = a_0 + \\tau G_i+ \\gamma b_i + \\epsilon_i \\] where \\(G_i\\) is 1 if participant \\(i\\) is in group \\(T\\) and 0 if they’re in group \\(C\\). This is a factor variable, which you may remember from Stats Modelling II (if you took it). If \\(G_i=1\\) (ie. participant \\(i\\) is in group \\(T\\)) then \\(\\tau\\) is added. If \\(G_i=0\\) (ie. participant \\(i\\) is in group \\(C\\)) then it isn’t. Thinking back to the expectations of \\(X,\\,B\\), we expect that \\[a_0 = \\mu - \\gamma\\mu_B,\\] though of course we won’t know the true values of \\(\\mu\\) or \\(\\mu_B\\). We now have four parameters to estimate: \\(a_0,\\,\\tau,\\,\\gamma\\) and \\(\\sigma^2_\\epsilon\\). For the first three we can use least squares (as you have probably seen for linear regression). Our aim is to minimise the sum of squares \\[S\\left(a_0,\\, \\tau,\\,\\gamma\\right) = \\sum\\limits_{i\\text{ in }T} \\left(x_i - a_0 - \\tau - \\gamma b_i\\right)^2 + \\sum\\limits_{i\\text{ in }C} \\left(x_i - a_0 - \\gamma b_i\\right)^2.\\] This leads to estimates \\(\\hat{a_0},\\, \\hat{\\tau}\\) and \\(\\hat{\\gamma}\\). We won’t worry about how this sum is minimised, since we’ll always be using pre-written R functions. We can use the estimates \\(\\hat{a_0},\\, \\hat{\\tau}\\) and \\(\\hat{\\gamma}\\) to estimate \\(\\sigma^2_\\epsilon\\), using \\[\\hat{\\sigma_\\epsilon}^2 = \\frac{S\\left(\\hat{a_0},\\hat{\\tau}, \\hat{\\gamma}\\right)}{N_T + N_C -3}.\\] The general form for this is \\[ \\hat{\\sigma_\\epsilon}^2 = \\frac{SSE}{n-p},\\] where \\(SSE\\) is the residual sum of squares, \\(n\\) is the number of data points and \\(p\\) the number of parameters (apart from \\(\\sigma_\\epsilon^2\\)) being estimated. If you want to know why that is, you can find out here (look particularly at page 62), but we will just take it as given! As well as generating a fitted value \\(\\hat{\\tau}\\), we (or rather R!) will also find the standard error of \\(\\hat\\tau\\), and we can use this to generate a confidence interval for the treatment effect \\(\\tau\\). The technique described above is a well-established statistical method known as ANCOVA (short for the Analysis of Covariance), which can be implemented in R and many other statistical software packages. Notice that it is really just a linear model (the like of which you have seen many times) with at least one factor variable, and with a particular focus (application-wise) on the coefficient of the treatment group variable. Some cautions As with any linear model, we need to ensure that it is appropriate for our dataset. Two key things we need to check for are: linear effect across the range of the dataset: a linear model is based on the assumption that the effect of the independent variables is the same across the whole range of the data. This is not always the case. For example, the rate of deterioration with age can be more at older ages. This can be dealt with either by binning age into categories, or by using a transformation, eg. age\\(^2\\). Note that this would still be a linear model, because it is linear in the coefficients. Multicollinearity: we should make sure that none of the independent variables are highly correlated. This is not uncommon in clinical datasets, since measurements are sometimes strongly related. Sometimes therefore, this can mean choosing only one out of a collection of two or more strongly related variables. We can check this by finding the correlation matrix of the data (excluding the output). If there is multicollinearity in the data, we would expect to see a very high standard error for the coefficients of the affected variables. Definition 4.1 In a linear regression model the variance of the fitted coefficient \\(\\hat\\beta_j\\) is \\[\\operatorname{var}\\left(\\hat\\beta_j\\right) = \\frac{s^2}{1-\\mathbf{R}^2_j},\\] where \\(s\\) is the root mean-square error (RMSE) and \\(\\mathbf{R}^2_j\\) is the squared multiple correlation for the regression of \\(x_j\\) on all the other covariates. The variance inflation factor is given by \\[\\operatorname{VIF}\\left(\\hat\\beta_j\\right) = \\frac{1}{1-\\mathbf{R}^2_j}.\\] If \\(\\operatorname{VIF}=1\\) then \\(x_j\\) is orthogonal to all other covariates. Large values of \\(\\operatorname{VIF}\\) indicate potential problems. Example 4.7 Let’s now implement ANCOVA on our Captopril data in R. We do this by first fitting a linear model using ‘lm’, with baseline measurement and arm as predictor variables and outcome as the predictand. lm_capt = lm(outcome ~ baseline + arm, data = df_hommel) summary(lm_capt) ## ## Call: ## lm(formula = outcome ~ baseline + arm, data = df_hommel) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.129 -3.445 1.415 2.959 11.076 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 67.5731 19.7577 3.420 0.00456 ** ## baseline 0.4578 0.1328 3.446 0.00434 ** ## armPlacebo 7.1779 2.9636 2.422 0.03079 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.869 on 13 degrees of freedom ## Multiple R-squared: 0.5629, Adjusted R-squared: 0.4957 ## F-statistic: 8.372 on 2 and 13 DF, p-value: 0.004608 We can find the variance inflation factor and see that there appear to be no problems with collinearity: vif(lm_capt) ## baseline arm ## 1.004117 1.004117 The variable ‘arm’ here is being included as a factor variable, so it behaves like \\[ \\text{arm}_i = \\begin{cases} 0 &amp; \\text{ if participant }i\\text{ is assigned Captopril}\\\\ 1 &amp; \\text{ if participant }i\\text{ is assigned Placebo}. \\end{cases} \\] Therefore, for a patient assigned Placebo, a value of 7.18 is added, as well as the intercept and baseline term. This results in a model with two parallel fitted lines. For our previous methods we have calculated a confidence interval for the treatment effect \\(\\tau\\), and we will do that here too. The second column of the linear model summary (above) gives the standard errors of each estimated parameter, and we see that the standard error of \\(\\hat{\\tau}\\) is 2.96. Therefore, to construct a 95/% confidence interval for \\(\\hat{\\tau}\\), we use (to 2 decimal places) \\(7.18\\; \\pm\\; t_{0.975;13}\\times{2.96} = \\left(0.78,\\; 13.57\\right).\\) The model has \\(n-p=13\\) degrees of freedom because there are \\(n=16\\) data points and we are estimating \\(p=3\\) parameters. Notice that unlike our previous confidence intervals, this doesn’t contain zero, and so our analysis has enabled us to conclude that there is a significant reduction in blood pressure with Captopril. You can also see this in that \\(p=0.03079&lt;0.05\\). However, you can tell from the width of the interval (and the fact that \\(p\\) is still quite close to 0.05) that there is still a lot of uncertainty about \\(\\tau\\). The ‘Residual standard error’ term near the bottom of the linear model summary is the estimate of \\(\\hat{\\sigma_\\epsilon}\\), so here we have \\(\\hat{\\sigma_\\epsilon}^2 = 5.869^2 = 34.44.\\) As with any fitted model, we should check the residuals. resid_capt = resid(lm_capt) df_hommel$resid= resid_capt ggplot(data = df_hommel, aes(x=baseline, y=resid, col=arm)) + geom_point() + geom_hline(yintercept=0)+ xlab(&quot;Baseline&quot;)+ ylab(&quot;Residual&quot;)+theme_bw() These look pretty good; there are no clear patterns and the distribution appears to be similar for each treatment group. Though, with such a small sample it’s difficult really to assess the fit of the model. 4.4 Some follow-up questions…. This might have raised a few questions, so we will address those now. 4.4.1 Didn’t we say that \\(X_T - X_C\\) was an unbiased estimator of \\(\\tau\\)? In Sections 4.1 and 4.2 we used both \\(\\bar{X}_T - \\bar{X}_C\\) and \\(\\left(\\bar{X}_T - \\bar{B}_T\\right) - \\left(\\bar{X}_C - \\bar{B}_C\\right)\\) as unbiased estimators of \\(\\tau\\). Then, in Section 4.3.1 we showed that \\[ \\begin{aligned} \\operatorname{E}\\left(\\bar{X}_T - \\bar{X}_C\\mid{\\bar{b}_T,\\;\\bar{b}_C}\\right)&amp; = \\tau + \\rho\\left(\\bar{b}_T - \\bar{b}_C\\right)\\\\ \\operatorname{E}\\left[\\left(\\bar{X}_T - \\bar{b}_T\\right) - \\left(\\bar{X}_C - \\bar{b}_C\\right)\\mid{\\bar{b}_T,\\,\\bar{b}_C}\\right] &amp;= \\tau + \\left(\\rho-1\\right)\\left(\\bar{b}_T - \\bar{b}_C\\right), \\end{aligned} \\] that is, neither of these quantities are unbiased estimators of \\(\\tau\\) (except in very specific circumstances). Is this a contradiction? You’ll be relieved to hear (and may already have realised) that it isn’t; the first pair of equations are blind to the baseline values \\(B_T\\) and \\(B_C\\), and are using their statistical properties. Because of the randomisation procedure, a priori they can be treated the same. However, once we have observed values for the baseline, \\(b_T\\) and \\(b_C\\), they are very unlikely to be exactly the same. They are also (along with all other baseline measurements, often things like age, sex, height etc.) definitely not affected by the trial, since they are taken before any placebo or treatment has been administered, and often even before allocation. However, conditioning on their observed values can reduce the variance of our estimate of \\(\\tau\\), as we have seen. In this sense, the observed baseline means \\(\\bar{b}_T\\) and \\(\\bar{b}_C\\) are known as ancillary statistics; they contain no direct information about the parameter we are interested in (in this case the treatment effect \\(\\tau\\)), but our inferences can be improved by conditioning on the observed values of the ancillary statistics. 4.4.2 What if the lines shouldn’t be parallel? The unequal slopes model In the analysis above, we have assumed that the coefficient \\(\\gamma\\) of baseline is the same in both groups; we have fitted an equal slopes model. It isn’t obvious that this should be the case, and indeed we can test for it. Allowing each group to have a different slope means including an interaction term between baseline and treatment group, \\[ x_i = \\mu + \\tau G_i+ \\gamma b_i + \\lambda b_i G_i + \\epsilon_i . \\] The term \\(\\lambda b_i G_i\\) is 0 if participant \\(i\\) is in group \\(C\\) and \\(\\lambda b_i\\) if participant \\(i\\) is in group \\(T\\). Therefore, for participants in group \\(C\\), the gradient is still \\(\\gamma\\), but for participants in group \\(T\\) it is now \\(\\gamma + \\lambda\\). We can test whether this interaction term should be included (that is, whether we should fit an unequal slopes model) by including it in a model and analysing the results. Example 4.8 Continuing once again with the Captopril dataset, we now fit the model lm_capt_int = lm(outcome ~ arm + baseline + baseline:arm, data = df_hommel) tbl_regression(lm_capt_int) #bnkpamthzd table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #bnkpamthzd thead, #bnkpamthzd tbody, #bnkpamthzd tfoot, #bnkpamthzd tr, #bnkpamthzd td, #bnkpamthzd th { border-style: none; } #bnkpamthzd p { margin: 0; padding: 0; } #bnkpamthzd .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #bnkpamthzd .gt_caption { padding-top: 4px; padding-bottom: 4px; } #bnkpamthzd .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #bnkpamthzd .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #bnkpamthzd .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #bnkpamthzd .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #bnkpamthzd .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #bnkpamthzd .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #bnkpamthzd .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #bnkpamthzd .gt_column_spanner_outer:first-child { padding-left: 0; } #bnkpamthzd .gt_column_spanner_outer:last-child { padding-right: 0; } #bnkpamthzd .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #bnkpamthzd .gt_spanner_row { border-bottom-style: hidden; } #bnkpamthzd .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #bnkpamthzd .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #bnkpamthzd .gt_from_md > :first-child { margin-top: 0; } #bnkpamthzd .gt_from_md > :last-child { margin-bottom: 0; } #bnkpamthzd .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #bnkpamthzd .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #bnkpamthzd .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #bnkpamthzd .gt_row_group_first td { border-top-width: 2px; } #bnkpamthzd .gt_row_group_first th { border-top-width: 2px; } #bnkpamthzd .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #bnkpamthzd .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #bnkpamthzd .gt_first_summary_row.thick { border-top-width: 2px; } #bnkpamthzd .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #bnkpamthzd .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #bnkpamthzd .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #bnkpamthzd .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #bnkpamthzd .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #bnkpamthzd .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #bnkpamthzd .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #bnkpamthzd .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #bnkpamthzd .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #bnkpamthzd .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #bnkpamthzd .gt_left { text-align: left; } #bnkpamthzd .gt_center { text-align: center; } #bnkpamthzd .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #bnkpamthzd .gt_font_normal { font-weight: normal; } #bnkpamthzd .gt_font_bold { font-weight: bold; } #bnkpamthzd .gt_font_italic { font-style: italic; } #bnkpamthzd .gt_super { font-size: 65%; } #bnkpamthzd .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #bnkpamthzd .gt_asterisk { font-size: 100%; vertical-align: 0; } #bnkpamthzd .gt_indent_1 { text-indent: 5px; } #bnkpamthzd .gt_indent_2 { text-indent: 10px; } #bnkpamthzd .gt_indent_3 { text-indent: 15px; } #bnkpamthzd .gt_indent_4 { text-indent: 20px; } #bnkpamthzd .gt_indent_5 { text-indent: 25px; } #bnkpamthzd .katex-display { display: inline-flex !important; margin-bottom: 0.75em !important; } #bnkpamthzd div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after { height: 0px !important; } Characteristic Beta 95% CI1 p-value arm     Captopril — —     Placebo 8.7 -80, 98 0.8 baseline 0.46 0.05, 0.87 0.031 arm * baseline     Placebo * baseline -0.01 -0.61, 0.59 >0.9 1 CI = Confidence Interval We see that the \\(p\\)-value for the coefficient \\(\\lambda\\) (seen in the arm:baseline row) is not at all significant (0.97). Therefore we can be confident that there is no need to fit unequal slopes for this dataset. This fits with our earlier conclusion (from inspecting the residuals) that just including first order terms is fine. 4.4.3 Can we include any other baseline covariates? In Section 4.2 when our estimated treatment effect was \\(\\hat\\tau = \\left(\\bar{x}_T - \\bar{b}_T\\right) - \\left(\\bar{x}_C - \\bar{b}_C\\right)\\), the only other variable we could take into account was the baseline measurement, because it is on the same scale as the outcome \\(X\\). However, in ANCOVA, our treatment effect is \\[ \\hat\\tau = \\left(\\bar{x}_T - \\bar{x}_C\\right) - \\hat\\gamma\\left(\\bar{b}_T - \\bar{b}_C\\right), \\] and the inclusion of the coefficient \\(\\gamma\\) means that we can include other covariates on different scales too. The key issue is that we can only include as covariates things that were already known before allocation (hence they are sometimes known as baseline covariates, not to be confused with ‘the baseline’, which would generally mean the same measurement as the primary outcome, but before treatment). This is because they cannot, at that point, have been affected by the treatment, or have had an influence on the post-trial outcome measurement. Indeed, as a rule, any variable that was used in the randomisation procedure (this particularly applies to minimisation and stratified sampling) should be included in the analysis. Example 4.9 The data for this example is taken from Kassambara (2019). In this study, 60 patients take part in a trial investigating the effect of a new treatment and exercise on their stress score, after adjusting for age. There are two treatment levels (yes or no) and three exercise levels (low, moderate and high) and 10 participants for each combination of treatment and exercise levels. Because in ANCOVA we fit a coefficient to every covariate, we can include exercise (another factor variable) and age (a continuous variable) in this analysis. Table 4.6 shows the mean and standard deviation of age for each combination of treatment and exercise level. If we were being picky / thorough, we might note that (perhaps unsurprisingly!) the mean and standard deviation of age are both lower in the high exercise groups. This might well affect our analysis, but we won’t go into this now. Table 4.6: Summary of the stress dataset Treatment Exercise Mean age SD age yes low 61.7 4.691600 yes moderate 59.6 2.590581 yes high 57.0 2.211083 no low 62.1 4.332051 no moderate 61.4 5.947922 no high 57.9 3.381321 Fitting a linear model, we see that treatment, high levels of exercise and age each have a significant effect on stress. lm_stresslin = lm(score ~ treatment + exercise + age, data = stress) summary(lm_stresslin) ## ## Call: ## lm(formula = score ~ treatment + exercise + age, data = stress) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.0261 -3.7497 -0.4285 3.0943 13.3696 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 55.72934 10.91888 5.104 4.27e-06 *** ## treatmentno 4.32529 1.37744 3.140 0.00272 ** ## exercisemoderate 0.08735 1.69032 0.052 0.95897 ## exercisehigh -9.61841 1.84741 -5.206 2.96e-06 *** ## age 0.49811 0.17648 2.822 0.00662 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.288 on 55 degrees of freedom ## Multiple R-squared: 0.6045, Adjusted R-squared: 0.5757 ## F-statistic: 21.01 on 4 and 55 DF, p-value: 1.473e-10 We can calculate the variance inflation factor for this model vif(lm_stresslin) ## GVIF Df GVIF^(1/(2*Df)) ## treatment 1.017841 1 1.008881 ## exercise 1.230692 2 1.053264 ## age 1.248533 1 1.117378 and see that values are fairly close to one. In particular, taking a high level of exercise reduced participants’ stress scores by around 9.6, and the treatment reduced stress scores by around 4.3. Participants’ stress scores increased slightly with age (just under half a point per year!). We can plot the residuals (Figure 4.4) to check that the model is a reasonable fit Figure 4.4: Residuals from the stress data model with only linear terms. The first thing we notice is that the data are sort of ‘clumped’. This is common in factor models, especially where one or more factors is highly influential. Working from right to left (higher fitted stress score to lower), the highest clump (in blue) are those who do moderate or low levels of exercise and didn’t receive the treatment. The next clump (in red) are those who do moderate or low levels of exercise and did receive the treatment (their stress score is around 4.3 points lower, for the same age). The next clump, in blue, are those who do high levels of exercise and didn’t receive the treatment. Their scores are around 9.6 points lower than the low/moderate exercise groups who didn’t receive treatment. Finally, the lowest scoring group are those who do high levels of exercise and did receive the treatment. We see that some clumps aren’t really centred around zero, and this should raise alarm bells. The data description says that the researchers want to test for the effect of ‘the intervention and exercise’ so it seems reasonable to include the interaction of these two terms: ## ## Call: ## lm(formula = score ~ treatment + exercise + age + treatment:exercise, ## data = stress) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.3250 -3.0192 0.2745 2.4650 10.6667 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 56.79090 10.41383 5.453 1.32e-06 *** ## treatmentno 1.52858 2.23026 0.685 0.4961 ## exercisemoderate 0.01746 2.25662 0.008 0.9939 ## exercisehigh -13.70331 2.36314 -5.799 3.78e-07 *** ## age 0.50355 0.16684 3.018 0.0039 ** ## treatmentno:exercisemoderate 0.15503 3.16129 0.049 0.9611 ## treatmentno:exercisehigh 8.21822 3.15375 2.606 0.0119 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.985 on 53 degrees of freedom ## Multiple R-squared: 0.6613, Adjusted R-squared: 0.623 ## F-statistic: 17.25 on 6 and 53 DF, p-value: 6.167e-11 Notice that now, the effect of the treatment on its own is not significant. Also notice that for both the linear exercise terms and the interactions between the exercise and treatment, the effects of moderate and low exercise are very similar. This has somewhat improved the homogeneity of our residuals, as shown in Figure 4.5. Figure 4.5: Residuals from the stress data model with the interaction between intervention and exercise included. Combining the coefficients, someone who does a high level of exercise: is likely to reduce their stress score by around 13.7 if they receive the treatment is likely to reduce their stress score by around \\(13.7 - 8.2 = 5.5\\) if they don’t receive the treatment Returning to our initial look at the dataset, the fact that age is a factor, and high levels of exercise are clearly very important should worry us slightly, since there are very few older people doing high levels of exercise. This may mean our model is inaccurate. An important caution! As you’ll have seen if you read Kendall (2003) (for formative assignment 1), we should have everything in place, including a statistical analysis plan, before the trial. We should already know which covariates we plan to include in our model, and how. ‘Trawling’ for the best possible model by trying lots of different things (and inevitably settling on the one that leads to the most significant conclusion) is poor practice, and can increase the type I error rate (\\(\\alpha\\)). I realise that is sort of what we’ve done in this Section on Analysis, but that was to demonstrate and compare the different methods. Proceeding in the way we have, trying lots of different models, when analysing and writing up a trial would be very poor practice! There’s another excellent episode of the JAMA Evidence podcast, with a focus on adjusting for covariates, that talks about this issue (you can find it here and linked from Ultra). That draws to a close our work with continuous outcome variables. In the next lecture, we’ll start thinking about binary outcome variables. References Borm, George F, Jaap Fransen, and Wim AJG Lemmens. 2007. “A Simple Sample Size Formula for Analysis of Covariance in Randomized Clinical Trials.” Journal of Clinical Epidemiology 60 (12): 1234–38. Hommel, EHEBMJ, Hans-Henrik Parving, Elisabeth Mathiesen, Berit Edsberg, M Damkjaer Nielsen, and Jørn Giese. 1986. “Effect of Captopril on Kidney Function in Insulin-Dependent Diabetic Patients with Nephropathy.” Br Med J (Clin Res Ed) 293 (6545): 467–70. Kassambara, Alboukadel. 2019. Datarium: Data Bank for Statistical Analysis and Visualization. https://CRAN.R-project.org/package=datarium. Kendall, John. 2003. “Designing a Research Project: Randomised Controlled Trials and Their Principles.” Emergency Medicine Journal: EMJ 20 (2): 164. Matthews, John NS. 2006. Introduction to Randomized Controlled Clinical Trials. CRC Press. "],["ss-bin.html", "5 Sample size for a binary variable 5.1 The Delta Method 5.2 A sample size formula", " 5 Sample size for a binary variable So far almost everything we’ve covered has related to continuous outcome variables, which we assumed to be normally distributed. This allowed us to use familiar techniques such as the \\(t\\)-test, and to take baseline information into account in an accessible way (the linear model / ANCOVA). However, very often clinical trials do not have a continuous, normally distributed output, and in the next two sections we will look at two other common possibilities: binary data (this section) and survival data (next section). A binary outcome might be something like ‘the patient was alive 2 years after the procedure’ or not, or ‘the patient was clear of eczema within a month’ or not. Such variables are often coded as ‘success’ or ‘failure’, or 1 or 0. For a trial whose primary outcome variables are binary, the sample size calculations we derived in Chapter 2 will not work, so in this section we’ll work through a similar method developed for binary variables. Suppose we conduct a trial with a binary primary outcome variable and two groups, \\(T\\) and \\(C\\), containing \\(n_T\\) and \\(n_C\\) participants respectively. The number of successes in each group, \\(R_T\\) and \\(R_C\\), will be Binomially distributed, \\[\\begin{align*} R_T &amp;\\sim{Bi\\left(n_T,\\, \\pi_t\\right)} \\\\ R_C &amp;\\sim{Bi\\left(n_C,\\,\\pi_C\\right)}. \\end{align*}\\] Our null hypothesis now is therefore that \\(\\pi_T = \\pi_C\\), ie. that the probability of success is the same in each group, and we will need enough participants to test this hypothesis with sufficient power. With the trial data we will be able to produce estimates \\[\\begin{align*} p_T &amp; = \\frac{r_T}{n_T} \\\\ p_C &amp; = \\frac{r_C}{n_C}, \\end{align*}\\] where \\(r_T,\\,r_C\\) are the obseved values of \\(R_T,\\,R_C.\\) Recall that the variance of \\(p_X\\) (where \\(X\\) is \\(T\\) or \\(C\\)) is \\(\\frac{\\pi_X\\left(1-\\pi_X\\right)}{n_X}\\), such that the variance depends on the mean. This means there is no free parameter equivalent to \\(\\sigma\\) in the binary situation, and the number of participants required will depend on the approximate values of \\(\\pi_T\\) and \\(\\pi_C\\). This makes the derivation of a sample size formula somewhat more complicated, and so we first of all make a transformation to remove the dependence of mean and variance. To do this we use an approximation technique called the delta method. 5.1 The Delta Method We start with a random variable \\(X\\) that has mean \\(\\mu\\) and variance \\(\\sigma^2 = \\sigma^2\\left(\\mu\\right)\\), ie. its variance depends on its mean. If we have a ‘well-behaved’ (infinitely differentiable etc.) function \\(f\\left(X\\right)\\), what are its mean and variance? To find this exactly requires us to evaluate a sum or integral, and this may be analytically intractable, so we use instead a crude approximation. First, we expand \\(f\\left(X\\right)\\) in a first-order Taylor series about \\(\\mu\\), which gives us \\[\\begin{equation} f\\left(X\\right) \\approx f\\left(\\mu\\right) + \\left(X-\\mu\\right)f&#39;\\left(\\mu\\right) \\tag{5.1} \\end{equation}\\] and therefore \\[\\begin{equation} \\left(f\\left(X\\right) - f\\left(\\mu\\right)\\right)^2 \\approx \\left(X-\\mu\\right)^2\\left[f&#39;\\left(\\mu\\right)\\right]^2. \\tag{5.2} \\end{equation}\\] If we take expectations of Equation (5.1) we find \\(E\\left(f\\left(X\\right)\\right) \\approx f\\left(\\mu\\right)\\). We can use this in the left-hand side of Equation (5.2) so that when we take expectations of Equation (5.2) we find \\[\\begin{equation} \\operatorname{var}\\left(f\\left(X\\right)\\right) = \\sigma^2\\left(\\mu\\right)\\left[f&#39;\\left(\\mu\\right)\\right]^2, \\tag{5.3} \\end{equation}\\] where both sides come from \\[\\operatorname{var}\\left(X\\right) = \\operatorname{E}\\left[\\left(X - \\mu\\right)^2\\right] .\\] This series of approximations, which generally works well, is the Delta method. One way in which it is often used, and the way in which we will use it now, is to find a transformation \\(f\\left(X\\right)\\) for which (at least approximately) the variance is unrelated to the mean. To do this, we solve the differential equation \\[ \\operatorname{var}\\left[f\\left(X\\right)\\right] = \\sigma^2\\left(\\mu\\right) \\left[f&#39;\\left(\\mu\\right)\\right]^2 = \\text{constant}. \\] In the case of proportions for a binary variable, this becomes \\[ \\frac{\\pi\\left(1-\\pi\\right)}{n} \\left[f&#39;\\left(\\pi\\right)\\right]^2 = K\\] for some constant \\(K\\). We can rearrange this to \\[f&#39;\\left(\\pi\\right) = \\sqrt{\\frac{Kn}{\\pi\\left(1-\\pi\\right)} } \\propto \\sqrt{\\frac{1}{\\pi\\left(1-\\pi\\right)}}.\\] So we need \\[\\int^\\pi \\sqrt{\\frac{1}{u\\left(1-u\\right)}}du, \\] where the notation indicates that we want the anti-derivative, evaluated at \\(\\pi\\). By substituting \\(u = w^2\\) we find \\[\\begin{align*} f\\left(\\pi\\right) &amp; \\propto \\int^\\pi{\\frac{1}{\\sqrt{w^2\\left(1-w^2\\right)}}2w\\,dw}\\\\ &amp;\\propto \\int{\\frac{1}{\\sqrt{1 - w^2}}}dw\\\\ &amp; \\propto \\arcsin{\\left(\\sqrt{\\pi}\\right)}. \\end{align*}\\] Setting \\(f\\left(\\pi\\right) = \\arcsin\\left(\\sqrt{\\pi}\\right)\\) and using the chain rule, we find \\[\\left[f&#39;\\left(\\pi\\right)\\right]^2 = \\frac{1}{4\\pi\\left(1-\\pi\\right)} .\\] Finally, we can substitute this into Equation (5.3), with \\(f\\left(X\\right) = \\arcsin\\left(\\sqrt{X}\\right)\\) to find \\[\\begin{align*} \\operatorname{var}\\left[f\\left(\\pi\\right)\\right] &amp; \\approx \\sigma^2\\left(\\pi\\right)\\left[f&#39;\\left(\\pi\\right)\\right]^2 \\\\ &amp; \\approx{\\frac{\\pi\\left(1-\\pi\\right)}{n}\\cdot\\frac{1}{4\\pi\\left(1-\\pi\\right)}}\\\\ &amp; \\approx{\\frac{1}{4n}}, \\end{align*}\\] and we have achieved our aim of finding a transformation of \\(X\\) whose variance is not related to the mean. This is sometimes called the angular transformation. 5.2 A sample size formula For a binary variable, our estimate \\(p_X\\) (the proportion of successes in group \\(X\\)) is approximately normally distributed, since the central limit theorem applies. This is not true for small values of \\(n\\) (less than around 30, which is very small for a clinical trial) or for values of \\(\\pi\\) close to 0 or 1, say \\(\\pi&lt;0.15\\) or \\(\\pi&gt;0.85\\) (this is more likely to be an issue for some trials). The linear approximation in Equation (5.1) shows us that if \\(p_X\\) is normally distributed then \\(f\\left(p_X\\right) = \\arcsin\\left(\\sqrt{p_X}\\right)\\) will be [approximately] normally distributed too. In fact, \\(\\arcsin\\left(\\sqrt{p_X}\\right)\\) is approximately normally distributed with mean \\(\\arcsin{\\left(\\sqrt{\\pi_X}\\right)}\\) and variance \\(1/\\left(4n_X\\right)\\). Using this information, we can test \\(H_0:\\,\\pi_T =\\pi_C\\) at the 100\\(\\alpha\\)% confidence level by using the variable \\[ D = \\frac{\\arcsin{\\left(\\sqrt{p_T}\\right)} - \\arcsin{\\left(\\sqrt{p_C}\\right)}}{\\sqrt{\\frac{1}{4n_T} + \\frac{1}{4n_C}}}= \\frac{\\arcsin{\\left(\\sqrt{p_T}\\right)} - \\arcsin{\\left(\\sqrt{p_C}\\right)}}{\\frac{1}{2}\\lambda\\left(n_T,n_C\\right)}, \\] which is analogous to the variable \\(D\\) constructed in Section 2.3; the difference in \\(f\\left(p_T\\right)\\) and \\(f\\left(p_C\\right)\\) divided by the standard error of the difference. Using the same logic as in Sections 2.4 and 2.5, the starting place for a sample size formula to achieve significance level \\(\\alpha\\) and power \\(1-\\beta\\) is \\[ \\frac{2\\left(\\arcsin{\\left(\\sqrt{\\pi_T}\\right)} - \\arcsin{\\left(\\sqrt{\\pi_C}\\right)}\\right)}{\\lambda\\left(n_T,n_C\\right)} = z_\\beta + z_{\\frac{\\alpha}{2}}. \\] For two groups of equal size \\(N\\), this leads us to \\[\\begin{equation} N = \\frac{\\left(z_\\beta + z_{\\frac{\\alpha}{2}}\\right)^2}{2\\left(\\arcsin{\\left(\\sqrt{\\pi_T}\\right)} - \\arcsin{\\left(\\sqrt{\\pi_C}\\right)}\\right)^2}. \\tag{5.4} \\end{equation}\\] Because \\[\\arcsin{\\left(\\sqrt{\\pi_T}\\right)} - \\arcsin{\\left(\\sqrt{\\pi_C}\\right)}\\] is not a function of \\(\\pi_T - \\pi_C\\), we cannot express this in terms of the difference itself, but instead need to specify the expected probabilities of success in each group. In practice, it is likely that the success rate for the control group \\(\\left(\\pi_C\\right)\\) is well understood, and the probability for the intervention group \\(\\left(\\pi_T\\right)\\) can be specified by using the nearest clinically important value of \\(\\pi_T\\). Example 5.1 (From Smith et al. 1994) This trial compares two approaches to managing malignent low bile duct obstruction: surgical biliary bypass and endoscopic insertion of a stent. The primary outcome variable was ‘Did the patient die within 30d of the procedure?’, and the trial was designed to have \\(\\alpha=0.05,\\;1-\\beta=0.95\\), which gives \\(z_{\\frac{\\alpha}{2}}=1.96,\\,z_{\\beta} = 1.65\\). The trial wanted to be able to determine a change in 30 day mortality rate from 0.2 to at most 0.05. Plugging these numbers into Equation (5.4)) gives us \\[ N = \\frac{\\left(1.65 + 1.96\\right)^2}{2\\left(\\arcsin{\\left(\\sqrt{0.2}\\right)} - \\arcsin{\\left(\\sqrt{0.05}\\right)}\\right)^2} = 114.9, \\] and so each group in our trial should contain 115 patients. If instead our aim had been to detect a change from around 0.5 to 0.35 (the same in terms of \\(\\pi_A - \\pi_B\\)), we would instead have needed \\[ N = \\frac{\\left(1.65 + 1.96\\right)^2}{2\\left(\\arcsin{\\left(\\sqrt{0.5}\\right)} - \\arcsin{\\left(\\sqrt{0.35}\\right)}\\right)^2} = 280.8 ,\\] that is 281 patients per trial arm. References Smith, AC, JF Dowsett, RCG Russell, ARW Hatfield, and PB Cotton. 1994. “Randomised Trial of Endoscopic Steriting Versus Surgical Bypass in Malignant Low Bileduct Obstruction.” The Lancet 344 (8938): 1655–60. "],["binary-analysis.html", "6 Analysis for binary outcomes 6.1 Simple hypothesis tests 6.2 Measures of difference for binary data 6.3 Accounting for baseline observations: logistic regression 6.4 Diagnostics for logistic regression", " 6 Analysis for binary outcomes For a group of \\(2n\\) participants, we will have allocated \\(n_C\\) to the control group (group \\(C\\)), and \\(n_T\\) to the treatment group (group \\(T\\)). The natural statistical model to apply to this situation is therefore a binomial distribution, for example in group \\(C\\) the number of ‘successes’ would be modelled by \\[R_C \\sim \\operatorname{Bi}\\left(n_C,\\,\\pi_C\\right).\\] Similarly the number of successes in the treatment group can be modelled as \\[R_T \\sim\\operatorname{Bi}\\left(n_T,\\,\\pi_T\\right),\\] and the focus of our analysis is on comparing \\(\\pi_C\\) and \\(\\pi_T\\). To do this we will require point estimates of both quantities and interval estimates for some measure of the discrepancy between them. We will also need ways to test the null hypothesis that \\(\\pi_C = \\pi_T.\\) 6.1 Simple hypothesis tests We can tabulate the results of a trial with a binary outcome like this: Successes Failures Total Treatment \\(r_T\\) \\(n_T-r_T\\) \\(n_T\\) Control \\(r_C\\) \\(n_C-r_C\\) \\(n_C\\) Total \\(r\\) \\(n - r\\) \\(n\\) Note that because this is a table of observed values, they are now all in lower case. We can estimate \\(\\pi_C\\) and \\(\\pi_T\\) by the sample proportions \\[ \\begin{aligned} p_C &amp;= \\frac{r_C}{n_C}\\\\ p_T &amp;= \\frac{r_T}{n_T} \\end{aligned}. \\] and can use this data to test our null hypothesis that \\(\\pi_T = \\pi_C = \\pi\\). Example 6.1 The data for the examples in this section comes from Marshall (1948), in which 109 patients with tuberculosis were assigned to either receive Streptomycin, or the control group. The primary outcome variable is whether or not the patient was improved after the treatment period. The data include several other covariates, including gender, baseline condition (good, fair or poor) and whether the patient had developed resistance to streptomycin after 6 months. ## improved ## arm FALSE TRUE ## Streptomycin 17 38 ## Control 35 17 We therefore have \\[ \\begin{aligned} n_C &amp; = 52 \\\\ n_T &amp; = 55 \\\\ p_C &amp; = \\frac{17}{17+35} &amp; = 0.327\\\\ p_T &amp; = \\frac{38}{38+17} &amp; = 0.691\\\\ p &amp; = \\frac{38+17}{107} &amp;= 0.514. \\end{aligned} \\] 6.1.1 A simple method: chi-squared One way to approach this would be to conduct a chi-squared test. In a chi-squared test, we first calculate the expected values \\(\\left(E_i\\right)\\) in each box of the summary table (ie. the values we’d expect under \\(H_0\\)), and compare them to the observed values \\(\\left(O_i\\right)\\) by finding the summary statistic \\[ X^2 = \\sum \\frac{\\left(o_i - e_i\\right)^2}{e_i}.\\] Under the null hypothesis (that \\(\\pi_C = \\pi_T\\)) this has a \\(\\chi^2\\) distribution with one degree of freedom. We see that the larger the differences between the observed and expected values, relative to the expected values, the larger the test statistic, and therefore the less probable it is under the \\(\\chi^2_1\\) distribution. Example 6.2 Applying this to our streptomycin example, we can calculate a table of expected values by observing that proportion \\(p=0.514\\) of the total number of patients were improved. There are 52 in the control group, therefore we expect \\(0.514\\times 52 = 26.73\\) improved patients in the control group, and by the same logic \\(0.514\\times 55 = 28.27\\) in the treatment group. Our expected table is therefore ## improved ## arm FALSE TRUE ## Streptomycin 26.730 28.270 ## Control 25.272 26.728 We can therefore calculate the \\(\\chi^2\\) statistic by looping through the elements of the tables: sum_chi_sq = 0 # set a running total going # in the following, tab_obs is the table of observed values and # tab_exp is the table of expected values for (i in 1:2){ for (j in 1:2){ tmp = ((tab_obs[i,j] - tab_exp[i,j])^2)/tab_exp[i,j] sum_chi_sq = sum_chi_sq + tmp } } sum_chi_sq ## [1] 14.17595 1-pchisq(sum_chi_sq, df=1) ## [1] 0.0001664847 and we have sufficient evidence to reject \\(H_0\\) for \\(\\alpha=0.05\\) (or indeed for many much lower values of \\(\\alpha\\)). 6.1.2 Likelihood: A more rigorous way Our method above was quite informal, and also made heavy use of the central limit theorem. We can use maximum likelihood to derive a more formally justified test for binary outcomes. This also lays a good foundation for more complex situations. We will use \\(y_{iC}\\) to denote the outcome variable (0 or 1, in this case) of the \\(i\\)-th participant in the control group (and so on). The contribution of the \\(i\\)-th patient in group \\(C\\) to the likelihood is \\[\\pi_C^{y_{iC}}\\left(1 - \\pi_C\\right)^{1-y_{iC}} \\] (remember we can ignore multiplicative constant terms). Combining all \\(n_C\\) patients in group \\(C\\), their contribution will be \\[ \\pi_C^{r_C}\\left(1-\\pi_C\\right)^{n_C - r_C},\\] where \\(r_C\\) is the number of ‘successes’ in group \\(C\\). Similarly for the treatment group we will have \\[ \\pi_T^{r_T}\\left(1-\\pi_T\\right)^{n_T - r_T}.\\] Gathering these terms together we can find the complete likelihood function \\[ \\begin{aligned} L\\left(\\pi_C,\\pi_T \\mid \\left\\lbrace y_{iC}\\right\\rbrace, \\left\\lbrace y_{iT}\\right\\rbrace \\right) &amp; = L\\left( \\pi_C,\\pi_T \\mid {n_C,n_T, r_C, r_T}\\right)\\\\ &amp; \\propto \\pi_C^{r_C}\\left(1-\\pi_C\\right)^{n_C - r_C}\\pi_T^{r_T}\\left(1-\\pi_T\\right)^{n_T - r_T}. \\end{aligned} \\] The log-likelihood is therefore \\[ l\\left( \\pi_C,\\pi_T \\mid {n_C,n_T, r_C, r_T}\\right) = r_C\\log\\pi_C + \\left(n_C-r_C\\right)\\log\\left(1-\\pi_C\\right) + r_T\\log\\pi_T + \\left(n_T-r_T\\right)\\log\\left(1-\\pi_T\\right).\\] If we differentiate with respect to \\(\\pi_C\\), we find \\[\\frac{\\mathrm{d} l\\left( \\pi_C,\\pi_T \\mid {n_C,n_T, r_C, r_T}\\right)}{\\mathrm{d}\\pi_C} = \\frac{r_C}{\\pi_C} - \\frac{n_C-r_C}{1-\\pi_C}.\\] Setting this to zero we find (reassuringly!) that \\(\\hat\\pi_C = \\frac{r_C}{n_C}\\). We can repeat this exercise for \\(\\pi_T\\). If we assume that there is one common probability \\(\\pi\\) of success, we can find \\(\\hat\\pi\\) by maximising \\(l\\left(\\pi,\\pi \\mid {n_C,n_T, r_C, r_T}\\right)\\) with respect to \\(\\pi\\), and again this works out to be \\(\\frac{r_{C} + r_T}{n}\\). We can use these to construct a likelihood ratio test, by calculating \\[ \\begin{aligned} \\lambda_{LR} = &amp; -2\\left[l\\left( \\hat\\pi,\\hat\\pi \\mid {n_C,n_T, r_C, r_T}\\right) - l\\left( \\hat\\pi_C,\\hat\\pi_T \\mid {n_C,n_T, r_C, r_T}\\right)\\right]\\\\ = &amp; 2\\left[\\underbrace{r_C\\log\\frac{r_C}{n_C} + \\left(n_C-r_C\\right)\\log\\left(1-\\frac{r_C}{n_C}\\right) + r_T\\log\\frac{r_T}{n_T} + \\left(n_T-r_T\\right)\\log\\left(1-\\frac{r_T}{n_T}\\right) }_{l\\left( \\hat\\pi_C,\\hat\\pi_T \\mid {n_C,n_T, r_C, r_T}\\right)} \\right. \\\\ &amp;\\;\\;\\;\\;\\;\\; \\left. - \\underbrace{\\Big(r\\log\\left(p\\right) + \\left(n-r\\right)\\log\\left(1-p\\right)\\Big)}_{l\\left( \\hat\\pi,\\hat\\pi \\mid {n_C,n_T, r_C, r_T}\\right)}\\right]\\\\ =&amp; 2\\left[\\underbrace{r_C \\log\\left(\\frac{r_C}{n_C p}\\right)}_{\\text{Group }C\\text{ success}} + \\underbrace{\\left(n_C - r_C\\right)\\log\\left(\\frac{n_C - r_C}{n_C\\left(1-p\\right)}\\right)}_{\\text{Group }C\\text{ fail}} \\right.\\\\ &amp; \\;\\;\\;\\;\\;\\; \\left.+ \\underbrace{r_T \\log\\left(\\frac{r_T}{n_T p}\\right)}_{\\text{Group }T\\text{ success}} + \\underbrace{\\left(n_T - r_T\\right)\\log\\left(\\frac{n_T - r_T}{n_T\\left(1-p\\right)}\\right)}_{\\text{Group }T\\text{ fail}}\\right] \\end{aligned} \\] where we use \\(p,\\, r,\\, n\\) to denote the pooled values (\\(n = n_C + n_T\\) etc.). Each term in the final line corresponds to a subgroup of the participants, as labelled, and if we rearrange them slightly we see that this can be re-written as \\[\\lambda_{LR} = 2 \\sum\\limits_{i\\in G} o_i \\log\\left(\\frac{o_i}{e_i}\\right),\\] where \\(G\\) is the set of subgroups (group \\(C\\) success etc.). Under the null hypothesis that \\(\\pi_C = \\pi_T = \\pi\\), and for sufficiently large \\(n_C,\\;n_T\\), \\(\\lambda_{LR}\\) has a \\(\\chi^2\\) distribution with one degree of freedom. Example 6.3 Continuing with the streptomycin example, we can calculate this new test statistic in R by looping through the subgroups. sum_LR = 0 # set a running total going # in the following, tab_obs is the table of observed values and # tab_exp is the table of expected values for (i in 1:2){ for (j in 1:2){ tmp = tab_obs[i,j] * log(tab_obs[i,j]/tab_exp[i,j]) sum_LR = sum_LR + tmp } } teststat_LR = 2*sum_LR teststat_LR ## [1] 14.5028 1-pchisq(teststat_LR, df=1) ## [1] 0.0001399516 Not surprisingly, this value is quite close to the one we obtained earlier! 6.2 Measures of difference for binary data An important note: we’re treating \\(\\pi_T&gt;\\pi_C\\) as good here, as we would when the primary outcome is something positive, like a patient being cured. All the methods can easily be adapted to a situation where \\(\\pi_C&gt;\\pi_T\\) is desirable. In the above section the question we were interested in was ‘is there sufficient evidence to reject \\(H_0:\\,\\pi_C = \\pi_T\\)?’ and in our streptomycin example the answer was a resounding ‘Yes!’. However, if we then ask questions like ‘How big is the difference between the effects of each treatment?’ or ‘What is the treatment effect?’, things get a bit less clear. In the continuous case, it made sense to simply think about the treatment effect as the difference \\(\\mu_T - \\mu_C\\) between outcomes. However, in the binary case there are a few different ways we can think of the difference between two proportions \\(\\pi_C\\) and \\(\\pi_T\\), and each of them requires a different approach. 6.2.1 Absolute risk difference and Number Needed to Treat The absolute risk difference is \\[\\text{ARD} = \\pi_T - \\pi_C,\\] and is sometimes used. However, it loses a lot of information that we’d probably like to keep in some how. For example, suppose a treatment increases the proportion cured from \\(\\pi_C=0.01\\) to \\(\\pi_T=0.03\\). The absolute risk difference is \\(0.02\\) here. For some other treatment that results in an increase from \\(\\pi_C=0.55\\) to \\(\\pi_T = 0.57\\) we have the same absolute risk difference, even though it feels (and is!) a much less significant reduction. It is useful though to remember that usually these numbers are about people. If the outcome is ‘cured’ or ‘not cured’, then for some cohort of \\(N\\) patients, \\(N\\times\\text{ARD}\\) is the number of extra patients you would expect to cure if you used treatment \\(T\\) instead of treatment \\(C\\) (which may be nothing or may be some usual course of treatment). Linked to this is the number needed to treat (NNT), which is defined as \\[ \\text{NNT} = \\frac{1}{\\pi_T - \\pi_C} = \\frac{1}{\\text{ARD}}. \\] The NNT is the number of patients you’d need to treat (with treatment \\(T\\) rather than \\(C\\)) before you would bring benefit to one extra patient. The website TheNNT collects together results from many clinical trials and uses the NNT as a summary. Some of the results are quite surprising, compared to how effective we think medicines are! Note that the NNT doesn’t tell us how many patients are cured: it is just a measure of how much more effective the treatment is than the control. In both the examples above, with ARD=0.02, we have NNT=50. However, in the first example we would expect around 0-1 patients out of 50 cured under the control and 1-2 out of 50 cured under the treatment. In the second case, we would expect 27-28 cured out of 50 under the control, and 28-29 out of 50 cured under the treatment. The NNT is popular as a clinical benchmark, and provides useful intuition in terms of the number of people it will help. For example, if \\(\\pi_T = 0.25,\\,\\pi_C=0.2\\), then \\(\\text{ARD} = 0.05\\) and \\(\\text{NNT} = 20.\\) After treating 20 patients with treatment \\(C\\) we expect to cure (say) 4, whereas treating 20 patients with treatment \\(T\\) it is expected that we will cure 5. For very small proportions, the NNT can be large even for what appears to be an important difference. For example, if \\(\\pi_C=0.005\\) and \\(\\pi_T = 0.015\\) then \\(\\text{ARD}=0.01\\) and \\(\\text{NNT}=100\\). It might be decided that the necessary changes and costs are not worth it for such a small difference. Although the NNT is popular with medics, it is not the easiest statistic to work with, as we shall see! 6.2.1.1 Confidence intervals for ARD and NNT Let’s suppose we want to work with the ARD, and to make a confidence interval for the treatment difference \\(\\tau_{ARD} = \\pi_T - \\pi_C\\). Using the same normal approximation as before, we can estimate \\(\\tau_{ARD}\\) by \\(p_T - p_C\\), and \\(\\operatorname{var}\\left(p_T - p_C\\right)\\) by \\[ \\frac{p_T\\left(1-p_T\\right)}{n_T} + \\frac{p_C\\left(1-p_C\\right)}{n_C}.\\] Our \\(100\\left(1-\\alpha\\right)\\)% confidence interval is therefore given by \\[\\left(p_T - p_C - z_{\\frac{\\alpha}{2}}\\sqrt{\\frac{p_T\\left(1-p_T\\right)}{n_T} + \\frac{p_C\\left(1-p_C\\right)}{n_C}},\\; p_T - p_C + z_{\\frac{\\alpha}{2}}\\sqrt{\\frac{p_T\\left(1-p_T\\right)}{n_T} + \\frac{p_C\\left(1-p_C\\right)}{n_C}}\\right) \\] Example 6.4 Back to our streptomycin example, we can now construct a \\(100\\left(1-\\alpha\\right)\\)% confidence interval for the ARD. Our estimated treatment effect is (to 3 decimal places) \\[\\hat\\tau_{ARD}=p_T - p_C = \\frac{38}{55} - \\frac{17}{52} = 0.364.\\] Our estimate of the standard error of \\(\\hat\\tau_{ARD}\\) is \\[ \\begin{aligned} \\frac{p_T\\left(1-p_T\\right)}{n_T} + \\frac{p_C\\left(1-p_C\\right)}{n_C} &amp; = \\frac{\\frac{38}{55}\\times \\frac{17}{55}}{55} + \\frac{\\frac{17}{52}\\times \\frac{35}{52}}{52}\\\\ &amp; = 0.00811 \\end{aligned} \\] and therefore a 95% confidence interval for \\(\\tau_{ARD}\\) is \\[\\left(0.364 - z_{0.975}\\sqrt{0.0811},\\; 0.364 + z_{0.975}\\sqrt{0.0811}\\right) = \\left(0.187,\\; 0.541\\right). \\] As we should expect from the very low \\(p\\)-values we saw in the examples in Section 6.1, the 95% confidence interval does not contain zero. If we want to think instead in terms of NNT (the number needed to treat), then we need to find the reciprocal of our estimate of \\(\\tau_{ARD}\\): \\[ \\text{NNT} = \\frac{1}{\\tau_{ARD}} = \\frac{1}{0.364} = 2.75.\\] That is, we would expect to treat nearly three patients before one is improved (in terms of their tuberculosis symptoms). We can use the limits of the 95% CI for \\(\\tau_{ARD}\\) to form a 95% CI for NNT, simply by taking the reciprocals of the limits to get \\[\\left(\\frac{1}{0.541},\\; \\frac{1}{0.187}\\right) = \\left(1.85,\\; 5.33 \\right).\\] Because the NNT is the reciprocal of something approximately normally distributed, it has a distribution with a long tail, and we see that the confidence interval is therefore skewed. 6.2.1.2 What if the difference is not significant? In the above section you might have already wondered what happens if the confidence interval for the absolute risk difference (ARD) contains zero. To illustrate this, we will make up some data for a small trial, loosely based on the Streptomycin data we’ve been using. The dataset for our made-up trial is Successes Failures Total Treatment 9 5 14 Control 4 8 12 Total 13 13 26 The ARD is now \\[\\frac{9}{14} - \\frac{4}{12} = \\frac{3}{13} \\approx 0.310 \\] and our 95% confidence interval for \\(\\tau_{ARD}\\) is \\(\\left(-0.0567,\\;0.676\\right)\\). Clearly because of the small size of the trial our confidence interval is very wide (this is not a very good trial!), but the important thing to note is that it now contains zero. It looks very likely that the treatment is effective (the interval only just contains zero) but how many patients might we need to treat before we expect to see an extra success? The expected value of NNT is \\[ \\frac{1}{0.310} = 3.23,\\] which does not pose a problem. However, our 95% confidence interval now contains the possibility that the ARD is zero, and in this case the NNT is in some sense infinite: no matter how many patients we treat, we don’t expect to see any extra improvements. Therefore, since our confidence interval for ARD contains zero it feels appropriate that our confidence interval for NNT should contain infinity. When thinking about a confidence interval for the NNT, we need to think about signs, and what negative and positive values mean. If both the lower and upper limits of the confidence interval for ARD are positive, there is no issue - the treatment is effective, and our NNT confidence interval is another entirely positive interval. If the confidence interval for ARD is entirely negative, we have an entirely negative interval for NNT. A negative value of NNT can be thought of as the ‘number needed to treat to harm one extra person’. The tricky situation is when the confidence interval for the ARD is \\(\\left(-L, U\\right)\\) with \\(L,U&gt;0\\), ie. an interval containing zero. As we approach zero from \\(U\\), the upper limit of the CI for \\(\\pi_T - \\pi_C\\), the number of patients we need to treat increases, since the treatment effect is getting smaller, until at \\(\\pi_T - \\pi_C=0\\) the NNT is infinite. Therefore, the part of the CI for NNT corresponding to the positive part of the CI for ARD is \\[\\left(\\frac{1}{U},\\; \\infty\\right)\\] As we approach zero from the left in the interval (ie. from \\(-L\\)), the treatment gets less and less effective (and right now we mean effective in a bad way, likely doing harm to the patients compared to the control), and so we need to treat more and more patients to harm one extra patient compared to the control. In this region the NNT is negative, since if we deny some patients the treatment we will benefit a few. Therefore the CI for the NNT corresponding to the negative part of the CI for ARD is \\[\\left(-\\infty,\\;-\\frac{1}{L}\\right), \\] and altogether the confidence interval for the number needed to treat (NNT) is the union of these two intervals \\[\\left(-\\infty,\\;-\\frac{1}{L}\\right) \\cup \\left(\\frac{1}{U},\\; \\infty\\right).\\] The plot below shows relationship between ARD and NNT, with the intervals for our toy example shown in bold on the respective axis (the NNT interval should continue infinitely in both directions so for obvious reasons this is not all shown!). Figure 6.1: The relationship between the confidence interval for the ARD and the NNT, when the ARD interval contains zero. Altman (1998) (available here) makes a compelling push for the use of confidence intervals for the number needed to treat. You can decide for yourself whether you agree! Problems with the confidence interval for the ARD You may well remember from the dim and distant past that the method we have been using so far (which in this section we’ll be calling the ‘standard’ method) is not so reliable if the proportion is close to zero or one. Newcombe (1998) compared eleven different methods for finding confidence intervals for the difference in proportions (as we are doing when we work with the ARD) and found the standard method to be the worst! The coverage probability turns out to be much lower than the nominal value, with a so-called 95% confidence interval being closer to 90% or even 85%. A further problem with this method (although it will rarely affect us in practice in this setting) is that the limits of the confidence interval aren’t forced to be in \\(\\left[-1,1\\right]\\). We will give a sketch of the favourite method of Newcombe (1998), chosen for its ease of implementation and its accuracy, now. This is the Wilson method, first presented in Wilson (1927). The first step is to find an interval estimate for a single proportion \\(\\pi\\). As before, this can be written \\[\\left\\lbrace \\pi \\mid \\frac{\\lvert p - \\pi \\rvert}{\\sqrt{\\pi\\left(1-\\pi\\right)/n}} \\leq z_{\\frac{\\alpha}{2}} \\right\\rbrace = \\left\\lbrace \\pi \\mid \\left(p - \\pi\\right)^2 \\leq z^2_{\\frac{\\alpha}{2}}\\frac{\\pi\\left(1-\\pi\\right)}{n} \\right\\rbrace. \\] We can find the limits of the \\(100\\left(1-\\alpha\\right)\\)% level confidence interval by changing the right hand side to an equality \\[\\begin{equation} \\left(p - \\pi\\right)^2 = z^2_{\\frac{\\alpha}{2}}\\frac{\\pi\\left(1-\\pi\\right)}{n}. \\tag{6.1} \\end{equation}\\] In the standard method, we substitute \\(p\\) (the estimated value of \\(\\pi\\) from our sample) into the right hand side of Equation (6.1) for \\(\\pi\\), to get \\[\\left(p - \\pi\\right)^2 = z^2_{\\frac{\\alpha}{2}}\\frac{p\\left(1-p\\right)}{n}\\] which we solve to get the limits \\[ \\pi = p \\pm z_{\\frac{\\alpha}{2}}\\sqrt{\\frac{p\\left(1-p\\right)}{n}}.\\] In Wilson’s method, we instead keep \\(\\pi\\) in the right hand side and solve the quadratic in Equation (6.1) in terms of \\(\\pi\\). The benefit of this new method will be most obvious for a probability that is close to 0 or 1. Example 6.5 Suppose we have 1 success out of 50 patients, so \\(p=0.02,\\;n=50\\). The limits of a standard 95% confidence interval will be \\[\\left(0.02 - z_{0.975}\\sqrt{\\frac{0.02\\times{0.98}}{50}},\\; 0.02 + z_{0.975}\\sqrt{\\frac{0.02\\times{0.98}}{50}}\\right) = \\left(-0.0188,\\;0.0588\\right),\\] whereas the limits to the Wilson 95% CI will be the roots of \\[\\left(0.02-\\pi\\right)^2 = z^2_{\\alpha/2}\\frac{\\pi\\left(1-\\pi\\right)}{50}\\] which work out to be ## [1] 0.004 0.105 Visually, we can represent this as in Figure 6.2 by plotting the LHS (solid) and RHS (dashed for new method, dotted for standard method). The thick solid red line shows \\(p_T\\), the estimated proportion, the thinner dashed red lines show the Wilson 95% CI and the dotted red lines show the standard 95% CI. Notice that the limits of each confidence interval are formed by the points at which the solid line (LHS) crosses the dashed / dotted lines (RHS). Figure 6.2: LHS of Equation 6.1 (solid black) with RHS of (black dashed) and RHS with estimate \\(p\\) subbed in (black dotted). The limits of the confidence intervals are where the curves cross, shown by red lines: dashed for Wilson, dotted for standard. Example 6.6 Returning to our streptomycin example, our estimate of the probability of success for the treatment group is \\(p_T = \\frac{38}{55},\\;n_T = 55\\), and therefore our equation becomes \\[\\left(\\frac{38}{55} - \\pi\\right)^2 = z^2_{\\frac{\\alpha}{2}}\\frac{\\pi\\left(1-\\pi\\right)}{55}.\\] Solving this equation in the usual way (using the quadratic formula) we find the Wilson 95% CI to be ## [1] 0.56 0.80 By contrast, in our standard method we have \\[\\left(\\frac{38}{55} - \\pi\\right)^2 = z^2_{\\frac{\\alpha}{2}}\\frac{\\frac{38}{55}\\left(1-\\frac{38}{55}\\right)}{55}\\] which is ## [1] 0.57 0.81 We can see this graphically Figure 6.3: As before, dashed for Wilson, dotted for standard Notice that the Wilson interval is asymmetric, which is more realistic. Similarly for the control proportion \\(\\pi_C\\), we have \\(p_C = \\frac{17}{52},\\; n_C=52\\), and our Wilson interval is ## [1] 0.22 0.46 compared to the standard confidence interval ## [1] 0.20 0.45 Again, we can see this graphically. Figure 6.4: As before, dashed for Wilson, dotted for standard 6.2.1.3 Extending this to \\(\\pi_T - \\pi_C\\) What the Wilson interval has given us is a superior method for creating confidence intervals for proportions. But, what we would like is a method for calculating a confidence interval for the difference in two proportions. You’ll be relieved to hear that there is such a method, and we’ll give a sketch here of how it works. The limits of the ‘standard method’ confidence interval at significance level \\(\\alpha\\) are given by \\[\\begin{equation} \\left(p_T - p_C - z_{\\frac{\\alpha}{2}}\\sqrt{\\frac{p_T\\left(1-p_T\\right)}{n_T} + \\frac{p_C\\left(1-p_C\\right)}{n_C}},\\; p_T - p_C + z_{\\frac{\\alpha}{2}}\\sqrt{\\frac{p_T\\left(1-p_T\\right)}{n_T} + \\frac{p_C\\left(1-p_C\\right)}{n_C}}\\right). \\tag{6.2} \\end{equation}\\] We can rewrite this as \\[\\begin{equation} \\left(p_T - p_C - \\sqrt{\\omega^2_T + \\omega^2_C},\\; p_T - p_C + \\sqrt{\\omega^2_T + \\omega^2_C}\\right) \\end{equation}\\] where \\(\\omega_T\\) and \\(\\omega_C\\) are the widths of the separate single-sample ‘standard’ confidence intervals for \\(p_T\\) and \\(p_C\\). In Wilson’s method, we proceed in the same way, but instead use the widths of the Wilson confidence intervals for the individual probabilities \\(p_T\\) and \\(p_C\\). This is obviously a little more complicated, since the widths (eg. \\(p_T - l_T\\) and \\(u_T - p_T\\)) will now not be the same, since the Wilson CI is not symmetrical. So, we have \\[ \\left(p_T - p_C - \\sqrt{\\left(p_T-l_T\\right)^2 + \\left(u_C - p_C\\right)^2},\\; p_T - p_C + \\sqrt{\\left(u_T - p_T\\right)^2 + \\left(p_C - l_C\\right)^2}\\right). \\] These differences must be calculated using the individual sample confidence interval method. Example 6.7 Applying this Wilson method to our Streptomycin example, recall that we have \\[ \\begin{aligned} p_T &amp; = \\frac{38}{55}\\\\ p_T - l_T &amp; = \\frac{38}{55} - 0.5597 = 0.1312\\\\ u_T - p_T &amp; = 0.7972 - \\frac{38}{55} = 0.1064\\\\ p_C &amp; = \\frac{17}{52} \\\\ p_C - l_C &amp; = \\frac{17}{52} - 0.2152 = 0.1117\\\\ u_C - p_C &amp; = 0.4624 - \\frac{17}{52} = 0.1355. \\end{aligned} \\] Our \\(95\\%\\) confidence interval is therefore \\[ \\begin{aligned} \\left(p_T - p_C - \\sqrt{\\left(p_T-l_T\\right)^2 + \\left(u_C - p_C\\right)^2}\\right.&amp;,\\left. p_T - p_C + \\sqrt{\\left(u_T - p_T\\right)^2 + \\left(p_C - l_C\\right)^2}\\right)\\\\ \\left(\\frac{38}{55}-\\frac{17}{52} - \\sqrt{0.1312^2 + 0.1355^2}\\right.&amp;,\\left.\\frac{38}{55}-\\frac{17}{52} + \\sqrt{0.1064^2 + 0.1117^2}\\right)\\\\ \\left(0.3640 - 0.1886 \\right.&amp;,\\left. 0.3640+ 0.1543\\right)\\\\ \\left(0.157 \\right.&amp;,\\left.0.500\\right). \\end{aligned} \\] This is skewed somewhat lower than our standard CI of \\(\\left(0.187,\\;0.541\\right).\\) Returning to our earlier material, we could invert this interval to find a confidence interval for the number needed to treat (NNT). 6.2.2 Risk Ratio (RR) and Odds ratio (OR) The measures we have looked at so far, particularly the ARD, are quite analagous to the continuous normally distributed case. However, there are yet more commonly used measures of difference for proportions, which need to be dealt with differently, but also afford more opportunities for modelling. The risk ratio is defined as \\[\\text{RR} = \\frac{\\pi_T}{\\pi_C}\\] The odds ratio is defined as \\[\\text{OR} = \\frac{\\pi_T/\\left(1-\\pi_T\\right)}{\\pi_C/\\left(1-\\pi_C\\right)}\\] The first thing to note is that for both the risk ratio and the odds ratio, the null value is one (not zero, as for the ARD), and both values must always be positive. We think about things multiplicatively, so for example if \\(RR=3\\) we can say that the event is “3 times more likely” in group \\(T\\) than in group \\(C\\). Odds Odds and odds ratios are a bit trickier to think about (this article explains them really well - it’s aimed at ‘kids and teens’ but don’t let that put you off!). The odds of an event are the probability of it happening over the probability of it not happening. So, if (for some event \\(A\\)), \\(p\\left(A\\right)=0.2\\), the odds of \\(A\\) are \\[\\frac{p\\left(A\\right)}{p\\left(A&#39;\\right)} = \\frac{0.2}{0.8} = \\frac{1}{4}, \\] which we say as “1 to 4” or 1:4. For every one time \\(A\\) occurs, we expect it not to occur four times. The odds ratio compares the odds of the outcome of interest in the Treament group with the odds of that event in the Control group. It tells us how the odds of the event are affected by the treatment (vs control). With the ARD, if we compare treatments in one direction (say \\(p_T - p_C\\)) we would obtain the negative of the interval for the other way (\\(p_C - p_T\\)). With the RR and OR, the discrepancy between two proportions is given by a ratio, and so comparing them in one direction (\\(p_T / p_C\\)) will give the reciprocal of the other direction (\\(p_C / p_T\\)). Example 6.8 For our Streptomycin example, we estimated the ARD by \\[\\hat\\tau_{ARD}=p_T - p_C = \\frac{38}{55} - \\frac{17}{52} = 0.364,\\] or could have alternatively had \\[\\hat\\tau_{ARD}=p_C - p_T = \\frac{17}{52} - \\frac{38}{55} = - 0.364.\\] For the risk ratio, we have \\[\\hat{\\tau}_{RR} = \\frac{p_T}{p_C} = \\frac{38/55}{17/52} = 2.113,\\] or could alternatively have \\[\\hat{\\tau}_{RR} = \\frac{p_C}{p_T} = \\frac{17/52}{38/55} = 0.473 = \\frac{1}{2.113}.\\] We could say that a patient is “more than twice as likely to be cured with streptomycin than by the control”. For the odds ratio, we have \\[\\hat{\\tau}_{OR} = \\frac{p_T/\\left(1-p_T\\right)}{p_C/\\left(1-p_C\\right)} = \\frac{(38/55)/(17/55)}{(17/52)/(35/52)} = 4.602, \\] and therefore the odds of recovery are around 4.6 greater for Streptomycin than for the control. Similarly, we could reframe this as \\[\\hat{\\tau}_{OR} = \\frac{p_C/\\left(1-p_C\\right)}{p_T/\\left(1-p_T\\right)} = \\frac{(17/52)/(35/52)}{(38/55)/(17/55)} = 0.217 = \\frac{1}{4.602}.\\] 6.2.2.1 Confidence intervals for RR and OR One thing to notice is that symmetry works differently on the RR and OR scale from on the ARD scale. There is an equivalence between an interval \\(\\left(l,\\,u\\right)\\) (with \\(l,u&gt;1\\)) and \\(\\left(\\frac{1}{u},\\frac{1}{l}\\right)\\), since these intervals would equate to comparing the same two treatments in different directions (assuming the difference was significant and neither interval contains 1). Similarly, on this scale the interval \\[\\left(\\frac{1}{k},\\,k\\right) \\text{ for some }k&gt;1 \\] can be thought of as symmetric, in that one treatment may be up to \\(k\\) times more effective than the other, in either direction. Therefore, to build a confidence interval for OR or RR, we will not be following the usual formula \\[\\text{point estimate } \\pm{z\\times{SE}}.\\] You may have already been thinking that a log transformation would be useful here, and you’d be correct! The sort-of symmetric intervals we’ve been discussing here actually are symmetric (about zero) on the log scale. Firstly we’ll consider the risk ratio. Let’s define \\[ \\phi = \\log\\left(\\frac{\\pi_T}{\\pi_C}\\right).\\] The natural way to estimate this is with the sample proportions \\[\\log\\left(\\frac{p_T}{p_C}\\right) = \\log\\left(p_T\\right) - \\log\\left(p_C\\right).\\] These estimated proportions should be approximately normal and independent of one another, and so \\(\\log\\left(\\frac{p_T}{p_C}\\right)\\) is approximately normal with mean \\(\\phi\\) (the true value) and variance \\[\\operatorname{var}\\left(\\log\\left(p_T\\right)\\right) + \\operatorname{var}\\left(\\log\\left(p_C\\right)\\right). \\] We can now apply the Delta method (see section 5.1) to find that (using Equation (5.3)) \\[\\operatorname{var}\\left[\\log\\left(p_T\\right)\\right] = \\operatorname{var}\\left[\\log\\left(\\frac{r_T}{n_T}\\right)\\right] \\approx \\frac{\\pi_T\\left(1-\\pi_T\\right)}{n_T}\\times{\\left(\\frac{1}{\\pi_T}\\right)^2} = \\frac{1}{n_T\\pi_T} - \\frac{1}{n_T}. \\] Since we estimate \\(\\pi_T\\) by \\(r_T/n_T\\) this can be estimated by \\(r_T^{-1} - n_T^{-1}\\). Notice that we are relying on the derivative of \\(\\log\\left(x\\right)\\) being \\(x^{-1}\\), so we must always use natural logarithms. This leads us to the result that, approximately \\[\\log\\left(\\frac{p_T}{p_C}\\right) \\sim N\\bigg(\\phi,\\,\\left(r_T^{-1} - n_T^{-1}\\right) + \\left(r_C^{-1} - n_C^{-1}\\right) \\bigg) \\] and so we can generate \\(100\\left(1-\\alpha\\right)\\)% confidence intervals for \\(\\phi\\) as \\(\\left(l_{RR},\\;u_{RR}\\right)\\), where the limits are \\[ \\log\\left(\\frac{p_T}{p_C}\\right) \\pm z_{\\frac{\\alpha}{2}}\\sqrt{\\left(r_T^{-1} - n_T^{-1}\\right) + \\left(r_C^{-1} - n_C^{-1}\\right)}. \\] This then translates to an interval for the risk ratio itself of \\(\\left(e^{l_{RR}},e^{u_{RR}}\\right)\\). Example 6.9 Returning once again to our streptomycin example, recall that we have \\[ \\begin{aligned} r_T &amp; = 38\\\\ n_T &amp; = 55 \\\\ r_C &amp; = 17 \\\\ n_C &amp; = 52 \\end{aligned} \\] and so the limits of the confidence interval (with \\(\\alpha=0.05\\)) on the log scale are \\[\\log\\left(\\frac{38/55}{17/52}\\right) \\pm 1.96\\sqrt{\\left(\\frac{1}{38} - \\frac{1}{55}\\right) + \\left(\\frac{1}{17} - \\frac{1}{52}\\right)} = \\log(2.11) \\pm 1.96 \\times 0.218\\] which gives us \\(\\left(0.320,\\,1.176\\right)\\) on the log scale, and a 95% CI for the risk ratio of \\(\\left(1.377,\\,3.243\\right)\\). So, we’ve seen that we can find confidence intervals for each of our four measures of difference. But we probably want to also be able to incorporate baseline measurements, as we did for continuous outcome variables. 6.3 Accounting for baseline observations: logistic regression We saw with the continuous outcomes that it is often advantageous to include baseline measurements of the outcome (if they are known) in our analysis, and this is no less true for binary outcomes. In this section we use the term ‘baseline observations’ to mean any measurement that was known before the trial started. Unlike with continuous measurements, with a binary outcome, there is not usually a pre-trial value of the primary outcome. A binary outcome is often already relative to pre-trial (for example ‘Have the patient’s symptoms improved?’) or refers to an event that definitely wouldn’t have happened pre-trial (for example ‘Did the patient die within the next 6 months?’ or ‘Was the patient cured?’). However, as we saw with ANCOVA, we can include other sorts of covariates in a linear model, so this is fine. The general form of model that we would like for patient \\(i\\) is \\[\\text{outcome}_i = \\mu + \\tau G_i + \\beta_1\\times{\\text{baseline}_{1i}} + \\ldots + \\beta_p\\times{\\text{baseline}_{pi}} + \\text{error}_i,\\] where \\(G_i\\) is an indicator function taking values 1 if patient \\(i\\) was in group \\(T\\) and 0 if they were in group \\(C\\), and \\(\\text{baseline}_1,\\;\\ldots,\\;\\text{baseline}_p\\) are \\(p\\) baseline measurements that we would like to take into account. However, this actually creates quite a few problems with binary variables. The outcome for patient \\(i\\) will be either 0 or 1, but the terms in the model above do not guarantee this at all. Adding a normally distributed error term doesn’t really make sense in this context, so we will remove it. We can also make the LHS more continuous by thinking of the mean outcome rather than a single outcome. This makes sense, since if several patients were identical to patient \\(i\\) (in the sense of having the same baseline covariate values and being allocated to the same treatment), we probably wouldn’t expect them all to have exactly the same outcome. Therefore we might instead think in terms of mean outcome, in which case our model becomes \\[\\text{mean outcome}_i = \\mu + \\tau G_i + \\beta_1\\times{\\text{baseline}_{1i}} + \\ldots + \\beta_p\\times{\\text{baseline}_{pi}}.\\] There is one final problem to overcome, which is that the LHS will certainly be in \\(\\left[0,\\;1\\right]\\), but the RHS could take any value. To address this we need to use a transformation, to take the mean outcome from \\(\\left[0,1\\right]\\) to \\(\\mathbb{R}\\). The transformation that is usually used for a binary variable is the logit function, which is the log of the odds, \\[\\operatorname{logit}\\left(\\pi\\right) = \\log\\frac{\\pi}{1-\\pi}.\\] As \\(\\pi\\) tends to zero, \\(\\operatorname{logit}\\left(\\pi\\right)\\) tends to \\(-\\infty\\), and as \\(\\pi\\) tends to one, \\(\\operatorname{logit}\\left(\\pi\\right)\\) tends to \\(\\infty\\). The derivative of the \\(\\operatorname{logit}\\) function is \\[ \\frac{d\\operatorname{logit}\\left(\\pi\\right)}{d\\pi} = \\frac{1}{\\pi\\left(1-\\pi\\right)}\\] which is always positive for \\(\\pi\\in\\left[0,1\\right]\\). This means that we can use it to transform our mean outcome (which we will now call \\(\\pi\\), since the mean outcome is the estimate of the probability of success) in the model \\[\\begin{equation} \\operatorname{logit}\\left(\\pi\\right) = \\mu + \\tau G + \\beta_1\\times{\\text{baseline}_{1}} + \\ldots + \\beta_p\\times{\\text{baseline}_{p}} \\tag{6.3} \\end{equation}\\] and any value in \\(\\mathbb{R}\\) is allowed on both sides. This model is known as logistic regression, and belongs to a class of models called Generalized Linear Models. If you did Advanced Statistical Modelling III you’ll have seen these before. If you haven’t seen them, and want to know more, this article gives a nice introduction (and some useful R tips!). 6.3.1 What does this model tell us? We now have an equation for a model that makes sense, but what is it actually modelling? And what does it tell us about the effect of the treatment? Consider the difference between two patients who are the same in every respect except one is assigned to group \\(C\\) (so \\(G=0\\)) and the other to group \\(T\\) (so \\(G=1\\)). The model gives: \\[ \\begin{aligned} \\operatorname{logit}\\left(\\pi\\right) = \\log\\left(\\frac{\\pi}{1-\\pi}\\right) = \\log\\left(\\text{Odds of success}\\right) &amp; = \\mu + \\tau + \\beta_1x_1 + \\ldots + \\beta_px_p &amp; \\text{ (group T)}\\\\ \\operatorname{logit}\\left(\\pi\\right) = \\log\\left(\\frac{\\pi}{1-\\pi}\\right) = \\log\\left(\\text{Odds of success}\\right) &amp; = \\mu + \\beta_1x_1 + \\ldots + \\beta_px_p &amp; \\text{ (group C)} \\end{aligned} \\] Subtracting one from the other, we find \\[ \\begin{aligned} \\log(\\text{Odds of success for group T}) - &amp; \\log(\\text{Odds of success for group C})\\\\ &amp;= \\log\\left(\\frac{\\text{Odds of success for group T}}{\\text{Odds of success for group C}}\\right) = \\log\\left(OR\\right) \\\\ &amp;= \\tau. \\end{aligned} \\] That is, \\(\\tau\\) is the log of the odds ratio, or \\(e^\\tau\\) is the odds ratio adjusted for variables \\(x_1,\\;\\ldots,\\;x_p\\). Put another way, while the baseline covariates \\(x_1,\\ldots,x_p\\) affect the probability of ‘success’ (or whatever our binary outcome’s one means), \\(\\tau\\) is a measure of the effect of the treatment compared to control given some set of baseline covariate values. 6.3.2 Fitting a logistic regression model Logistic regression models are generally fitted using maximum likelihood. In the notation of Equation (6.3), the parameters we need to fit are the coefficients \\(\\mu,\\;\\tau\\) and \\(\\beta_1,\\ldots,\\beta_p\\). To ease notation for this section we will collect these into a vector \\(\\boldsymbol\\beta\\), with \\(\\beta_0=\\mu\\) and \\(\\beta_{p+1}=\\tau\\), so that our \\(G\\) variable is now \\(x_{p+1}\\). This notation allows us to write the linear function on the RHS of Equation (6.3) for participant \\(i\\) as \\[x_i^T\\boldsymbol\\beta = \\sum\\limits_{j=0}^{p+2} x_{ij}\\beta_j, \\] where \\(x_{i0}=1\\) (so that \\(\\beta_0\\) is the intercept \\(\\mu\\)) \\(x_{i1},\\ldots,x_{ip}\\) are the baseline covariates. \\(x_{i,p+1}= \\begin{cases} 0\\text{ if participant }i\\text{ is in group }C\\\\ 1\\text{ if participant }i\\text{ is in group }T \\end{cases}\\) If \\(\\pi_i\\) is the probability that the outcome for participant \\(i\\) is 1, where \\(i=1,\\ldots,n\\), then the logistic model specifies these \\(n\\) parameters through the \\(p+2\\) parameters \\(\\beta_j\\), via the \\(n\\) expressions \\[\\begin{equation} \\operatorname{logit}\\left(\\pi_i\\right) = x_i^T\\boldsymbol\\beta. \\tag{6.4} \\end{equation}\\] Using the Bernoulli distribution, the log-likelihood given data \\(y_1,\\ldots,y_n\\) is \\[\\begin{align*} \\ell\\left(\\left\\lbrace\\pi_i \\right\\rbrace \\mid\\left\\lbrace y_i\\right\\rbrace\\right) &amp; = \\sum\\limits_{i=1}^n\\left[y_i\\log(\\pi_i) + \\left(1-y_i\\right)\\log\\left(1-\\pi_i\\right)\\right]\\\\ &amp; = \\sum\\limits_{i=1}^n\\left[y_i\\log\\left(\\frac{\\pi_i}{1-\\pi_i}\\right) + \\log\\left(1-\\pi_i\\right)\\right], \\end{align*}\\] where \\(y_i=0\\) or 1 is the outcome for participant \\(i\\). Using Equation (6.4) we can rewrite this in terms of \\(\\boldsymbol\\beta\\) as \\[\\ell\\left(\\left\\lbrace\\beta_j \\right\\rbrace\\mid{\\text{data}}\\right) = \\sum\\limits_{i=1}^n \\left[y_i x_i^T\\boldsymbol\\beta - \\log\\left(1+e^{x_i^T\\boldsymbol\\beta}\\right)\\right].\\] The fitted model is then the one with the values \\(\\beta_j\\), \\(j=0,\\dots,q\\), that maximise this expression (and hence maximise the likelihood itself), which we will label the \\(\\left\\lbrace \\hat{\\beta}_j\\right\\rbrace\\). This is generally done some via some numerical method, and we won’t go into that here. The method used by R will generate the MLE \\(\\hat\\beta_j\\) for each \\(\\beta_j\\), and also an estimate of the standard error of each \\(\\hat\\beta_j\\). In particular there will be an estimate of the standard error of \\(\\hat\\beta_1\\), better known as \\(\\hat\\tau\\), the estimate of the treatment effect. This is important, because it means we can test the hypothesis that \\(\\tau=0\\), and can form a confidence interval for the adjusted log odds ratio. Example 6.10 This study is detailed in Elmunzer et al. (2012). ERCP, or endoscopic retrograde cholangio-pancreatogram, is a procedure performed by threading an endoscope through the mouth to the opening in the duodenum where bile and pancreatic digestive juices are released into the intestine. ERCP is helpful for treating blockages of flow of bile (gallstones, cancer), or diagnosing cancers of the pancreas, but has a high rate of complications (15-25%). The occurrence of post-ERCP pancreatitis is a common and feared complication, as pancreatitis can result in multisystem organ failure and death, and can occur in ~ 16% of ERCP procedures. This study tests whether the use of anti-inflammatory NSAID therapies at the time of ERCP reduce the rate of this complication. The study had 602 participants. The dataset contains 33 variables, but we will focus on a small number: \\(X\\): (primary outcome) - incidence of post-ercp pancreatitis 0 (no), 1 (yes). Treatment arm rx: 0 (placebo), 1 (treatment) Site: 1, 2, 3, 4 Risk: Risk score (1 to 5). Should be factor but treated as continuous. Age: from 19 to 90, mean 45.27, SD 13.30. The correlation between risk and age is -0.216, suggesting no problems of collinearity between those two variables. Note: an obvious one to include would be gender, but I tried it and it is not at all significant, so I have pre-whittled it down for [even more] simplicity. data(&quot;indo_rct&quot;) summary(indo_rct[ ,c(1,2,3,4,6,32)]) ## id site age risk outcome rx ## Min. :1001 1_UM :164 Min. :19.00 Min. :1.000 0_no :523 0_placebo :307 ## 1st Qu.:1152 2_IU :413 1st Qu.:35.00 1st Qu.:1.500 1_yes: 79 1_indomethacin:295 ## Median :2138 3_UK : 22 Median :45.00 Median :2.500 ## Mean :1939 4_Case: 3 Mean :45.27 Mean :2.381 ## 3rd Qu.:2289 3rd Qu.:54.00 3rd Qu.:3.000 ## Max. :4003 Max. :90.00 Max. :5.500 ## Some things to note: # There are very few patients from site 4, and not many from site 3 # The age range goes from 19 to 90 # &#39;rx&#39; is the group variable ## We will try models with age and age^2 glm_indo_agelin = glm(outcome ~ age + site + risk + rx, data=indo_rct, family = binomial(link = &quot;logit&quot;)) glm_indo_agesq = glm(outcome ~ I(age^2) + site + risk + rx, data=indo_rct, family = binomial(link = &quot;logit&quot;)) print(summary(glm_indo_agelin), digits=2) ## ## Call: ## glm(formula = outcome ~ age + site + risk + rx, family = binomial(link = &quot;logit&quot;), ## data = indo_rct) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.7863 0.6414 -2.8 0.005 ** ## age -0.0085 0.0099 -0.9 0.394 ## site2_IU -1.2293 0.2693 -4.6 5e-06 *** ## site3_UK -1.1279 0.7759 -1.5 0.146 ## site4_Case -13.8644 827.9211 0.0 0.987 ## risk 0.5619 0.1423 3.9 8e-05 *** ## rx1_indomethacin -0.7633 0.2615 -2.9 0.004 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 468.01 on 601 degrees of freedom ## Residual deviance: 427.07 on 595 degrees of freedom ## AIC: 441.1 ## ## Number of Fisher Scoring iterations: 14 vif(glm_indo_agelin) ## GVIF Df GVIF^(1/(2*Df)) ## age 1.046789 1 1.023127 ## site 1.089701 3 1.014420 ## risk 1.127145 1 1.061671 ## rx 1.008980 1 1.004480 print(summary(glm_indo_agesq), digits=2) ## ## Call: ## glm(formula = outcome ~ I(age^2) + site + risk + rx, family = binomial(link = &quot;logit&quot;), ## data = indo_rct) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.0e+00 4.9e-01 -4.0 7e-05 *** ## I(age^2) -9.4e-05 1.1e-04 -0.9 0.385 ## site2_IU -1.2e+00 2.7e-01 -4.6 5e-06 *** ## site3_UK -1.1e+00 7.8e-01 -1.5 0.144 ## site4_Case -1.4e+01 8.3e+02 0.0 0.987 ## risk 5.6e-01 1.4e-01 3.9 9e-05 *** ## rx1_indomethacin -7.6e-01 2.6e-01 -2.9 0.004 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 468.01 on 601 degrees of freedom ## Residual deviance: 427.03 on 595 degrees of freedom ## AIC: 441 ## ## Number of Fisher Scoring iterations: 14 vif(glm_indo_agesq) ## GVIF Df GVIF^(1/(2*Df)) ## I(age^2) 1.052813 1 1.026067 ## site 1.089772 3 1.014431 ## risk 1.132658 1 1.064264 ## rx 1.007972 1 1.003978 Since neither age nor age^2 appear influential, we’ll remove it and keep the other covariates. glm_indo = glm(outcome ~ site + risk + rx, data=indo_rct, family = binomial(link = &quot;logit&quot;)) print(summary(glm_indo), digits=2) ## ## Call: ## glm(formula = outcome ~ site + risk + rx, family = binomial(link = &quot;logit&quot;), ## data = indo_rct) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.23 0.38 -5.8 5e-09 *** ## site2_IU -1.22 0.27 -4.5 6e-06 *** ## site3_UK -1.13 0.78 -1.5 0.145 ## site4_Case -13.84 833.24 0.0 0.987 ## risk 0.58 0.14 4.2 3e-05 *** ## rx1_indomethacin -0.75 0.26 -2.9 0.004 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 468.01 on 601 degrees of freedom ## Residual deviance: 427.81 on 596 degrees of freedom ## AIC: 439.8 ## ## Number of Fisher Scoring iterations: 14 vif(glm_indo) ## GVIF Df GVIF^(1/(2*Df)) ## site 1.088963 3 1.014306 ## risk 1.094106 1 1.045995 ## rx 1.005852 1 1.002922 Notice that The estimate and standard error of \\(\\hat{\\tau}\\) (the coefficient of rx) haven’t changed much at all (mainly because, as the VIF shows us, there are no issues with multicollinearity) The standard error for the site 4 cases is enormous - this is because there are so few observations. From the summary we see that \\(\\hat\\tau = -0.752\\), with a standard error of 0.261. A 95% CI for \\(\\tau\\) is therefore \\[-0.752 \\pm 1.96\\times 0.261 = \\left(-1.26,\\;-0.240\\right).\\] This model supports the hypothesis that the log of the odds ratio isn’t zero, and therefore we can reject \\(H_0\\). We can also use the model to estimate the odds of ‘success’ (the outcome 1) for different groups of patients, by fixing the values of the covariates. The linear expression \\(x^T\\hat{\\boldsymbol\\beta}\\) for given values of \\(x\\) gives us as estimate of \\[\\log\\left(\\frac{p(X=1)}{1-p(X=1)}\\right),\\] where \\(X\\) here is the primary outcome. The exponent of this therefore gives the odds, and this can be rearranged to find the probability, \\[p\\left(X_i=1\\right) = \\frac{\\exp(\\text{logit}_i)}{1+\\exp(\\text{logit}_i)}, \\] where \\(\\text{logit}_i\\) is the fitted value of the linear model (on the logit scale) given all the baseline characteristics of some patient \\(i\\). This will be the probability, according to the model, that a patient with this particular combination of baseline characteristics will have outcome 1. Example 6.11 Continuing with Example 6.10, we can find estimates of the log odds (and therefore the odds) of post-ECRP pancreatitis for various categories of patient. For this we will make use of the summary table print(summary(glm_indo), digits=2) ## ## Call: ## glm(formula = outcome ~ site + risk + rx, family = binomial(link = &quot;logit&quot;), ## data = indo_rct) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.23 0.38 -5.8 5e-09 *** ## site2_IU -1.22 0.27 -4.5 6e-06 *** ## site3_UK -1.13 0.78 -1.5 0.145 ## site4_Case -13.84 833.24 0.0 0.987 ## risk 0.58 0.14 4.2 3e-05 *** ## rx1_indomethacin -0.75 0.26 -2.9 0.004 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 468.01 on 601 degrees of freedom ## Residual deviance: 427.81 on 596 degrees of freedom ## AIC: 439.8 ## ## Number of Fisher Scoring iterations: 14 For example, a patient from site 1, with risk level 3, in the control group would have odds \\[\\exp\\left(-2.2307 + 3\\times 0.5846\\right) = 0.6207, \\] which translates to a probability of post-ECRP pancreatitis of \\[\\frac{0.6207}{1+0.6207} = 0.383. \\] By contrast, a patient in group \\(T\\), from site 2, at risk level 1, would have odds \\[\\exp\\left(-2.2307 - 1.2204 + 1\\times 0.5846 - 0.7523\\right) = 0.0268, \\] which is equivalent to a probability of post-ECRP pancreatitis of \\[\\frac{0.0268}{1+0.0268} = 0.0261.\\] Being more methodical we can collect these into a table, and use predict.glm. Since the site 3 and 4 coefficents are not significant (mainly due to a lack of data), we will focus only on the site 1 and 2 participants. ## Create data frame with every combination of factors (only showing sites 1 and 2) df_indo = data.frame( site = rep(c(rep(&quot;2_IU&quot;, 5), rep(&quot;1_UM&quot;, 5)),2), risk = rep(1:5, 4), rx = rep(c(&quot;0_placebo&quot;, &quot;1_indomethacin&quot;), c(10,10)) ) ## Find and transform fitted value df_indo$logodds = predict(glm_indo, newdata=df_indo) df_indo$odds = exp(df_indo$logodds) df_indo$prob = df_indo$odds/(1+df_indo$odds) df_indo_round2 &lt;- df_indo %&gt;% # Using dplyr functions mutate_if(is.numeric, round, digits = 2) df_indo_round2 ## site risk rx logodds odds prob ## 1 2_IU 1 0_placebo -2.87 0.06 0.05 ## 2 2_IU 2 0_placebo -2.28 0.10 0.09 ## 3 2_IU 3 0_placebo -1.70 0.18 0.15 ## 4 2_IU 4 0_placebo -1.11 0.33 0.25 ## 5 2_IU 5 0_placebo -0.53 0.59 0.37 ## 6 1_UM 1 0_placebo -1.65 0.19 0.16 ## 7 1_UM 2 0_placebo -1.06 0.35 0.26 ## 8 1_UM 3 0_placebo -0.48 0.62 0.38 ## 9 1_UM 4 0_placebo 0.11 1.11 0.53 ## 10 1_UM 5 0_placebo 0.69 2.00 0.67 ## 11 2_IU 1 1_indomethacin -3.62 0.03 0.03 ## 12 2_IU 2 1_indomethacin -3.03 0.05 0.05 ## 13 2_IU 3 1_indomethacin -2.45 0.09 0.08 ## 14 2_IU 4 1_indomethacin -1.86 0.15 0.13 ## 15 2_IU 5 1_indomethacin -1.28 0.28 0.22 ## 16 1_UM 1 1_indomethacin -2.40 0.09 0.08 ## 17 1_UM 2 1_indomethacin -1.81 0.16 0.14 ## 18 1_UM 3 1_indomethacin -1.23 0.29 0.23 ## 19 1_UM 4 1_indomethacin -0.64 0.52 0.34 ## 20 1_UM 5 1_indomethacin -0.06 0.94 0.49 6.4 Diagnostics for logistic regression There are many diagnostic techniques for binomial data (see eg. Collett (2003a)) but we will only touch on a small number. Unlike with a linear regression model, we don’t have residuals to analyse, because our model output is fundamentally different from our data: our model outputs are probabilities, but our data is all either 0 or 1. Just because a particular patient had an outcome of 1, we can’t conclude that their probability should have been high. If the ‘true’ probability of \\(X=1\\) for some group of similar (in the baseline covariates sense) patients is 0.9, this means we should expect 1 in 10 of these patients to have \\(X=0\\). This makes assessing logistic regression models somewhat tricky. Diagnostics for logistic regression fall into two categories: discrimination and calibration. We will look at each of these in turn, though by no means exhaustively. 6.4.1 Discrimination Here we are thinking of the logistic regression model as a classifier: for each participant the model outputs some value, on the \\(\\operatorname{logit}\\left(p\\right)\\) scale. If that value is below some threshold, we classify that participant as 0. If the value is above the threshold, we classify them as 1. We are slightly abandoning the notion that the model is predicting probabilities, and instead testing whether the model can successfully order the patients correctly. Can we set some threshold on the model output that (almost) separates the cohort into its ones and zeros? A classic way to asses this is by using Receiver Operating Characteric (ROC) analysis. ROC analysis was developed during the second world war, as radar operators analysed their classification accuracy in distinguishing signal (eg. an enemy plane) from noise. It is still widely used in the field of statistical classification, including in medical diagnostics. ROC analysis can be applied to any binary classifier, not just logistic regression. 6.4.1.1 ROC analysis To understand ROC analysis, we need to revisit two concepts relating to tests or classifiers that you might not have seen since Stats I, and we will introduce (or remind ourselves of) some notation to do this: \\(\\hat{p}_i\\in\\left(0,1\\right)\\) is the fitted value of the logistic regression model for patient \\(i\\) \\(X_i=0\\) or \\(1\\) is the true outcome for patient \\(i\\) \\(t\\in\\left(0,1\\right)\\) is the threshold value. If \\(\\hat{p}_i&lt;t\\) we classify patient \\(i\\) as 0, if \\(\\hat{p}\\geq t\\) we classify them as 1. The language of ROC analysis is so entrenched in diagnostic/screening tests that I have kept it here for consistency. A ‘positive’ result for us is \\(X=1\\), and a ‘negative’ result is \\(X=0\\). Definition 6.1 The sensitivity of a test (or classifier) is the probability that it will output positive (or 1) if the true value is positive (or 1): \\[p\\left(\\hat{p}_i \\geq t \\mid{X_i=1}\\right).\\] Definition 6.2 The specificity of a test (or classifier) is the probability that it will output negative (or 0) if the true value is negative (or 0): \\[p\\left(\\hat{p}_i &lt; t \\mid{X_i=0}\\right) \\] We estimate these by the proportions within the dataset. These are very commonly used for thinking about diagnostic tests and screening tests, and in these contexts a ‘success’ or ‘positive’ is almost always the presence of some condition or disease. In our context, we need to be mindful that a 1 could be good or bad, depending on the trial. The core part of a ROC analysis is to plot sensitivity against 1-specificity for every possible value of the threshold. In a logistic regression context, the lowest the threshold can be is zero. If we set the \\(t=0\\), the model will predict everyone to have an outcome of 1. The sensitivity will be 1 and the specificity will be 0. At the other extreme, if we set \\(t=1\\), we will classify everyone as a 0, and have sensitivity 0 and specificity 1. If we vary the threshold from 0 to 1 the number of people classified in each group will change, and so will the sensitivity and specificity. This forms a ROC curve. The plots below shows the distributions of fitted values for patients with \\(X=0\\) and \\(X=1\\), for models with good and poor separation, and the corresponding ROC curves. The threshold is (arbitrarily) set to 0.5, and so the Sensitivity and Specificity reflec that. The AUC (area under the curve) is an overall summary of the model’s predictive efficacy. If AUC=1, the model is perfect. If AUC is 0.5, the model is no better than random guessing. Generally it is thought that AUC around 0.8 is quite good, and AUC around 0.9 is excellent. Figure 6.5: ROC example with good separation. Figure 6.6: ROC example with poor separation. Note that I’ve used beta distributions for some hypothetical distributions of fitted values for the different groups, but this is just for convenience: ROC analysis makes no distributional assumptions. Example 6.12 Let’s look at the model we fitted in Example 6.10. To draw the ROC curve of this data, we will use the R package pROC. fit_indo = fitted(glm_indo) # Fitted values from glm_indo out_indo = indo_rct$outcome # outcome values (0 or 1) roc_indo_df = data.frame(fit = fit_indo, out = out_indo) The main function in the package pROC is roc, which creates a roc object. and ggroc that sort and plot the data for us: roc_indo = roc(data=roc_indo_df, response = out, predictor=fit) With that object we can do various things, such as plot the ROC curve: ggroc(roc_indo, legacy.axes=T) + geom_abline(slope=1, intercept=0, type=2) Figure 6.7: ROC curve for our logistic regression model of the indo RCT data (solid line). The line y=x shows the ROC curve we’d expect with random guessing. and find the area under the curve for the model auc(roc_indo) ## Area under the curve: 0.7 So we see that our model is better than random guessing, but really not all that good! In particular, wherever we put a threshold (if we use the model that way), many people will be mis-classified. It’s also worth noting that here we’re performing the diagnostics on the data we used to fit the model: if we were to use the model on a new set of patients, the fit would likely be slightly worse. 6.4.2 Calibration In this section we are thinking of the model as actually predicting probabilities, and therefore we want to determine whether these probabilities are, in some sense, ‘correct’ or ‘accurate’. One intuitive way to do this is to work through different ‘types’ of patient (by which we mean different combinations of baseline covariate values) and see whether the proportions of ones in the data broadly match the probability given by the model. If the explanatory variables are factors, and we have repeated observations for the different combinations of factor levels, then for each combination we can estimate the probability of success (or whatever our outcome variable is) using the data, and compare this to the fitted model value. Example 6.13 This example uses the model fitted in Example 6.10. The trial has 602 participants and there are many fewer than 602 combinations of the above factor variables, so for many such combinations we will have estimates. Since we are in three dimensions, plotting the data is moderately problematic. We will have a plot for each site (or for the two main ones), use risk score for the \\(x\\) axis and colour points by treatment group. The circles show the proportions of ones in the data, and are sized by the number of observations used to calculate that estimate, and the crosses and lines show the mean and 95% CI of the fitted value. ## what columns do I want? # site, risk, rx # minus one for mean because outcome is 1 or 2 indo_sum = indo_rct %&gt;% group_by(site, risk, rx) %&gt;% summarise(est = mean(as.numeric(outcome))-1, size=length(id), .groups=&quot;keep&quot;) indo_sum = indo_sum[indo_sum$site %in% c(&quot;1_UM&quot;, &quot;2_IU&quot;),] fit_sum = predict(glm_indo, newdata=indo_sum[,1:3], se.fit=T, type=&quot;response&quot;) indo_sum$fit = fit_sum$fit indo_sum$fit_se = fit_sum$se.fit ggplot(data=indo_sum, aes(x=risk, col=rx)) + geom_point(aes(y=est, size=size), pch=16) + geom_point(aes(y=fit), pch=4) + geom_segment(aes(x=risk, xend=risk, y=fit-1.96*fit_se, yend=fit+1.96*fit_se))+ theme_bw()+ facet_wrap(&quot;site&quot;) + theme(legend.position = &quot;bottom&quot;) Figure 6.8: Calibration-based plots for indo RCT data for sites 1 (left) and 2 (right). These plots are not the easiest to interpret, but there seems to be no evidence of systematic trends away from the model. We will look some more at this in the upcoming practical class, as well as some further principles of model validation. For now, we’re done with Binary data, and in our next few lectures we’ll think about survival, or time-to-event data. References ———. 1998. “Confidence Intervals for the Number Needed to Treat.” Bmj 317 (7168): 1309–12. Collett, David. 2003a. Modelling Binary Data. 2nd ed. Texts in Statistical Science. Chapman &amp; Hall. Elmunzer, B Joseph, James M Scheiman, Glen A Lehman, Amitabh Chak, Patrick Mosler, Peter DR Higgins, Rodney A Hayward, et al. 2012. “A Randomized Trial of Rectal Indomethacin to Prevent Post-ERCP Pancreatitis.” New England Journal of Medicine 366 (15): 1414–22. Marshall, Geoffrey. 1948. “STREPTOMYCIN TREATMENT OF PULMONARY TUBERCULOSIS a MEDICAL RESEARCH COUNCIL INVESTIGATION.” British Medical Journal. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2091872/pdf/brmedj03701-0007.pdf. Newcombe, Robert G. 1998. “Interval Estimation for the Difference Between Independent Proportions: Comparison of Eleven Methods.” Statistics in Medicine 17 (8): 873–90. Wilson, Edwin B. 1927. “Probable Inference, the Law of Succession, and Statistical Inference.” Journal of the American Statistical Association 22 (158): 209–12. "],["working-with-time-to-event-data.html", "7 Working with time-to-event data 7.1 Censored times 7.2 The Survival Curve and the Hazard function", " 7 Working with time-to-event data A data type that is commonly found in clinical trials is time to event data. This type of data captures the amount of time that elapses before a particular event happens. As a sub-field of statistics, survival analysis has been around for a long time, as people have thought about and worked with data like mortality records (most notably John Graunt, who used the ‘Bills of Mortality’ during the 1600s to better understand the plague and other causes of death). However, it developed rapidly during the many cancer related clinical trials of the 1960s and 1970s. In these cases, the event in question was very often death, and which is why this branch of statistics came to be known as survival analysis. However, the event can be many other things, and indeed can be a positive outcome (for example being cured of some condition). Time-to-event data also appears in other applications, such as engineering (eg. monitoring the reliability of a machine) and marketing (eg. thinking of the time-to-purchase). As well as the books already mentioned, this chapter makes use of Collett (2003b). Usually, survival data is given in terms of time, but it can also be the number of times something happens (for example, the number of clinic appointments attended) before the event in question occurs. Survival data is trickier to handle than the data types we have seen so far, for two main reasons. Firstly (and simply) survival data is very often skewed, so even though it is (usually) continuous, we can’t just treat it as normally distributed. Secondly (and more complicatedly, if that’s a word) with time-to-event data we don’t usually observe the full dataset. 7.1 Censored times If a trial monitors a sample of participants for some length of time, many will experience the event before the trial is finished. However, for some of the sample we likely won’t observe the event. This could be because it doesn’t happen within the lifetime of the trial, or it could be because the participant exits the trial prematurely for some reason (eg. withdrawal), or simply stops attending follow-up appointments after a certain times. For these participants, we do know that they had not experienced the event up to some time \\(t\\), but we don’t know what happened next. All we know is that their time-to-event or survival time is greater than that time \\(t\\). These partial observations are known as censored times, and in particular as right-censored times, because the event happens after the censored time. It is possible (but less common) to have left-censored or interval-censored data, but in this course we will deal only with right-censoring. Figure 7.1: An example of some censored data. The dashed line at time 10 indicates the end of the trial period. If we were to treat censored times as observations, ie. as though the event had happened at time \\(t\\), we would bias the results of the trial very seriously. The survival times reported would be systematically shorter than the true ones. For example, in the dataset shown in Figure 7.1, we would estimate the survival probability at time 10 as 0.2, since only two of the 10 participants were still in the trial after time 10. But it may well be that some of the participants whose observations were censored before \\(t=10\\) were still alive at \\(t=10\\). If we were to remove the censored times, and only analyse the data in which the event was observed during the lifespan of the trial, we would be losing data and therefore valuable information. This approach may well also lead to bias, for example if some subset of patients died quite soon into the trial, but the remainder lived a long time (past the end of the trial). If our analysis ignores the survivors, we are likely to underestimate the general survival time. In the dataset in Figure 7.1 there are five participants (3,6,7,8,10) whom we are no longer able to observe at time 10, but of whom none had experienced the event by the point at which they were censored. So we know that we need to somehow include these censored times in our analysis. How we do so will depend on our approach. 7.2 The Survival Curve and the Hazard function The field of survival analysis is relatively unusual in statistics, in that it isn’t treated predominantly parametrically. For most continuous data, it is overwhelmingly common to work with the normal distribution and its friends (eg. the student’s t distribution). Similarly binary data is dominated by the binomial distribution. Inference is therefore often focussed on the parameters \\(\\mu,\\;\\sigma\\) or \\(p\\), as an adequate summary of the truth given whatever parameteric assumption has been made. However, in survival analysis, it is often the case that we focus on the whole shape of the data; there isn’t an accepted dominating probability distribution. In order to be able to deal with time-to-event data, we need to introduce some key ways of working with such data. The survival time (or time-to-event) \\(t\\) for a particular individual can be thought of as the value of a random variable \\(T\\), which can take any non-negative value. We can think in terms of a probability distribution over the range of \\(T\\). If \\(T\\) has a probability distribution with underlying probability density function \\(f\\left(t\\right)\\), then the cumulative distribution function is given by \\[F\\left(t\\right) = P\\left(T&lt;t\\right) = \\int\\limits_0^t f\\left(u\\right)du, \\] and this gives us the probability that the survival time is less than \\(t\\). Definition 7.1 The survival function, \\(\\operatorname{S}\\left(t\\right)\\), is the probability that some individual (in our context a participant) survives longer than time \\(t\\). Therefore \\(\\operatorname{S}\\left(t\\right) = 1 - F(t)\\). Conventionally we plot \\(\\operatorname{S}\\left(t\\right)\\) against \\(t\\) and this gives us a survival curve. We can immediately say two things about survival curves: Since all participants must be alive (or equivalent) at the start of the trial, \\(\\operatorname{S}\\left(0\\right)=1\\). Since it’s impossible to survive past \\(t_2&gt;t_1\\) but not past time \\(t_1\\), we must have \\(\\frac{d\\operatorname{S}\\left(t\\right)}{dt}\\leq{0}\\), ie. \\(\\operatorname{S}\\left(t\\right)\\) is non-increasing. One summary that is often used is the median survival time, the time at which half of the participants have already experienced the event, and half haven’t. Figure 7.2 shows two survival curves, comparing different therapies. We see that the hormonal therapy reduces the survival time slightly compared to no hormonal therapy. Figure 7.2: An example of two survival curves, taken from Syriopoulou et al. (2022). Following on from the survival function, we have another (slightly less intuitive) quantity: the Hazard function \\(h\\left(t\\right)\\). Definition 7.2 The Hazard function \\(h(t)\\) is the probability that an individual who has survived up to time \\(t\\) fails just after time \\(t\\); in other words, the instantaneous probability of death (or experiencing the event) at time \\(t\\). If we use \\(T\\) to denote the random variable of survival time (or time-to-event) then \\(\\operatorname{S}\\left(t\\right)\\) and \\(h(t)\\) are defined by \\[\\begin{align*} \\operatorname{S}\\left(t\\right)&amp;= \\operatorname{Pr}\\left(T&gt;t\\right)\\\\ h\\left(t\\right) &amp; = \\lim\\limits_{s\\rightarrow{0+}}\\frac{\\operatorname{Pr}\\left(t&lt;T&lt;t+s\\mid{T&gt;t}\\right)}{s}. \\end{align*}\\] Using the definition of conditional probability, we can rewrite \\(h(t)\\) as \\[\\begin{align*} h\\left(t\\right) &amp; = \\lim\\limits_{s\\rightarrow{0+}}\\frac{\\operatorname{Pr}\\left(t&lt;T&lt;t+s\\mid{T&gt;t}\\right)}{s} \\\\ &amp; = \\lim\\limits_{s\\rightarrow{0+}}\\left[\\frac{1}{\\operatorname{Pr}\\left(T&gt;t\\right)}\\cdot\\frac{\\operatorname{Pr}\\left(\\left(t&lt;T&lt;t+s\\right)\\cap\\left(T&gt;t\\right)\\right)}{s}\\right] \\\\ &amp; = \\lim\\limits_{s\\rightarrow{0+}}\\left[\\frac{1}{\\operatorname{Pr}\\left(T&gt;t\\right)}\\cdot\\frac{\\operatorname{Pr}\\left(t&lt;T&lt;t+s\\right)}{s}\\right]\\\\ &amp; = \\frac{f\\left(t\\right)}{\\operatorname{S}\\left(t\\right)}, \\end{align*}\\] where \\(f\\left(\\cdot\\right)\\) is the probability density of \\(T\\). The hazard function can take any positive value (unlike the survival function), and for this reason \\(\\log\\left(h\\left(t\\right)\\right)\\) is often used to transform it to the real line. The hazard function can also be called the ‘hazard rate’, the ‘instantaneous death rate’, the ‘intensity rate’ or the ‘force of mortality’. As we hinted before, there are fundamentally two ways to deal with survival data: we can go about things either parametrically or non-parametrically. Unusually for statistics in general, the non-parametric paradigm is prevalent in survival analysis. We will consider some methods from both paradigms. 7.2.1 The Kaplan-Meier estimator The Kaplan-Meier estimator is a non-parametric estimate of \\(\\operatorname{S}\\left(t\\right)\\), originally presented in Kaplan and Meier (1958). The idea behind it is to divide the interval \\(\\left[0,\\;t\\right]\\) into many short consecutive intervals, \\[\\left[0,\\,t\\right] = \\bigcup\\limits_{k=0}^K \\left[s_k,\\,s_{k+1}\\right],\\] where \\(s_k&lt;s_{k+1}\\;\\forall{k}\\), \\(s_0=0\\) and \\(s_{K+1}=t\\). We then estimate the probability of surviving past some time \\(t\\) by multiplying together the probabilities of surviving the successive intervals up to time \\(t\\). No distributional assumptions are made, and the probability of surviving interval \\(\\left[s_k,\\,s_{k+1}\\right]\\) is estimated by \\(1-Q\\), where \\[Q = \\frac{\\text{Number who die in that interval}}{\\text{Number at risk of death in that interval}}.\\] More precisely, let’s say that deaths are observed at times \\(t_1&lt;t_2&lt;\\ldots &lt; t_n\\), and that the number of deaths at time \\(t_i\\) is \\(d_i\\) out of a possible \\(n_i\\). Then for some time \\(t\\in\\left[t_J,\\,t_{J+1}\\right)\\), the Kaplan-Meier estimate of \\(\\operatorname{S}\\left(t\\right)\\) is \\[\\hat{\\operatorname{S}}\\left(t\\right) = \\prod\\limits_{j=0}^J \\frac{\\left(n_j - d_j\\right)}{n_j}.\\] Notice that the number of people at risk at time \\(t_{j+1}\\), denoted \\(n_{j+1}\\), will be the number of people at risk at time \\(t_j\\) (which was \\(n_j\\)), minus any who died at time \\(t_j\\) (which we write as \\(d_j\\)) and any who were censored in the interval \\(\\left[t_j,\\,t_{j+1}\\right)\\). In this way, the Kaplan-Meier estimator incorporates information from individuals with censored survival times up to the point they were censored. Greenwood (1926) derived an approximation to the variance of the Kaplan-Meier estimate of survival curve, given by \\[\\hat{V}\\left(t\\right) = \\left(\\hat{\\operatorname{S}}\\left(t\\right)\\right)^2\\sum\\limits_{t_{i}\\leq{t}} \\frac{d_i}{n_i\\left(n_i-d_i\\right)} \\] This uses the Delta method (which we’ve seen before in the binary outcome chapters) and makes the assumption that events at time \\(t_i\\) are independent binomial draws from a population of size \\(n_i\\). We can use this to form confidence intervals for the survival curve, and indeed ggsurvfit can add these automatically to plots. Example 7.1 Edmonson et al. (1979) conducted a trial on patients with advanced ovarian cancer, comparing cyclophosphamide (group \\(C\\)) with a mixture of cyclophosphamide and adriamycin (group \\(T\\)). Patients were monitored, and their time of death was recorded, or a censoring time if they were alive at their last observation. The data are shown in Table 7.1. Table 7.1: Table 7.2: Ovarian cancer data. FU time gives the survival or censoring time, and FU status the type: 0 for a censored observation, 1 for death. FU_time FU_status 1 59 1 2 115 1 3 156 1 22 268 1 23 329 1 24 353 1 25 365 1 26 377 0 4 421 0 5 431 1 6 448 0 7 464 1 8 475 1 9 477 0 10 563 1 11 638 1 12 744 0 13 769 0 14 770 0 15 803 0 16 855 0 17 1040 0 18 1106 0 19 1129 0 20 1206 0 21 1227 0 We see that there are 26 individuals, and we have the time of death for 12 of them. The remaining 14 observations are censored. We can use this data to calculate the Kaplan-Meier estimator of the survival curve, as shown in Table 7.3. The columns are (from left to right): time \\(t_j\\); number at risk \\(n_j\\); number of events/deaths \\(d_j\\); number of censorings in \\(\\left[t_{j-1},\\,t_j\\right)\\); estimate of survival curve and standard error of the estimate (using Greenwood’s formula). Table 7.3: Table 7.4: Kaplan-Meier estimator calculations for ovarian cancer dataset. time n_risk n_event n_cens survival SE_surv 59 26 1 0 0.9615385 0.0377146 115 25 1 0 0.9230769 0.0522589 156 24 1 0 0.8846154 0.0626563 268 23 1 0 0.8461538 0.0707589 329 22 1 0 0.8076923 0.0772920 353 21 1 0 0.7692308 0.0826286 365 20 1 0 0.7307692 0.0869893 431 17 1 2 0.6877828 0.0918815 464 15 1 1 0.6419306 0.0965213 475 14 1 0 0.5960784 0.0999261 563 12 1 1 0.5464052 0.1032094 638 11 1 0 0.4967320 0.1051027 Figure 7.3 shows the Kaplan-Meier survival curve estimate for the ovarian data. Using the package ggsurvfit we can add in a table below the \\(x\\) axis showing the number at risk and the number of events at some times points. Figure 7.3: Kaplan-Meier estimate of survival curve for ovarian cancer data with a 95% confidence level. The Kaplan-Meier estimate may seem a bit dissatisfying, since it stops changing at \\(t=638\\) with a probability of \\(0.497\\). However, this is really (in a non-parametric setting) all we can say with the data available; 10 of the participants were definitely still alive at \\(t=638\\), and some of the other censored participants may also have been. For a clinical trial, we want to plot the survival curves separately for the different treatment groups. This will give a first, visual, idea of whether there might be a difference, and also of the suitability of certain models (we’ll talk about this later). Example 7.2 Figure 7.4 shows the Kaplan Meier plots for the ovarian cancer data from Figure 7.3, this time split by treatment group. Figure 7.4: Kaplan-Meier curves for the ovarian cancer data, split by treatment group. The second dataset we will use throughout this chapter has been simulated based on a trial of acute myeloid leukemia (Le-Rademacher et al. (2018)) and is from the survival package Therneau (2024). The Kaplan-Meier estimate for the myeloid data is shown in Figure 7.5. Figure 7.5: Kaplan Meier curves for the Myeloid data, split by treatment. 7.2.2 A parametric approach In a parametric approach, we’ll assume that the survival time \\(T\\) follows some probability distribution, up to unknown parameters which we will estimate from the data. The simplest distribution for time-to-event data is the exponential distribution, which has density \\[f\\left(t\\right) = \\lambda e^{-\\lambda t} \\text{ for }t&gt;0,\\] survival function \\[\\operatorname{S}\\left(t\\right) = 1- \\int\\limits_{0}^t{\\lambda e^{-\\lambda t}} = e^{-\\lambda t}, \\] and mean survival time \\(\\frac{1}{\\lambda}\\). The hazard function is therefore \\[h\\left(t\\right) = \\frac{f\\left(t\\right)}{S\\left(t\\right)} = \\lambda, \\] that is, the hazard is constant. Given some dataset, we want to be able to find an estimate for \\(\\lambda\\) (or the parameters of our distribution of choice). 7.2.2.1 Maximum likelihood for time-to-event data Suppose our dataset has \\(n\\) times \\(t_1,\\,t_2,\\,\\ldots,t_n\\). Of these, \\(m\\) are fully observed and \\(n-m\\) are censored. We can create a set of indicators \\(\\delta_1,\\ldots,\\delta_n\\), where \\(\\delta_i=1\\) if observation \\(i\\) is fully observed and \\(\\delta_i=0\\) if it is censored. Usually, the likelihood function is computed by multiplying the density function evaluated at each data point, \\(f\\left(t_i\\mid{\\text{params}}\\right)\\). However, this won’t work for survival data, because for our censored times (those for which \\(\\delta_i=0\\)) we only know that the time-to-event is greater than \\(t_i\\). For these observations, it is the survival function (remember that this is \\(p\\left(T&gt;t\\right)\\)) that contributes what we need to the likelihood function. Therefore (for any probability distribution) we have \\[\\begin{equation} L = \\prod\\limits_{i=1}^n f\\left(t_i\\right)^{\\delta_i} S\\left(t_i\\right)^{1-\\delta_i}. \\tag{7.1} \\end{equation}\\] If we have \\(T\\sim Exp\\left(\\lambda\\right)\\) then the log-likelihood is \\[\\begin{align*} \\ell\\left(\\lambda \\mid \\text{data}\\right) &amp;= \\sum\\limits_{i=1}^n \\delta_i\\left(\\log\\lambda - \\lambda t_i\\right) - \\sum\\limits_{i=1}^n\\left(1-\\delta_i\\right)\\lambda t_i \\\\ &amp; = m\\log\\lambda - \\lambda\\sum\\limits_{i=1}^n t_i. \\end{align*}\\] From this we can find the maximum likelihood estimator (MLE) \\[\\hat{\\lambda} = \\frac{m}{\\sum\\limits_{i=1}^n t_i}.\\] The variance of the MLE is \\[\\begin{equation} \\operatorname{var}\\left(\\hat\\lambda\\right) = \\frac{\\lambda^2}{m}, \\tag{7.2} \\end{equation}\\] which we can approximate by \\[\\operatorname{var}\\left(\\hat\\lambda\\right) \\approx \\frac{m}{\\left(\\sum\\limits_{i=1}^n t_i\\right)^2}.\\] Notice that the numerator in Equation (7.2) is \\(m\\), the number of complete observations (rather than \\(n\\) the total number including censored observations). This shows that there is a limit to the amount we can learn if a lot of the data is censored. Example 7.3 Returning to the dataset from Example 7.1, we can fit an exponential distribution to the data simply by estimating the MLE \\[\\begin{align*} \\hat{\\lambda}_C &amp;= \\frac{m_C}{\\sum\\limits_{i=1}^{n_C} t_i}\\\\ &amp; =\\frac{7}{6725}\\\\ &amp; = 0.00104 \\end{align*}\\] and \\[\\begin{align*} \\hat{\\lambda}_T &amp;= \\frac{m_T}{\\sum\\limits_{i=1}^{n_T} t_i}\\\\ &amp; =\\frac{5}{8863}\\\\ &amp; = 0.00056 \\end{align*}\\] mC_ov = sum((ovarian$fustat==1)&amp;(ovarian$rx==1)) mT_ov = sum((ovarian$fustat==1)&amp;(ovarian$rx==2)) tsum_ov_C = sum(ovarian$futime[ovarian$rx==1]) tsum_ov_T = sum(ovarian$futime[ovarian$rx==2]) m_ov = mT_ov + mC_ov tsum_ov = tsum_ov_C + tsum_ov_T lamhat_ov_C = mC_ov / tsum_ov_C lamhat_ov_T = mT_ov / tsum_ov_T Figure 7.6: Kaplan Meier estimates of survival curves for the ovarian data (solid lines), with the fitted exponential S(t) shown in dashed lines (red = group C, blue = group T). We can do the same for the myeloid data. Figure 7.7 shows the fitted curves, using \\(S\\left(t\\right)=\\exp\\left[-\\hat{\\lambda}_Xt\\right]\\) for group \\(X\\). Figure 7.7: Kaplan Meier estimates of survival curves for the Myeloid data (solid lines), with the fitted exponential S(t) shown in dashed lines (red = group C, blue = group T). 7.2.3 The Weibull distribution Having only one parameter, the exponential distribution is not very flexible, and often doesn’t fit data at all well. A related, but more suitable distribution is the Weibull distribution. Definition 7.3 The probability density function of a Weibull random variable is \\[ f\\left(t\\mid \\lambda,\\,\\gamma\\right) = \\begin{cases} \\lambda\\gamma t^{\\gamma-1}\\exp\\left[{-\\lambda t^{\\gamma}}\\right] &amp; \\text{for }t\\geq{0}\\\\ 0 &amp; \\text{otherwise}. \\end{cases} \\] Here, \\(\\gamma\\) is the shape parameter, and \\(\\lambda\\) is the scale parameter. If \\(\\gamma=1\\) then this reduces to an exponential distribution. You can read more about it, should you choose to, in Collett (2003b). For the Weibull distribution, we have \\[\\operatorname{S}\\left(t\\right) = \\exp\\left(-\\lambda t^{\\gamma}\\right). \\] As with the exponential distribution, we can use Equation (7.1) for the likelihood. For the Weibull distribution this becomes \\[\\begin{align*} L\\left(\\lambda,\\gamma\\mid{\\text{data}}\\right) &amp; = \\prod\\limits_{i=1}^n\\left\\lbrace \\lambda \\gamma t_i^{\\gamma-1}\\exp\\left(-\\lambda t_i^{\\gamma}\\right) \\right\\rbrace^{\\delta_i} \\left\\lbrace \\exp\\left[-\\lambda t_i^{\\gamma}\\right] \\right\\rbrace ^{1-\\delta_i}\\\\ &amp; = \\prod\\limits_{i=1}^n \\left\\lbrace \\lambda\\gamma t_i^{\\gamma-1} \\right\\rbrace^{\\delta_i} \\exp\\left(-\\lambda t_i^{\\gamma}\\right) \\end{align*}\\] and therefore \\[\\begin{align*} \\ell\\left(\\lambda,\\gamma\\mid{\\text{data}}\\right) &amp; = \\sum\\limits_{i=1}^n \\delta_i \\log\\left(\\lambda\\gamma\\right) + \\left(\\gamma-1\\right)\\sum\\limits_{i=1}^n\\delta_i \\log{t_i} - \\lambda \\sum\\limits_{i=1}^n t_i^{\\gamma}\\\\ &amp; = m\\log\\left(\\lambda\\gamma\\right) + \\left(\\gamma-1\\right)\\sum\\limits_{i=1}^n \\delta_i\\log t_i - \\lambda\\sum\\limits_{i=1}^n t_i^{\\gamma}. \\end{align*}\\] For the maximum likelihood estimators, we differentiate (separately) with respect to \\(\\lambda\\) and \\(\\gamma\\) and equate to zero, to solve for the estimators \\(\\hat\\lambda\\) and \\(\\hat\\gamma\\). The equations we end up with are \\[\\begin{align} \\frac{m}{\\hat{\\lambda}} - \\sum\\limits_{i=1}^n t_i^{\\hat\\gamma} &amp; = 0 \\tag{7.3}\\\\ \\frac{m}{\\hat{\\gamma}} + \\sum\\limits_{i=1}^n \\delta_i\\log t_i - \\hat{\\lambda}\\sum\\limits_{i=1}^n t_i^{\\hat\\gamma} \\log t_i &amp;=0 \\tag{7.4}. \\end{align}\\] We can rearrange Equation (7.3) to \\[\\hat{\\lambda} = \\frac{m}{\\sum\\limits_{i=1}^n t_i^{\\hat\\gamma}},\\] and substitute this into Equation (7.4) to find \\[\\frac{m}{\\hat{\\gamma}} + \\sum\\limits_{i=1}^n \\delta_i\\log t_i - \\frac{m}{\\sum\\limits_{i=1}^n t_i^{\\gamma}}\\sum\\limits_{i=1}^n t_i^{\\hat\\gamma} \\log t_i=0. \\] This second equation is analytically intractable, so numerical methods are used to find \\(\\hat\\gamma\\), and then this value can be used to find \\(\\hat\\lambda\\). Example 7.4 We can fit Weibull distributions to our myeloid dataset, as shown in Figure 7.8. \\[S\\left(t\\right) = \\exp\\left(-\\lambda t^{\\gamma}\\right).\\] Figure 7.8: Weibull fit to survival curve of Myeloid data, dashed lines (Kaplan Meier estimate also shown in solid lines). Red for group T, black for group C. We see that there is some improvement compared to the exponential fit in Figure 7.7, but it still seems not to capture the fundamental shape. Aside: Sample size calculations for time-to-event data There are implications here for sample size calculations, which must take into account the duration of a trial; it is important that trials monitor patients until a sufficient proportion have experienced the event (whatever it is). Sample size calculations for time-to-event data therefore have two components: The power of the trial can first be expressed in terms of \\(m\\), the number of complete observations. A separate calculation is needed to estimate the number of participants needing to be recruited, and length of trial, to be sufficiently likely to achieve that value of \\(m\\). Both of these calculations rely on a number of modelling assumptions, and on previous scientific/clinical data (if available). We will think more about how this can be used in the next section, when we come to compare treatment effects. References ———. 2003b. Modelling Survival Data in Medical Research. 2nd ed. Texts in Statistical Science. Chapman &amp; Hall. Edmonson, John H, Thomas R Fleming, David G Decker, George D Malkasian, Edward O Jorgensen, John A Jefferies, Maurice J Webb, and Larry K Kvols. 1979. “Prognosis in Advanced Ovarian Carcinoma Versus Minimal Residual.” Cancer Treatment Reports 63 (2): 241–47. Greenwood, Major. 1926. “The Natural Duration of Cancer.” Reports on Public Health and Medical Subjects 33: 1–26. Kaplan, Edward L, and Paul Meier. 1958. “Nonparametric Estimation from Incomplete Observations.” Journal of the American Statistical Association 53 (282): 457–81. Le-Rademacher, Jennifer G, Ryan A Peterson, Terry M Therneau, Ben L Sanford, Richard M Stone, and Sumithra J Mandrekar. 2018. “Application of Multi-State Models in Cancer Clinical Trials.” Clinical Trials 15 (5): 489–98. Syriopoulou, Elisavet, Tove Wästerlid, Paul C Lambert, and Therese M-L Andersson. 2022. “Standardised Survival Probabilities: A Useful and Informative Tool for Reporting Regression Models for Survival Data.” British Journal of Cancer 127 (10): 1808–15. Therneau, Terry M. 2024. A Package for Survival Analysis in r. https://CRAN.R-project.org/package=survival. "],["comparing-survival-curves.html", "8 Comparing survival curves 8.1 Parametric: likelihood ratio test 8.2 Non-parametric: the log-rank test 8.3 Semi-parametric: the proportional hazards model", " 8 Comparing survival curves Really what we would like to be able to do is to compare two survival curves (showing, for example, the results from different treatments), so that we can say whether one is significantly different from the other. In most cases, this boils down to constructing a hypothesis test along the lines of \\[\\begin{align*} H_0:&amp;\\text{ the treatments are the same}\\\\ H_1:&amp;\\text{ the treatments are different.} \\end{align*}\\] There are various ways to do this, and we will look at some now. 8.1 Parametric: likelihood ratio test For a parametric analysis, our null hypothesis that the two treatments are the same can be reduced to a test of whether the parameter(s) for each group are the same. We can do this using a likelihood ratio test. We’ve already calculated the log-likelihood for the exponential distribution in Section 7.2.2.1, and found the MLE. \\[\\begin{align*} \\ell\\left(\\lambda\\right) &amp; = m\\log\\lambda - \\lambda\\sum\\limits_{i=1}^n t_i\\\\ \\hat{\\lambda} &amp; = \\frac{m}{\\sum\\limits_{i=1}^n t_i}. \\end{align*}\\] Working with the exponential distribution, we can model the survival function as \\[S\\left(t\\right) = \\begin{cases} e^{-\\lambda_Ct}\\; \\text{ for participants in group C}\\\\ e^{-\\lambda_Tt}\\; \\text{ for participants in group T} \\end{cases} \\] and the null hypothesis boils down to \\[H_0: \\; \\lambda_C = \\lambda_T = \\lambda. \\] We can adapt the log-likelihood we found in Section 7.2.2.1 in light of the separate groups, and we find \\[\\begin{equation} \\ell\\left(\\lambda_C,\\,\\lambda_T\\right) = m_C\\log\\lambda_C - \\lambda_C\\sum\\limits_{i=1}^{n_C}t_{iC} + m_T\\log\\lambda_T - \\lambda_T\\sum\\limits_{i=1}^{n_T}t_{iT} \\tag{8.1} \\end{equation}\\] and \\[\\hat{\\lambda}_X = \\frac{m_X}{\\sum\\limits_{i=1}^{n_X}t_{iX}}\\] where \\(X=C\\) or \\(T\\). In these equations \\(m_X\\) is the number of non-censored observations in group \\(X\\), \\(n_X\\) is the total number of participants in group \\(X\\) and \\(t_{iX}\\) is the time for participant \\(i\\) in group \\(X\\). To simplify notation, we will write \\[t^+_X = \\sum\\limits_{i=1}^{n_X}t_{iX},\\] and \\(t^+\\) for the sum over both groups. Substituting the MLEs into Equation (8.1) gives \\[\\ell\\left(\\hat{\\lambda}_C,\\,\\hat{\\lambda}_T\\right) = m_C\\log\\left(\\frac{m_C}{t^+_C}\\right) - m_C + m_T\\log\\left(\\frac{m_T}{t^+_T}\\right) - m_T \\] and \\[\\ell\\left(\\hat\\lambda,\\,\\hat\\lambda\\right) = m\\log\\left(\\frac{m}{t^+}\\right) - m,\\] where \\(n,\\,m\\) are the corresponding totals over both groups. We can therefore perform a maximum likelihood test by finding \\[\\begin{align*} \\lambda_{LR}&amp; = -2\\left[\\ell\\left(\\hat{\\lambda},\\hat{\\lambda}\\right) - \\ell\\left(\\hat{\\lambda}_C,\\hat{\\lambda}_T\\right)\\right] \\\\ &amp; = 2\\left[\\left(m_C\\log\\left(\\frac{m_C}{t^+_C}\\right) - m_C + m_T\\log\\left(\\frac{m_T}{t^+_T}\\right) - m_T \\right) - \\left(m\\log\\left(\\frac{m}{t^+}\\right)\\right)\\right]\\\\ &amp; = 2\\left(m_C\\log\\left(\\frac{m_C}{t^+_C}\\right) + m_T\\log\\left(\\frac{m_T}{t^+_T}\\right) - m\\log\\left(\\frac{m}{t^+}\\right)\\right) \\end{align*}\\] and referring this value to a \\(\\chi^2_1\\) distribution. We can also find a confidence interval for the difference between \\(\\lambda_T\\) and \\(\\lambda_C\\), by using the asymptotic variances of the MLEs, which are \\(\\frac{\\lambda_C^2}{m_C}\\) and \\(\\frac{\\lambda_T^2}{m_T}\\). Therefore, the limits of a \\(100\\left(1-\\alpha\\right)\\)% CI for \\(\\lambda_T - \\lambda_C\\) is given by \\[ \\frac{m_T}{t^+_T} - \\frac{m_C}{t^+_C} \\pm z_{\\alpha/2}\\sqrt{\\frac{m_T}{\\left(t^+_T\\right)^2} + \\frac{m_C}{\\left(t^+_C\\right)^2}}.\\] Example 8.1 In this example we’ll conduct a likelihood ratio test for each of the datasets in Example 7.1. For each dataset, the quantities we need are: \\(m_C,\\,m_T\\): the number of complete observations in each group \\(t^+_C,\\,t^+_T\\) the sum of all observation times (including censored times) in each group Note that \\(m=m_C + m_T\\) and \\(t^+ = t^+_C + t^+_T\\). For the ovarian data we have mC_ov = sum((ovarian$fustat==1)&amp;(ovarian$rx==1)) mT_ov = sum((ovarian$fustat==1)&amp;(ovarian$rx==2)) tsum_ov_C = sum(ovarian$futime[ovarian$rx==1]) tsum_ov_T = sum(ovarian$futime[ovarian$rx==2]) m_ov = mT_ov + mC_ov tsum_ov = tsum_ov_C + tsum_ov_T ## Can now plug these into LR test stat LRstat_ov = 2*(mC_ov*log(mC_ov/tsum_ov_C) + mT_ov*log(mT_ov/tsum_ov_T) - m_ov*log(m_ov/tsum_ov)) LRstat_ov ## [1] 1.114895 We can find the p-value of this test by 1-pchisq(LRstat_ov, df=1) ## [1] 0.2910204 and we find that it isn’t significant. A 95% confidence interval for the difference is given by ## [1] -0.0013927697 0.0004392714 For the Myeloid data we can do the same thing mC_my = sum((myeloid$death==1)&amp;(myeloid$trt==&quot;A&quot;)) mT_my = sum((myeloid$death==1)&amp;(myeloid$trt==&quot;B&quot;)) tsum_my_C = sum(myeloid$futime[myeloid$trt==&quot;A&quot;]) tsum_my_T = sum(myeloid$futime[myeloid$trt == &quot;B&quot;]) m_my = mT_my + mC_my tsum_my = tsum_my_C + tsum_my_T ## Can now plug these into LR test stat LRstat_my = 2*(mC_my*log(mC_my/tsum_my_C) + mT_my*log(mT_my/tsum_my_T) - m_my*log(m_my/tsum_my)) LRstat_my ## [1] 11.95293 Again, we refer this to \\(\\chi^2_1\\): 1-pchisq(LRstat_my, df=1) ## [1] 0.0005456153 This time we find that the difference is significant at even a very low level, and the 95% CI is given by ## [1] -3.028814e-04 -8.108237e-05 Although the confidence around \\(\\hat\\lambda_X\\) is high (ie. small standard error of the estimate), because of the large amount of data, the fit appears to actually be rather poor (recall Figure 7.7), mainly because of the inflexibility of the exponential distribution. One feature of the exponential model that is convenient is that the hazard function is constant. Comparisons between treatment groups in survival trials are often summarised by the hazard ratio: the ratio of the hazard functions for the two groups. In general this is a function of \\(t\\), but for two exponential hazard functions it is simply the ratio of the \\(\\lambda\\) values. We could also perform LR tests with the fitted Weibull distributions, but instead we will continue on through some more commonly used methods. 8.2 Non-parametric: the log-rank test The log-rank test is performed by creating a series of tables, and combining the information to find a test statistic. We work through each time \\(t_j\\) at which an event is observed (by which we mean a death or equivalent, not a censoring) in either of the groups. For notation, we will say that at time \\(t_j\\), \\(n_j\\) patients are ‘at risk’ of the event \\(d_j\\) events are observed (often the ‘event’ is death, so we will sometimes say this) For groups \\(C\\) and \\(T\\) we would therefore have a table representing the state of things at time \\(t_j\\), with this general form: Group No. surviving No. events No. at risk Treatment \\(n_{Tj}-d_{Tj}\\) \\(d_{Cj}\\) \\(n_{Cj}\\) Control \\(n_{Cj}-d_{Cj}\\) \\(d_{Tj}\\) \\(n_{Tj}\\) Total \\(n_j-d_j\\) \\(d_j\\) \\(n_j\\) Under \\(H_0\\), we expect the deaths (or events) to be distributed proportionally between groups \\(C\\) and \\(T\\), and so the expected number of events in group \\(X\\) (\\(C\\) or \\(T\\)) at time \\(t_j\\) is \\[e_{Xj} = n_{Xj}\\times{\\frac{d_j}{n_j}}.\\] This means that \\(e_{Cj}+e_{Tj} = d_{Cj} + d_{Tj} = d_j\\). If we take the margins of the table (by which we mean \\(n_j, \\,d_j,\\,n_{Cj}\\) and \\(n_{Tj}\\)) as fixed, then \\(d_{Cj}\\) has a hypergeometric distribution. Definition 8.1 The hypergeometric distribution is a discrete probability distribution describing the probability of \\(k\\) successes in \\(n\\) draws (without replacement), taken from a finite population of size \\(N\\) that has exactly \\(K\\) objects with the desired feature. The probability mass function for a variable \\(X\\) following a hypergeometric function is \\[p\\left(X=k\\mid{K,N,n}\\right) = \\frac{{\\binom{K}{k}}{\\binom{N-K}{n-k}}}{{\\binom{N}{n}}}. \\] An example would be an urn containing 50 (\\(N\\)) balls, of which 16 (\\(K\\)) are green and the rest (34, \\(N-K\\)) are red. If we draw 10 (\\(n\\)) balls without replacement, \\(X\\) is the random variable whose outcome is \\(k\\), the number of green balls drawn. In the notation of the definition, the mean is \\[\\operatorname{E}\\left(X\\right)= n\\frac{K}{N} \\] and the variance is \\[\\operatorname{var}\\left(X\\right) = n\\frac{K}{N}\\frac{N-K}{N}\\frac{N-n}{N-1}.\\] In the notation of our table at time \\(t_j\\), we have \\[\\begin{align*} \\operatorname{E}\\left(d_{Cj}\\right) = e_{Cj} &amp; = n_{Cj}\\times{\\frac{d_j}{n_j}}\\\\ \\operatorname{var}\\left(d_{Cj}\\right) = v_{Cj} &amp;= \\frac{d_j n_{Cj}n_{Tj} \\left(n_j-d_j\\right)}{n_j^2\\left(n_j-1\\right)} \\end{align*}\\] With the marginal totals fixed, the value of \\(d_{Cj}\\) fixes the other three elements of the table, so considering this one variable is enough. Under \\(H_0\\), the numbers dying at successive times are independent, so \\[U = \\sum\\limits_{j}\\left(d_{Cj}-e_{Cj}\\right) \\] will (asymptotically) have a normal distribution, with \\[U \\sim {N}\\left(0,\\;\\sum\\limits_j v_{Cj}\\right). \\] We label \\(V = \\sum\\limits_jv_{Cj}\\), and in the log-rank test we refer \\(\\frac{U^2}{V}\\) to \\(\\chi^2_1\\). A somewhat simpler, and more commonly used, version of the log-rank test uses the fact that under \\(H_0\\), the expected number of events (eg. deaths) in group \\(X\\) is \\(E_X = \\sum\\limits_je_{Xj}\\), and the observed number is \\(O_X = \\sum\\limits_j d_{Xj}\\). The standard \\(\\chi^2\\) test formula can then be applied, and the test-statistic is \\[\\frac{\\left(O_C - E_C\\right)^2}{E_C} + \\frac{\\left(O_T - E_T\\right)^2}{E_T}.\\] It turns out that this test statistic is always smaller than \\(\\frac{U^2}{V}\\), so this test is slightly more conservative (ie. it has a larger p-value). Notice that for both of these test statistics, the actual difference between observed and expected is used, not the absolute difference. Therefore if the differences change in sign over time, the values are likely to cancel out (at least to some extent) and the log-rank test is not appropriate. Example 8.2 Let’s now perform a log-rank test on our data from Example 8.1. First, the ovarian cancer dataset. To do this, we can tabulate the key values at each time step. ## Time n_Cj d_Cj e_Cj n_Tj d_Tj e_Tj n_j d_j ## 1 59 13 1 0.5000000 13 0 0.5000000 26 1 ## 2 115 12 1 0.4800000 13 0 0.5200000 25 1 ## 3 156 11 1 0.4583333 13 0 0.5416667 24 1 ## 4 268 10 1 0.4347826 13 0 0.5652174 23 1 ## 5 329 9 1 0.4090909 13 0 0.5909091 22 1 ## 6 353 8 0 0.3809524 13 1 0.6190476 21 1 ## 7 365 8 0 0.4000000 12 1 0.6000000 20 1 ## 8 431 8 1 0.4705882 9 0 0.5294118 17 1 ## 9 464 6 0 0.4000000 9 1 0.6000000 15 1 ## 10 475 6 0 0.4285714 8 1 0.5714286 14 1 ## 11 563 5 0 0.4166667 7 1 0.5833333 12 1 ## 12 638 5 1 0.4545455 6 0 0.5454545 11 1 From this, we can find the \\(v_{j}\\) and the test statistic \\(\\frac{U^2}{V}\\): # Add up the differences UC = sum(logrank_df$d_Cj - logrank_df$e_Cj) vCj_vec = sapply( 1:n_event, function(j){ nCj = logrank_df$n_Cj[j] nTj = logrank_df$n_Tj[j] dj = logrank_df$d_j[j] nj = logrank_df$n_j[j] (nCj*nTj*dj*(nj-1))/((nj^2)*(nj-1)) }) VC = sum(vCj_vec) cs_ov_stat = (UC^2)/VC 1-pchisq(cs_ov_stat, df=1) ## [1] 0.3025911 For the simpler, more conservative, version of the log-rank test, we have EC = sum(logrank_df$e_Cj) ET = sum(logrank_df$e_Tj) OC = sum(logrank_df$d_Cj) OT = sum(logrank_df$d_Tj) test_stat = ((EC-OC)^2)/EC + ((ET-OT)^2)/ET test_stat ## [1] 1.057393 and we can find the p-value by 1-pchisq(test_stat, df=1) ## [1] 0.3038106 As we expected, slightly larger, but not much different from the first version. These values are also pretty close to the results of our LR test in Example 8.1, where we had \\(p=0.291\\). Since the Myeloid dataset is much bigger, we won’t go through the rigmarole of making the table, but will instead use an inbuilt R function from the survival package (more on this in practicals). myeloid$trt = as.factor(myeloid$trt) survdiff(Surv(futime, death) ~ trt, data = myeloid, rho=0) ## Call: ## survdiff(formula = Surv(futime, death) ~ trt, data = myeloid, ## rho = 0) ## ## N Observed Expected (O-E)^2/E (O-E)^2/V ## trt=A 317 171 143 5.28 9.59 ## trt=B 329 149 177 4.29 9.59 ## ## Chisq= 9.6 on 1 degrees of freedom, p= 0.002 This time the p-value is quite far from the one we found using the likelihood ratio test (p=0.00055), further supporting the view that the likelihood ratio test was not appropriate because of the poor fit of the exponential distribution. 8.3 Semi-parametric: the proportional hazards model As with continuous and binary outcome variables, what we would really like to be able to do is to adjust our model for baseline covariates. It seems intuitively reasonable to suppose that factors like age, sex, disease status etc. might affect someone’s chances of survival (or whatever event we’re concerned with). The conventional way to do this is using a proportional hazards model, where we assume that \\[h_T\\left(t\\right) = \\psi h_C\\left(t\\right) \\] for any \\(t&gt;0\\) and for some constant \\(\\psi&gt;0\\). We call \\(\\psi\\) the relative hazard or hazard ratio. If \\(\\psi&lt;1\\) then the hazard at time \\(t\\) under treatment \\(T\\) is smaller than under control \\(C\\). If \\(\\psi&gt;1\\) then the hazard at time \\(t\\) is greater in greater in group \\(T\\) than in group \\(C\\). The important point is that \\(\\psi\\) doesn’t depend on \\(t\\). The hazard for a particular patient might be greater than for another, due to things like their age, disease history, treatment group and so on, but the extent of this difference doesn’t change over time. We can adopt the concept of a baseline hazard function \\(h_0\\left(t\\right)\\), where for someone in group \\(C\\) (for now), their hazard at time \\(t\\) is \\(h_0\\left(t\\right)\\), and for someone in group \\(T\\) it is \\(\\psi h_0\\left(t\\right)\\). Since we must have \\(\\psi&gt;0\\), it makes sense to set \\[\\psi = e^{\\beta},\\] so that \\(\\beta = \\log\\psi\\) and \\(\\psi&gt;0\\;\\forall\\beta\\in\\mathbb{R}\\). Note that \\(\\beta&gt;0 \\iff \\psi&gt;1\\). We can now (re)-introduce our usual indicator variable \\(G_i\\), where \\[ G_i = \\begin{cases} 0\\text{ if participant }i\\text{ is in group }C\\\\ 1\\text{ if participant }i\\text{ is in group }T \\end{cases} \\] and model the hazard function for participant \\(i\\) as \\[h_i\\left(t\\right) = \\exp\\left[\\tau G_i\\right]h_0\\left(t\\right).\\] This is the proportional hazards model for the comparison of two groups. Now, the relative hazard is a function of the participant’s characteristics. Naturally, we can extend it to include other baseline covariates, as we have with linear models in ANCOVA, and with logistic regression. 8.3.1 General proportional hazards model Extending the model to include baseline covariates \\(X_1,\\ldots,X_p\\), we have \\[\\psi\\left(\\mathbf{x}_i\\right) = \\exp\\left(\\tau G_i + \\beta_1x_{1i} + \\ldots + \\beta_p x_{pi}\\right) = \\exp\\left(\\mathbf{x}_i^T \\boldsymbol\\beta\\right),\\] where we collect \\(\\tau\\) into \\(\\boldsymbol\\beta\\) and \\(G\\) into \\(\\mathbf{x}\\), and the hazard function for participant \\(i\\) is \\[h_i\\left(t\\right) = \\psi\\left(\\mathbf{x}_i\\right)h_0\\left(t\\right). \\] Now, our baseline hazard function \\(h_0\\left(t\\right)\\) is the hazard function for a participant in group \\(C\\) for whom all baseline coviariates are either zero (if continuous) or the reference level (if a factor variable). For factor covariates this makes sense, since all levels are realistic values, but for continuous variables zero is likely to be unrealistic (for example you’d never expect zero for age, weight, height, blood pressure etc.). So, if any continuous variables are present, the baseline will always need to be adjusted, but if all covariates are factors, it is likely that the baseline hazard function will be applicable for some set of participants. The linear component \\(\\mathbf{x}_i^T\\boldsymbol\\beta\\) is often called the risk score or prognostic index for participant \\(i\\). The general form of the model is therefore \\[\\begin{equation} h_i\\left(t\\right) = \\exp\\left[\\mathbf{x}_i^T\\boldsymbol\\beta\\right]h_0\\left(t\\right), \\tag{8.2} \\end{equation}\\] and we can rewrite it as \\[\\log\\left(\\frac{h_1\\left(t\\right)}{h_0\\left(t\\right)}\\right) = \\mathbf{x}_i^T\\boldsymbol\\beta.\\] Notice that there is no constant in the linear term - if there was, it could just be absorbed into the baseline hazard function. There are ways of fitting this model that rely on specifying the hazard function using parametric methods, but the method we will study (and the most widely used) is one developed by Cox (1972). 8.3.1.1 Interpreting the parameters in a proportional hazards model Since our primary interest is in comparing the effect of some new treatment with that of the control, it is important that we understand what the coefficients mean, and in particular how they relate to the treatment effect. Let’s do that (as usual) by considering two participants who are identical in all baseline covariates, one in group C and one in group T. We have \\[\\begin{align*} h^C_i\\left(t\\right) &amp; = \\exp\\left(\\beta_1x_{1i} + \\ldots + \\beta_p x_{pi}\\right)h_0\\left(t\\right) &amp; \\text{ in group C}\\\\ h^T_i\\left(t\\right) &amp; = \\exp\\left(\\tau + \\beta_1x_{1i} + \\ldots + \\beta_p x_{pi}\\right)h_0\\left(t\\right) &amp; \\text{ in group T.} \\end{align*}\\] From this we can find the hazard ratio at time \\(t\\) for the two treatments to be \\[\\frac{h^T_i\\left(t\\right)}{h^C_i\\left(t\\right)} = \\exp{\\left(\\tau\\right)} \\] and \\(\\tau\\) is the log of the hazard ratio for the two treatments, adjusting for the other covariates. A value of \\(\\tau=0\\) implies a hazard ratio of one, and of no evidence of difference between the treatments. 8.3.2 Cox regression The beauty of Cox regression is that it avoids specifying a form for \\(h_0\\left(t\\right)\\) altogether. To fit the model in Equation (8.2) we must estimate the coefficients \\(\\boldsymbol\\beta = \\left(\\tau,\\,\\beta_1,\\ldots,\\beta_p\\right)^T\\). It also appears as though we should estimate the baseline hazard \\(h_0\\left(t\\right)\\) somehow too, but the great advance made by Cox was to develop a method where this isn’t necessary. We don’t need to estimate \\(h_0\\left(t\\right)\\) to make inferences about the hazard ratio \\[\\frac{h_i\\left(t\\right)}{h_0\\left(t\\right)}.\\] We will estimate the coefficients \\(\\boldsymbol\\beta\\) using maximum likelihood, and so we’ll need to specify a likelihood function for the \\(\\boldsymbol\\beta\\), which will be a function of \\(\\mathbf{x}^T\\boldsymbol\\beta\\) and our observed data, the survival times \\(t_i\\). Suppose we have data for \\(n\\) participants, and that these include \\(m\\) complete observations (often referred to as deaths) and \\(n-m\\) right-censored survival times. Suppose also that all the complete observation times are distinct. Since time itself is continuous, this is always technically true, but in data the time will be rounded and so there may be multiple observations at one time. We can order these \\(m\\) event times \\[t_{(1)}&lt; t_{(2)} &lt; \\ldots &lt; t_{(m)},\\] such that \\(t_{(j)}\\) is the time of the \\(j^{\\text{th}}\\) event to be observed. At time \\(t_{(j)}\\), there will be some number of individuals who are ‘at risk’ of the event, because either their observation time or their censored survival time is greater than \\(t_{(j)}\\). The set of these individuals is the risk set, denoted \\(R\\left(t_{(j)}\\right)\\). Cox (1972) shows that the relevant likelihood function for the proportional hazards model in Equation (8.2) is \\[\\begin{equation} L\\left(\\boldsymbol\\beta\\right) = \\prod\\limits_{j=1}^m\\frac{\\exp\\left[\\mathbf{x}_{(j)}^T\\boldsymbol\\beta\\right]}{\\sum\\limits_{l\\in R\\left(t_{(j)}\\right)}{\\exp\\left[\\mathbf{x}_l^T\\boldsymbol\\beta\\right]}} \\tag{8.3} \\end{equation}\\] where \\(\\mathbf{x}_{(j)}\\) is the vector of covariates for the individual who dies (or equivalent) at time \\(t_{(j)}\\). Notice that the product is over only those individuals with complete observations, but individuals with censored data do contribute to the sum in the denominator. The numerator of the fraction inside the product in Equation (8.3) is the relative hazard for the person who actually did die at time \\(t_{(j)}\\). The denominator is the sum of the relative hazards for all those who possibly could have died at time \\(t_{(j)}\\) (the risk set \\(R\\left(t_{(j)}\\right)\\)). Thus, in very loose terms, maximizing the likelihood means finding values for \\(\\boldsymbol\\beta\\) that mean the people who did die were ‘the most likely’ to die at the time they did. Notice that this is not a true likelihood, since it depends only on the ordering of the data (the observation and censoring times) and not the data itself. This makes it a partial likelihood. The argument given to justify this is that because the baseline hazard \\(h_0\\left(t\\right)\\) has an arbitrary form, it’s possible that except for at these observed times, \\(h_0\\left(t\\right)=0\\), and therefore \\(h_i\\left(t\\right)=0\\). This means the intervals between successive observations convey no information about the effect of the covariates on hazard, and therefore about the \\(\\boldsymbol\\beta\\) parameters. If you want to know more detail about how this likelihood was derived, you can find in in Section 3.3 of Collett (2003b), or in Cox’s original paper (Cox (1972)). Moving on, if we set \\[ \\delta_i = \\begin{cases} 0\\;\\;\\text{ if individual }i\\text{ is censored}\\\\ 1\\;\\;\\text{ if individual }i\\text{ is observed} \\end{cases} \\] then we can write Equation (8.3) as \\[L\\left(\\boldsymbol\\beta\\mid{\\text{data}}\\right) = \\prod\\limits_{i=1}^n\\left(\\frac{\\exp\\left[\\mathbf{x}_i^T\\boldsymbol\\beta\\right]}{\\sum\\limits_{l\\in R\\left(t_i\\right)}{\\exp\\left[\\mathbf{x}_l^T\\boldsymbol\\beta\\right]}}\\right)^{\\delta_i},\\] where \\(R\\left(t_i\\right)\\) is the risk set at time \\(t_i\\). From this we can find the log-likelihood \\[\\ell\\left(\\boldsymbol\\beta\\mid{\\text{data}}\\right) = \\sum\\limits_{i=1}^n \\delta_i\\left[\\mathbf{x}_i^T\\boldsymbol\\beta - \\log\\sum\\limits_{l\\in R\\left(t_i\\right)}\\exp\\left(\\mathbf{x}_l^T\\boldsymbol\\beta\\right)\\right].\\] The MLE \\(\\hat{\\boldsymbol\\beta}\\) is found using numerical methods (often Newton-Raphson, which you’ll have seen if you did Numerical Analysis II). How can we tell if a proportional hazards model is appropriate? We can’t easily visualise the hazard function for a dataset, and instead would plot the survival curve. So can we tell if the proportional hazards assumption is met by looking at the survival curve? It turns out that if two hazard functions are proportional, their survival functions won’t cross one another, as we will show now. Suppose \\(h_C\\left(t\\right)\\) is the hazard at time \\(t\\) for an individual in group \\(C\\), and \\(h_T\\left(t\\right)\\) is the hazard for that same individual in group \\(T\\). If the two hazards are proportional then we have \\[h_C\\left(t\\right) = \\psi h_T\\left(t\\right) \\] for some constant \\(\\psi\\). Recall from Section 7.2 that \\[h\\left(t\\right) = \\frac{f\\left(t\\right)}{S\\left(t\\right)},\\] where \\(S\\left(t\\right)\\) is the survival function and \\(f\\left(t\\right)\\) is the probability density of \\(T\\). We can therefore write \\[h\\left(t\\right) = -\\frac{d}{dt}\\left[\\log\\left(S\\left(t\\right)\\right)\\right]\\] and rearrange this to \\[\\begin{equation} S\\left(t\\right) = \\exp \\left(-H\\left(t\\right)\\right) \\tag{8.4} \\end{equation}\\] where \\[H\\left(t\\right) = \\int\\limits_0^t h\\left(u\\right) du.\\] Therefore for our two hazard functions, we have \\[\\exp\\left\\lbrace - \\int\\limits_0^t h_C\\left(u\\right)du \\right\\rbrace =\\exp\\left\\lbrace -\\int\\limits_0^t\\psi h_T\\left(u\\right) du \\right\\rbrace \\] From Equation (8.4) we see that therefore \\[S_C\\left(t\\right) = \\left[S_T\\left(t\\right)\\right]^\\psi.\\] Since the survival function is always between 0 and 1, we can see that the value of \\(\\psi\\) determines whether \\(S_C\\left(t\\right)&lt;S_T\\left(t\\right)\\) (if \\(\\psi&gt;1\\)) or \\(S_C\\left(t\\right)&gt;S_T\\left(t\\right)\\) (if \\(0&lt;\\psi&lt;1\\)). The important thing is that the survival curves will not cross. This is an informal conclusion, and lines not crossing is a necessary condition but not a sufficient one. It may also be that the survival curves cross when a particular [influential] covariate is factored out, but not when it isn’t. Example 8.3 First of all, we can use Cox regression adjusted only for the Group (or treatment arm) of the participants. For the ovarian dataset coxph(formula = Surv(futime, fustat)~rx, data=ovarian) ## Call: ## coxph(formula = Surv(futime, fustat) ~ rx, data = ovarian) ## ## coef exp(coef) se(coef) z p ## rx -0.5964 0.5508 0.5870 -1.016 0.31 ## ## Likelihood ratio test=1.05 on 1 df, p=0.3052 ## n= 26, number of events= 12 and for the myeloid1 dataset coxph(formula = Surv(futime, death)~trt, data=myeloid) ## Call: ## coxph(formula = Surv(futime, death) ~ trt, data = myeloid) ## ## coef exp(coef) se(coef) z p ## trtB -0.3457 0.7077 0.1122 -3.081 0.00206 ## ## Likelihood ratio test=9.52 on 1 df, p=0.002029 ## n= 646, number of events= 320 We see that for both results, our p-values are close to what we have found with the log rank test. For the ovarian dataset there is no evidence of a significant difference (likely due to the small sample size). For the myeloid data we find that there is evidence of a difference - we can use the coefficient estimate and standard error to construct a 95% confidence interval for the log hazard ratio of \\[ -0.346 \\pm 1.96 \\times 0.112 = \\left(-0.566,\\, -0.126\\right) \\] and therefore for the hazard ratio itself of \\[\\left(0.568,\\, 0.881\\right). \\] We see that there is strong evidence that the intervention reduces the hazard. We can also account for more baseline covariates. For the ovarian data we can include age and resid.ds (whether residual disease is present): coxph(formula = Surv(futime, fustat)~rx+age+resid.ds, data=ovarian) ## Call: ## coxph(formula = Surv(futime, fustat) ~ rx + age + resid.ds, data = ovarian) ## ## coef exp(coef) se(coef) z p ## rx -0.8489 0.4279 0.6392 -1.328 0.18416 ## age 0.1285 1.1372 0.0473 2.718 0.00657 ## resid.ds 0.6964 2.0065 0.7585 0.918 0.35858 ## ## Likelihood ratio test=16.77 on 3 df, p=0.0007889 ## n= 26, number of events= 12 What this shows is that the most significant factor by far is the particpant’s age, with the hazard function increasing as age increases. The coefficient for treatment group (rx) has increased in magnitude and the p-value has decreased now that age is being adjusted for (although it is still not significant). We can do the same for the myeloid data: coxph(formula = Surv(futime, death)~trt+sex, data=myeloid) ## Call: ## coxph(formula = Surv(futime, death) ~ trt + sex, data = myeloid) ## ## coef exp(coef) se(coef) z p ## trtB -0.3582 0.6989 0.1129 -3.174 0.00151 ## sexm 0.1150 1.1219 0.1128 1.020 0.30782 ## ## Likelihood ratio test=10.56 on 2 df, p=0.005093 ## n= 646, number of events= 320 We see that the only covariate we have, sex has very little effect, and that our confidence interval for the treatment effect will not have changed much at all. 8.3.3 Diagnostics for Cox regression Having fit a Cox proportional hazards model, it’s important to check that it is an appropriate fit to the data. We’ve seen already that the survival curves mustn’t cross, but there are other more sophisticated methods we can use to assess the model. It is important to examine the proportional hazards assumption for every covariate we include in the model (including the group / arm variable), and how we do this depends on whether the covariate is continuous or categorical. 8.3.3.1 Continuous variables Schoenfeld (1982) derived partial residuals, known as Schoenfeld residuals, that can be used to assess whether the proportional hazards assumption is appropriate for a continuous variable. We can think of \\(X_i = \\left(X_{i1},\\ldots,X_{ip}\\right)&#39;\\), the set of covariates for a participant who experiences the event at time \\(t_i\\), as a random variable. Schoenfeld (1982) showed that \\[E\\left(X_{ij}\\mid{R_i}\\right) = \\frac{\\sum\\limits_{k\\in{R_i}}X_{kj}\\exp\\left(\\beta&#39;X_k\\right)}{\\sum\\limits_{k\\in{R_i}}\\exp\\left(\\beta&#39;X_k\\right)}, \\] where \\(R_i\\) are the indices of those at risk at time \\(t_i\\). You can think of this as the average of the \\(X_{\\cdot j}\\) values of those at risk as time \\(t_i\\), weighted by their relative hazard. We can write \\[ \\hat{E}\\left(X_{ij}\\mid R_i\\right)\\] to denote this quantity with the MLE \\(\\hat\\beta\\) substituted for \\(\\beta\\). The partial residual at time \\(t_i\\) is therefore the vector \\[\\hat{r} = \\left(\\hat{r}_{i1},\\ldots,\\hat{r}_{ip}\\right), \\] where \\[\\hat{r}_{ik} = X_{ik} - \\hat{E}\\left(X_{ik}\\mid{R_i}\\right). \\] If we plot the Schoenfeld residuals against time, we should see a random scatter around zero (the kind of plot we look for when assessing residuals against fitted values of a linear regression model). Grambsch and Therneau (1994) proposed a statistical test using the Schoenfeld residuals, in which the null hypothesis is that the proportional hazards assumption holds. This can be implemented by the function cox.zph in the survival package. Example 8.4 The ovarian data contains the continuous variable age, and so we can test the assumption of proportional hazards in relation to age using Schoenfeld residuals. ## chisq df p ## rx 0.631 1 0.43 ## age 0.210 1 0.65 ## resid.ds 1.120 1 0.29 ## GLOBAL 1.958 3 0.58 We see from the age line that the data are consistent with the proportional hazards assumption. The object created by cox.zph also contains the Schoenfeld residuals themselves, and so we can plot them: Because there is so little data it’s hard to conclude anything, and indeed our lack of significance in the test may be due to small sample size rather than excellent model fit. 8.3.3.2 Categorical variables Recall from Equation (8.4) that \\[S\\left(t\\right) = \\exp\\left(-H\\left(t\\right)\\right), \\] where \\(H\\left(t\\right)\\) is the cumulative hazard function \\[ H\\left(t\\right) = \\int\\limits_{0}^t h\\left(u\\right) du. \\] From this we find that \\[ \\log \\left(S\\left(t\\right)\\right) = -H\\left(t\\right).\\] If we have two groups \\(A\\) and \\(B\\) for which the proportional hazards assumption is satisfied, then for some constant \\(\\psi\\) \\[H_A\\left(t\\right) = \\psi H_B\\left(t\\right). \\] We can combine these two equations to find \\[\\begin{align*} \\log\\left[-\\log\\left(S_A\\left(t\\right)\\right)\\right] = \\log\\left(H_A\\left(t\\right)\\right) &amp; = \\log\\psi + \\log\\left(H_B\\left(t\\right)\\right)\\\\ &amp; = \\log\\psi + \\log\\left[-\\log\\left(S_B\\left(t\\right)\\right)\\right]. \\end{align*}\\] Under the Cox Regression model, \\(\\log\\left(H\\left(t\\right)\\right)\\) is linear in the covariates, and so we will have [roughly] parallel lines. We can plot this in R using ggsurvplot and setting fun=\"cloglog\". Example 8.5 First we’ll check the ovarian dataset, split by treatment group (Figure 8.1). Figure 8.1: Log-log plot for ovarian data by treatment group. As before, there isn’t really enough data to tell whether the assumption is violated. We can do the same for the myeloid data, as in Figures 8.2 and 8.3. Figure 8.2: Log-log plot for myeloid data by treatment group. Figure 8.3: Log-log plot for myeloid data by treatment group. To explore these diagnostic checks further we will introduce the veteran dataset, which focuses on a trial for lung cancer patients. The treatment variable is trt and there are a mixture of continuous and categorical covariates. We will include celltype, a categorical variable with four levels and karno, a score from 0 to 100 (which we will treat as continuous). Firstly we can check the proportional hazards assumption for the categorical covariates, in Figure 8.4. sf_vet = survfit(Surv(time, status) ~ trt+celltype, data=veteran) ggsurvplot(sf_vet, fun = &quot;cloglog&quot;) Figure 8.4: Log-log plot for veteran data split by treatment group and cell type Next we can fit a Cox proportional hazards model cox_vet = coxph(Surv(time, status) ~ trt + celltype + karno, data = veteran) summary(cox_vet) ## Call: ## coxph(formula = Surv(time, status) ~ trt + celltype + karno, ## data = veteran) ## ## n= 137, number of events= 128 ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## trt 0.261744 1.299194 0.200923 1.303 0.19267 ## celltypesmallcell 0.824980 2.281836 0.268911 3.068 0.00216 ** ## celltypeadeno 1.153994 3.170833 0.295038 3.911 9.18e-05 *** ## celltypelarge 0.394625 1.483828 0.282243 1.398 0.16206 ## karno -0.031271 0.969213 0.005165 -6.054 1.41e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## trt 1.2992 0.7697 0.8763 1.9262 ## celltypesmallcell 2.2818 0.4382 1.3471 3.8653 ## celltypeadeno 3.1708 0.3154 1.7784 5.6534 ## celltypelarge 1.4838 0.6739 0.8534 2.5801 ## karno 0.9692 1.0318 0.9595 0.9791 ## ## Concordance= 0.737 (se = 0.022 ) ## Likelihood ratio test= 61.07 on 5 df, p=7e-12 ## Wald test = 63.41 on 5 df, p=2e-12 ## Score (logrank) test = 66.55 on 5 df, p=5e-13 and check the Schoenfeld residuals for karno, as in Figure 8.5. ggcoxzph(cox.zph(cox_vet), var = &quot;karno&quot;) Figure 8.5: Schoenfeld residuals for Cox regression model fit to veteran data, for covariate ‘karno’ Although these look quite evenly spread, they are mostly negative, and it appears there is a slight trend with time. Indeed the p-value shows a significant deviation from proportional hazards. References ———. 2003b. Modelling Survival Data in Medical Research. 2nd ed. Texts in Statistical Science. Chapman &amp; Hall. Cox, David R. 1972. “Regression Models and Life-Tables.” Journal of the Royal Statistical Society: Series B (Methodological) 34 (2): 187–202. Grambsch, Patricia M, and Terry M Therneau. 1994. “Proportional Hazards Tests and Diagnostics Based on Weighted Residuals.” Biometrika 81 (3): 515–26. Schoenfeld, David. 1982. “Partial Residuals for the Proportional Hazards Regression Model.” Biometrika 69 (1): 239–41. "],["cluster-rct.html", "9 Cluster randomised trials 9.1 What is a cluster RCT? 9.2 Sample size 9.3 Allocation 9.4 Analysing a cluster RCT", " 9 Cluster randomised trials For the trials we’ve been studying so far, the intervention is applied at an individual level. For many treatments this is realistic, for example a medicine, injection or operation. However, for some treatments this is not practical. One example would be implementing a new cleaning regime in operating theatres. It would be almost impossible to implement this if different patients within the same hospital would be allocated to different cleaning styles. Logistically it would be very difficult, and there would likely be contamination as staff may be reluctant to clean an operating theatre in what might now seem an inferior way, for a control participant. In general it is very difficult (if not impossible) to implement changes in practice across healthcare systems at an individual level. The solution to this is to work at the group level, rather than the individual level. 9.1 What is a cluster RCT? In a cluster RCT, participants within the same natural group (e.g.. doctor’s surgery, hospital, school, classroom,…) are all allocated to the same group together. This means that, in the cleaning example above, the staff at a hospital in the treatment group can be trained in the new practice, all patients at that hospital will ‘receive the new treatment’, and contamination between groups is minimised. The main issue that makes cluster RCTs different is that participants within the same group are often likely to be more similar to one another than to participants from a different group. We expect that each group has its own ‘true’ mean \\(\\mu_k\\) , which is different from the underlying population mean \\(\\mu\\), and that the cluster means are distributed with mean \\(\\mu\\) and variance \\(\\sigma^2_b\\) (more on \\(\\sigma^2_b\\) soon). This violates one of the key assumptions we’ve held so far, that the data are independent, and leads us to a very important quantity called the intracluster correlation (ICC). 9.1.1 Intracluster correlation The ICC quantifies the relatedness of data that are clustered in groups by comparing the variance within groups to the variance between groups. The ICC is given by \\[ICC = \\frac{\\sigma^2_b}{\\sigma^2_b + \\sigma^2_w}, \\] where \\(\\sigma^2_b\\) is the variance between groups and \\(\\sigma^2_w\\) is the variance within groups. At one extreme, where \\(\\sigma_w^2=0\\), we have \\(ICC=1\\) and all measurements within each group are the same. At the other extreme, where \\(\\sigma^2_b=0\\), \\(ICC=0\\) and in fact all groups are independent and identically distributed. We can estimate \\(\\sigma_w^2\\) and \\(\\sigma_b^2\\) using \\(s_w^2\\) and \\(s_b^2\\), which we find by decomposing the pooled variance. Here, \\(g\\) is the number of groups, \\(n_j\\) is the number of participants in group \\(j\\) and \\(n\\) is the total number of participants. \\[ s^2_{Tot} = \\frac{\\sum\\limits_{j=1}^g \\sum\\limits_{i=1}^{n_j}\\left(x_{ij}-\\bar{x}\\right)^2}{n-g}\\\\ \\] We can split this up as \\[ \\begin{aligned} s^2_{Tot} &amp; = \\frac{1}{n-g}\\sum\\limits_{j=1}^g \\sum\\limits_{i=1}^{n_j}\\left(x_{ij} - \\bar{x}_j + \\bar{x}_j-\\bar{x}\\right)^2\\\\ &amp; = \\frac{1}{n-g}\\sum\\limits_{j=1}^g \\sum\\limits_{i=1}^{n_j}\\left[\\left(x_{ij} - \\bar{x}_j\\right)^2 + \\left(\\bar{x}_j-\\bar{x}\\right)^2 + 2\\left(x_{ij} - \\bar{x}_j\\right)\\left(\\bar{x}_j-\\bar{x}\\right)\\right]\\\\ &amp; = \\frac{1}{n-g}\\sum\\limits_{j=1}^g \\sum\\limits_{i=1}^{n_j}\\left[\\left(x_{ij} - \\bar{x}_j\\right)^2 + \\left(\\bar{x}_j-\\bar{x}\\right)^2 \\right]\\\\ &amp;= \\underbrace{\\frac{1}{n-g}\\sum\\limits_{j=1}^g \\sum\\limits_{i=1}^{n_j}\\left(x_{ij} - \\bar{x}_j\\right)^2}_{\\text{Within groups}} + \\underbrace{\\frac{1}{n-g}\\sum\\limits_{j=1}^g n_j\\left(\\bar{x}_j - \\bar{x}\\right)^2}_{\\text{Between groups}} \\end{aligned} \\] Example 9.1 We will demonstrate the ICC using a dataset that has nothing to do with clinical trials. The cheese dataset contains the price per unit and volume of sales of cheese (who knows what kind) at many Kroger stores in the US. We also know which city the Krogers are in, and we have data for 706 stores across 11 cities. It might be reasonable to expect that if we have information about the price and volume for several stores within a particular city, this gives us more information about the price and volume for another store in that same city than for a store in another city. Figure 9.1 shows the price and volume for all stores, coloured by city. cheese = read.csv(&quot;kroger.csv&quot;, header=T) cheese$city = as.factor(cheese$city) ggplot(data=cheese, aes(x=price, y=vol, col=city)) + geom_point() Figure 9.1: Price per unit and volume of sales of cheese for 706 Kroger stores. To calculate the ICC we define two functions, between.var and within.var, to calculate the between group and within group variance, as explained above. Click to show R functions # Firstly we define functions for the estimates between.var = function( data, groupvec ){ groups = levels(as.factor(groupvec)) ng = length(groups) ntot = length(data) means = sapply(1:ng, function(i){mean(data[groupvec == groups[i]])}) njvec = sapply(1:ng, function(i){length(data[groupvec == groups[i]])}) mean = mean(data) ssqvec = sapply(1:ng, function(i){(njvec[i]*(means[i]-mean)^2)}) sum(ssqvec)/(ntot-ng) } within.var = function( data, groupvec ){ groups = levels(as.factor(groupvec)) ng = length(groups) ntot = length(data) means = sapply(1:ng, function(i){mean(data[groupvec == groups[i]])}) njvec = sapply(1:ng, function(i){length(data[groupvec == groups[i]])}) g_sums = rep(NA, ng) for (j in 1:ng){ data_j = data[groupvec == groups[j]] ssqvec = rep(NA, njvec[j]) for (i in 1:njvec[j]){ ssqvec[i] = (data_j[i] - means[j])^2 } g_sums[j] = sum(ssqvec)/(ntot - ng) } sum(g_sums) } wv_price = within.var(cheese$price, cheese$city) wv_price ## [1] 0.08860193 bv_price = between.var(cheese$price, cheese$city) bv_price ## [1] 0.03002494 icc = bv_price / (bv_price + wv_price) icc ## [1] 0.253104 If we had to predict the price of cheese in a new city, all we can say is we expect the mean \\(\\mu_{g+1}\\) to come from \\(N\\left(\\mu,\\;\\sigma^2_b\\right)\\), where \\(\\mu\\) is the mean price of cheese in the overall population and \\(\\sigma^2_b\\) is the between group variance, and the individual cheese prices to come from \\(N\\left(\\mu_{g+1},\\,\\sigma^2_w\\right)\\). mean(cheese$price) ## [1] 2.641317 Estimating the ICC when planning a study is an important step, but isn’t always easy. For a well-understood (or at least well-documented) condition, it can often be estimated from existing data, which is likely to cover many sites. In non-medical studies like education or social interventions (where cluster RCTs are very common), it can be much more difficult because there is generally less data. Statistical studies are much newer in these areas, though they are becoming increasingly common, and even mandated by some organisations (for example the Educational Endowment Foundation). 9.2 Sample size The upshot of the non-independence of the sample is that we have less information from \\(n\\) participants in a cluster RCT than we would do for an individual-based RCT where all the participants were independent (at least conditional on some covariates). At one extreme, where ICC=0, there is in fact no intracluster correlation, all the groups have the same mean, and this is the same as a normal RCT. At the other extreme, where ICC=1, all measurements within a cluster are identical, and to achieve the same power as with \\(n\\) participants in a standard RCT, we would need \\(n\\) clusters (and their size would be irrelevant). Obviously neither of these is ever true! In most studies, the ICC is in \\(\\left(0,\\;0.15\\right)\\). We will consider the sample size (and indeed most other things) for a cluster RCT in which the outcome is continuous (as in Chapter 2), but you can equally do a cluster RCT with a binary or time-to-event outcome. The first step is to think about how the clustering affects the variance of the sample mean for either group. An estimate of the outcome variance in the control group, ignoring the clustering, is \\[\\begin{equation} \\hat{\\sigma}^2 = \\frac{\\sum\\limits_{j=1}^g\\sum\\limits_{i=1}^{n_j}\\left(X_{ij} - \\bar{X}\\right)^2}{n-1}, \\tag{9.1} \\end{equation}\\] where as before there are \\(g\\) clusters, cluster \\(j\\) has size \\(n_j\\) and \\(\\sum\\limits_{j=1}^gn_j=n\\) is the total sample size. The mean is also calculated without reference to the clustering, so \\[\\bar{X} = \\frac{\\sum\\limits_{j=1}^g\\sum\\limits_{i=1}^{n_j}X_{ij}}{n}.\\] It can be shown that the variance of the overall mean in either group is inflated by a factor of \\[1 + \\rho_{ICC}\\left[ \\frac{\\sum\\limits_j n_j^2}{N} - 1 \\right], \\] where \\(\\rho_{ICC}\\) is the intracluster correlation. This quantity is known as the design effect. Notice that if all the groups are the same size \\(n_g = \\frac{n}{g}\\) then the design effect simplifies to \\[1 + \\rho_{ICC}\\left(n_g - 1\\right).\\] We will assume from now on that this is the case. 9.2.1 A formula for sample size Now that we know \\(\\operatorname{E}\\left(\\hat{\\sigma}^2\\right)\\) we can adapt our sample size formula from Section 2.5. For an individual-level RCT with a continuous outcome, and assuming an unpaired, two-sided t-test comparing the outcomes, we had \\[\\begin{equation} n = \\frac{2\\sigma^2\\left(z_{\\beta} + z_{\\alpha/2}\\right)^2}{\\tau^2_M}, \\tag{9.2} \\end{equation}\\] and the reason we were able to do this was because the variance of the treatment effect estimate was \\(\\sigma^2/n\\). For a cluster RCT, the variance of the treatment effect under the same model assumptions is \\[\\begin{equation} \\frac{\\sigma^2}{n} \\left[1 + \\rho_{ICC}\\left(\\frac{\\sum\\limits_{j=1}^g n_j}{n}-1\\right)\\right] \\tag{9.3} \\end{equation}\\] At the planning stage of a cluster RCT we are unlikely to know the size of each cluster; each individual involved will usually need to give their consent, so knowing the size of the hospital / GP surgery / class is not enough. Instead, usually a [conservative] average cluster size \\(n_g\\) is specified, and this is used. In this case, the variance of the treatment effect in Equation (9.3) becomes \\[\\begin{equation} \\frac{\\sigma^2}{n} \\left[1 + \\rho_{ICC}\\left(n_g-1\\right)\\right] . \\tag{9.4} \\end{equation}\\] Equation (9.4) can be combined with Equation (9.2) to give the sample size formula for a cluster RCT: \\[n = \\frac{2\\sigma^2\\left[1 + \\rho_{ICC}\\left(n_g-1\\right)\\right]\\left(z_{\\beta} + z_{\\alpha/2}\\right)^2}{\\tau^2_M}. \\] Since \\(n=n_gg\\), this can be rearranged to find the number of clusters of a given size needed, or the size of cluster if a given number of clusters is to be used. The sample size (and therefore the real power of the study) depends on two additional quantities that are generally beyond our control, and possibly knowledge: \\(\\rho_{ICC}\\) and \\(n_g\\). It is therefore sensible to conduct some sensitivity analysis, with several scenarios of \\(\\rho_{ICC}\\) and \\(n_g\\), to see what the implications are for the power of the study if things don’t quite go to plan. Example 9.2 This is something I wrote for an education study I’m involved in (funded by the EEF) where the treatment is a particular way of engaging 2 year olds in conversation. The outcome variable is each child’s score on the British Picture Vocabulary Scale (BPVS), a test aimed at 3 - 16 year olds designed to assess each child’s vocabulary. We needed to recruit some number of nurseries, but had very little information about the ICC (notice that the age in our study is outside the intended range of the BPVS test!). To help the rest of the evaluation team understand the sensitivity of the power of the study to various quantities, I designed this dashboard, so that they could play around with the variables and see the effect. As well as giving the sample size for a simple t-test (as we’ve done above) it also shows the size for a random effects model (similar to ANCOVA, more on this soon), which is why the baseline-outcome correlation (about which we also know very little) is included. The plot shows the minimum detectable effect size (MDES, or \\(\\tau_M\\), in SD units), since the evaluation team wanted to know what size of effect we could find with our desired power. 9.3 Allocation In a cluster RCT, everyone within a particular cluster will be in the same group (\\(T\\) or \\(C\\)). Therefore, the allocation needs to be performed at the cluster level, rather than at the individual level as we did in Chapter 3. In theory we could use any of the first few methods we learned (simple random sampling, random permuted blocks, biased coin, urn design) to allocate the clusters. However, there are often relatively few clusters, and so the potential for imbalance in terms of the nature of the clusters would be rather high. This means we are more likely to use a stratified method or minimisation. In terms of prognostic factors, there are now two levels: cluster level and individual level. For example, in a study with GP practices as clusters, some cluster-level covariates could be the size of the practice, whether it was rural or urban, the IMD (index of multiple deprivation) of the area it was in. It would be sensible to make sure there was balance in each of these in the allocation. One might also include aggregates of individual-level characteristics, for example the mean age, or the proportion of people with a particular condition (especially if the study relates to a particular condition). However, a key feature of cluster RCTs means that in fact some different, and perhaps more effective, allocation methods are open to us. 9.3.1 Allocating everyone at once The methods we’ve covered so far assume that participants are recruited sequentially, and begin the intervention at different points in time. In this scenario, when a particular participant (participant \\(n\\)) is allocated we only know the allocation for the previous \\(n-1\\) participants. It is very likely that we don’t know the details of the following participants, in particular their values of any prognostic variables. This makes sense in many medical settings, where a patient would want to begin treatment as soon as possible, and there may be a limited number of patients with particular criteria at any one time. However, cluster RCTs rarely deal with urgent conditions (at least in the sense of administering a direct treatment), and so the procedure is usually that the settings (the clusters) are recruited over some recruitment period and all begin the intervention at the same time. This means that at the point of allocation, the details of all settings involved are known. There are a couple of proposals for how to deal with allocation in this scenario, and we will look at one now. 9.3.2 Covariate constrained randomization This method is proposed in Dickinson et al. (2015), and implemented in the R package cvcrand. We’ll review the key points of the method, but if you’re interested you can find the details in the article. Baseline information must be available for all settings, for any covariate thought to be potentially important. These can be setting-level variables or aggregates of individual-level variables. Once all this data has been collected, the randomisation procedure is as follows. Firstly, generate all possible allocations of the clusters into two arms (\\(T\\) and \\(C\\)). Secondly, rule out all allocations that don’t achieve the desired balance criteria. For a categorical covariate, the procedure is very simple. For example, we may stipulate that we want groups \\(T\\) and \\(C\\) to have the same number of rural GP practices as one another. In this case, we would remove from our set of possible allocations any where the number was different, or perhaps where it differed by more than some number \\(d_{rural}\\). We continue for all the covariates we want to balance, setting rules for each one. Continuous covariates are standardized and used to calculate a ‘balance score’ \\(B\\) for each of the remaining allocations. A cut-off is used to rule out all allocations that don’t achieve the desired level of balance. This leaves an ‘optimal set’ of allocations. Finally, an allocation is chosen at random from the optimal set, and this is the allocation that is used. 9.4 Analysing a cluster RCT As with the other stages of a cluster RCT, to conduct an effective and accurate analysis we need to take into account the clustered nature of the data. There are several ways to do this, and we will whizz through the main ones now. Example 9.3 The data we use will be from an educational trial, contained in crtData in the package eefAnalytics, shown in Figure 9.2 . The dataset contains 22 schools and 265 pupils in total. Each school was assigned to either 1 (group \\(T\\)) or 0 (group \\(C\\)). Each pupil took a test before the trial, and again at the end of the trial. We also know the percentage attendance for each pupil. We will use this data to demonstrate each method. ## &#39;data.frame&#39;: 265 obs. of 4 variables: ## $ School : Factor w/ 22 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ Posttest : num 16 13 18 14 25 13 23 26 16 8 ... ## $ Intervention: Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ Prettest : num 1 4 5 4 5 2 5 5 2 2 ... ggplot(data=crt_df, aes(y=Posttest, fill=Intervention, group = School)) + geom_boxplot() Figure 9.2: Box plots of the outcome Posttest for each school, coloured by Intervention. 9.4.1 At the cluster level In cluster level analysis, the data are aggregated to the cluster level, so that for each cluster (school, in our case), there is effectively one data point. The advantage of this approach is that, because the design is conducted at the cluster level, the statistical methodology is relatively simple - for example, a t-test. However, if cluster sizes vary a lot, a cluster-level analysis is often not appropriate because they usually rely on the assumption that the variance of the outcome in each cluster is approximately the same. As we have seen, the larger a group, the smaller the variance of its sample mean. There are methods designed to account for this, such as the weighted t-test, but these methods are generally inefficient and less robust. These methods are generally thought to be appropriate for fewer than 15-20 clusters per treatment arm. One possibility for our trial would be to collect the mean and SD of scores within each school, and perform a t-test to find out if there is a significant difference between the intervention and control arms. If we wanted to find out whether this depended on, say, gender, we could split the data set and perform separate t-tests for the different gender groups. This has the advantage that it is simple to implement, but the disadvantage that it is difficult to take into account covariates (apart from in the simple way discussed for eg. gender). With a small study, it is likely that there is some imbalance in the design in terms of covariates. The required sample size for this option would be \\[ g = \\frac{2 \\sigma^2 \\left[1+ \\left(n_g-1\\right)\\rho_{ICC}\\right]}{n_g\\tau^2_M}\\left(z_{\\beta} + z_{\\frac{1}{2}\\alpha}\\right)^2, \\] where \\(n_g\\) is the average cluster size and \\(g\\) is the number of clusters per treatment arm. This is the value we worked out in Section 9.2.1 Example 9.4 We can perform this analysis on our schools data. The first step is to calculate the difference between posttest and pretest for each pupil. crt_df$diff = crt_df$Posttest - crt_df$Prettest We can then aggregate this to find the mean of diff for each school, shown in Table 9.1: Table 9.1: EEF data with difference calculated (first 10 rows). School MeanDiff ng Group 1 13.76923 13 1 2 19.09091 33 1 3 20.43333 30 1 4 19.00000 30 0 5 16.66667 15 1 6 14.40000 5 1 7 16.75000 24 1 8 13.83333 12 0 9 15.00000 4 0 10 18.28571 14 1 We can also visualise the mean differences by group Figure 9.3: Boxplots of the mean differences for each trial arm From Figure 9.3 it certainly looks likely that a significant difference will be found. t.test( x=crt_summ$MeanDiff[crt_summ$Group==0], y=crt_summ$MeanDiff[crt_summ$Group==1], alternative = &quot;two.sided&quot;, paired = F, var.equal=F ) ## ## Welch Two Sample t-test ## ## data: crt_summ$MeanDiff[crt_summ$Group == 0] and crt_summ$MeanDiff[crt_summ$Group == 1] ## t = -2.2671, df = 19.983, p-value = 0.03463 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -6.4133879 -0.2667059 ## sample estimates: ## mean of x mean of y ## 13.72454 17.06459 This is fairly easy to implement, but it seems rather unsatisfactory. It’s also probably not entirely appropriate because the group sizes vary from one (!) to 33. What we need is a linear (in the continuous outcome case at least) model that takes into account the covariates, and makes the most of the available data. 9.4.2 At the individual level: mixed effects models Perhaps the prevalent way of analysing the data from cluster randomized trials is the mixed effects model or random effects model, or multilevel model. In basic terms, mixed effects models are used whenever it cannot be assumed that the outputs are all independent of one another. In the cluster randomized trial setting, this is because outcomes for participants within the same cluster can be expected to be more highly correlated than outcomes for patients from different clusters, but we will see examples of other designs in the next chapter. To understand mixed effects models, we need to think about the difference between fixed effects and random effects, in particular for categorical/factor variables. Fixed effects These are the sorts of factor variables we’re used to dealing with in linear models: we don’t assume any relationship between the levels, and we are generally interested in comparing the groups or categories represented by the fixed effect factors. We’ve seen these throughout the course, most notably with the treatment group variable \\(G_i\\), but also with things like smoking history, sex, disease details. When conducting a clinical trial, we’re likely to try to include participants with all levels of the fixed effects we’re interested in, so that we can make inferences about the effect of the different levels of those effects. For example, we might want a good balance of male and female participants, so that we can understand the effect of the treatment on both groups. Random effects Random effects are probably just as common in real life, but we haven’t seen them yet. These are factor variables that we think of as being drawn from some underlying model or distribution. For example, this could be GP surgery or school class, or an individual. We expect each GP surgery / school class / individual to behave slightly differently (depending on the extent of the intracluster correlation) but to behave as though from some overall distribution. Unlike fixed effects, random effects are generally things we aren’t specifically interested in understanding the effect of, but we want to account for the variation they bring. We’re also unable to include all levels of the random effect in our study - for example, a study looking at the effect of an intervention in schools will involve perhaps 50 schools, but we want to apply to results to all schools in the UK (say). We therefore assume that the schools are drawn from some normal distribution (in terms of the outcome we’re interested in), and that therefore all the schools we haven’t included also belong to this distribution. In Example 9.1 we aren’t trying to compare different cities, and we certainly don’t have data for Kroger stores in all cities, but we’re assuming that the mean cheese price \\(\\mu_{\\text{city}_i}\\) in the different cities is drawn from \\(N\\left(\\mu,\\,\\sigma^2_B\\right)\\), and that within each city the cheese price is drawn from \\(N\\left(\\mu_{\\text{city}_i},\\;\\sigma^2_W\\right)\\). Including random effects allows us to account for the fact that some schools might be in general a bit better / worse performing, or some individuals might be a bit more / less healthy, because of natural variation. We will see more examples of the use of random effects in the next chapter The mixed effects model Mixed effects models allow us to combine fixed effects and random effects. They are useful for many more situations than cluster RCTs, but this is as good a place as any to start! The mixed effects model takes the form \\[\\begin{equation} x_{ijk} = \\alpha + \\beta G_i + \\sum\\limits_l \\gamma_l z_{ijkl} + u_{ij} + v_{ijk} \\tag{9.5} \\end{equation}\\] where \\(x_{ijk}\\) is the outcome for the \\(k\\)-th individual in the \\(j\\)-th cluster in the \\(i\\)-th treatment arm (usually \\(i=0\\) is the control arm and \\(i=1\\) is the intervention arm) \\(\\alpha\\) is the intercept of the model \\(\\beta\\) is the intervention effect, and \\(G_i\\) the group indicator variable (0 for group \\(C\\), 1 for group \\(T\\)). Our null hypothesis is that \\(\\beta\\) is also zero) The \\(z_{ijkl}\\) are \\(L\\) different individual level covariates that we wish to take into account, and the \\(\\gamma_l\\) are the estimated coefficients. \\(u_{ij}\\) is a random effect relating to the \\(j\\)-th cluster in the \\(i\\)-th treatment arm. This is the term that accounts for the between-cluster variation. We assume \\(u_{ij}\\) is normally distributed with mean 0 and variance \\(\\sigma^2_B\\) (the between-cluster variance). \\(v_{ijk}\\) is a random effect relating to the \\(k\\)-th individual in the cluster (ie. an individual level random error term), assumed normally distributed with mean 0 and variance \\(\\sigma^2_W\\) (the within-cluster variance). The part of the model that makes this particularly suitable to a cluster randomized trial is \\(u_{ij}\\). Notice that this has no \\(k\\) index, and is therefore the same for all participants within a particular cluster. With a random effects model we can take into account the effects of individual-level covariates and also the clustered design of the data. Approximately, our sample size requirements are \\[ k = \\frac{2 \\sigma^2 \\left[1+ \\left(m-1\\right)\\rho_{ICC}\\right]\\left(1-\\rho^2\\right) }{m\\tau^2_M}\\left(z_{\\beta} + z_{\\frac{1}{2}\\alpha}\\right)^2. \\] Broadly this follows on from the logic we used to show the reduction in variance from the ANCOVA model in Section 4.3.1.2, and you’ll notice that the factor of \\(1-\\rho^2\\) is the same. The details for cluster randomized trials are given in Teerenstra et al. (2012). This is the ‘Random effects model’ line in the shiny dashboard.) The random effects model is more suitable when there are more than around 15-20 clusters in each arm. Example 9.5 We’ll now fit a random effects model to the crtData dataset from eefAnalytics. A good starting point is to plot the data with this in mind, to see what we might expect. Figure 9.4: Posttest against Prettest, coloured by Intervention From Figure 9.4 it appears there might be a positive relationship between Prettest and Posttest, and also that the Posttest scores might be higher in the intervention group. We’ll do this using the R package lme4. The function to specify a linear mixed effects model is called lmer, and works very similarly to lm. The term (1|School) tells R that the variable School should be treated as a random effect, not a fixed effect. library(lme4) library(sjPlot) lmer_eef1 = lmer(Posttest ~ Prettest + Intervention + (1|School), data=crt_df ) summary(lmer_eef1) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: Posttest ~ Prettest + Intervention + (1 | School) ## Data: crt_df ## ## REML criterion at convergence: 1493.8 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.46249 -0.64166 0.01626 0.59655 2.60277 ## ## Random effects: ## Groups Name Variance Std.Dev. ## School (Intercept) 5.674 2.382 ## Residual 14.779 3.844 ## Number of obs: 265, groups: School, 22 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 11.2286 1.1250 9.981 ## Prettest 1.7889 0.2004 8.928 ## Intervention1 3.1097 1.2094 2.571 ## ## Correlation of Fixed Effects: ## (Intr) Prttst ## Prettest -0.681 ## Interventn1 -0.493 -0.010 The package sjPlot contains functions to work with mixed effect model objects, for example plot_model sjPlot::plot_model(lmer_eef1) Figure 9.5: CIs for the model coefficients Perhaps unsurprisingly, the coefficient of the baseline test score is very significant, and the intervention also has a significant effect. This function also estimates the intracluster correlation. There are various different ways we can include random effects in the model, as shown in Figure 9.6. In our EEF data example we have used a fixed slope and random intercept. Figure 9.6: Different ways of including random effects in a mixed effects model. The mixed effects model can be extended to a generalized mixed effects model, which is akin to a generalized linear model. For example, with a binary outcome \\(X_{ijk}\\) we can adopt the model \\[ \\operatorname{logit}\\left(\\pi_{ijk}\\right) = \\alpha + \\beta G_i + \\sum\\limits_l \\gamma_l z_{ijkl} + u_{ij} + v_{ijk}. \\] The next section covers some more trial designs for which mixed effects models are useful. References Dickinson, L Miriam, Brenda Beaty, Chet Fox, Wilson Pace, W Perry Dickinson, Caroline Emsermann, and Allison Kempe. 2015. “Pragmatic Cluster Randomized Trials Using Covariate Constrained Randomization: A Method for Practice-Based Research Networks (PBRNs).” The Journal of the American Board of Family Medicine 28 (5): 663–72. Teerenstra, Steven, Sandra Eldridge, Maud Graff, Esther de Hoop, and George F Borm. 2012. “A Simple Sample Size Formula for Analysis of Covariance in Cluster Randomized Trials.” Statistics in Medicine 31 (20): 2169–78. "],["random-effects-for-individuals.html", "10 Random effects for individuals 10.1 Crossover trials 10.2 Longitudinal data / repeated measurements", " 10 Random effects for individuals In the last lecture we introduced the mixed effects model as a way to combine fixed effects (the kind we’re used to, like sex, disease status etc.) and random effects. For cluster randomized trials this was useful because we could account for the intracluster correlation, where participants within the same school / GP surgery / other cluster are likely to be more similar to one another than to participants within another group. In this lecture, we’ll think about a couple of examples where the individual is the random effect. 10.1 Crossover trials In the trials we’ve thought about so far, each patient has been subject to one, and only one, treatment. Often this is necessary, because the end goal is something relatively final, like a disease being cured, or the circumstance is a one-off, like medication to prevent complications of surgery. By doing things this way, we lose the opportunity to make direct comparisons; we assume that all participants are randomly drawn from some population, and make efforts to ensure the trial is balanced in terms of their characteristics, but we can’t truly compare like-for-like. Crossover trials, also known as AB or AB/BA trials, involve giving each participant both treatments in sequence. These are generally used in contexts where the goal is ongoing management of a condition, for example asthma or diabetes. If we give each participant each of the different treatments in sequence, then we can make a more precise comparison, since we are comparing the outcomes under the different treatments on the same patient. You can find more detail of this type of trial in Armitage and Hills (1982). 10.1.1 The AB/BA design If we gave all patients treatment A first (say for one month) and then treatment B (for the next), then we would be inviting bias and ambiguity into the trial It may be that whichever treatment comes second appears to perform better, for example as patients get used to measurements being taken (it is common for the first few blood pressure measurements in a series to be higher) If there is some external trend or pattern, this will manifest itself as an effect of the change in treatments. Therefore in a crossover trial with two treatments there are two groups, who receive the treatments in different orders: Group Period 1 Period 2 Group 1 A B Group 2 B A Therefore if there is a temporal effect it won’t be ascribed to either of the treatments. 10.1.2 Analysis of the AB/BA design There are a few things our analysis needs to account for: Measurements for the same participant need to be treated as such, in some way The possibility of systematic differences between treatment periods needs to be allowed for In the model we describe, we assume there are \\(n_1\\) participants in group 1, and \\(n_2\\) in group 2. We will also assume that the outcome is normally distributed. In the treatment of ongoing conditions, this is generally the case. We will start by thinking about the outcome for particular individuals. We’ll assume that one outcome measurement is taken per patient per trial period. So the outcome for participant \\(i\\) \\((i = 1,\\ldots,n_1)\\) from group 1 is modelled by \\[\\begin{align*} x_{i1} &amp; = \\mu + \\pi_1 + \\tau_A + \\xi_i + \\epsilon_{i1} \\text{ in trial period 1}\\\\ x_{i2} &amp; = \\mu + \\pi_2 + \\tau_B + \\xi_i + \\epsilon_{i2} \\text{ in trial period 2.} \\end{align*}\\] In this model \\(\\pi_j\\;\\;(j=1,2)\\) is the systematic effect of period \\(j\\) \\(\\tau_A,\\,\\tau_B\\) are the systematic effects of treatments A and B \\(\\mu\\) is a general mean \\(\\xi_i\\) represents the tendency for the outcome of participant \\(i\\) to be generally higher or lower than the mean \\(\\epsilon_{i1},\\,\\epsilon_{i2}\\) are independent \\(N\\left(0,\\,\\sigma_{\\epsilon}^2\\right)\\) error terms. Notice that the first three bullet points describe fixed effects, and the fourth \\((\\xi_i)\\) is a random effect. We can assume that \\(\\xi_i \\sim N\\left(0,\\,\\sigma_B^2\\right)\\). Notice that \\(\\xi_i\\) appears in the outcome model for participant \\(i\\) for both trial periods. We can us the same logic to model the outcome for participant \\(i\\) \\(\\left(i=n_1+1,\\,\\ldots,\\,n_1+n_2\\right)\\) in group 2 \\[\\begin{align*} x_{i1} &amp; = \\mu + \\pi_1 + \\tau_B + \\xi_i + \\epsilon_{i1} \\text{ in trial period 1}\\\\ x_{i2} &amp; = \\mu + \\pi_2 + \\tau_A + \\xi_i + \\epsilon_{i2} \\text{ in trial period 2.} \\end{align*}\\] Because we have been able to treat each participant with both treatments (A and B), we have effectively bundled all the participant specific covariates, along with individual variation we can’t really account for, into \\(\\xi_i\\). We can remove \\(\\xi_i\\) from from the analysis by finding the differences for each patient \\[\\begin{align*} d_i &amp; = x_{i1} - x_{i2} = \\pi + \\tau + \\eta_i&amp; \\text{ for } i=1,\\,\\ldots,\\,n_1\\\\ d_i &amp; = x_{i1} - x_{i2} = \\pi - \\tau + \\eta_i&amp; \\text{ for } i=n_1+1,\\,\\ldots,\\,n_1+n_2, \\end{align*}\\] where \\(\\pi=\\pi_1 - \\pi_2\\) measures the difference between treatment periods \\(\\tau = \\tau_A - \\tau_B\\) is the treatment effect we are interested in \\(\\eta_i = \\epsilon_{i1} - \\epsilon_{i2}\\) is another random error, with mean 0 and variance \\(\\sigma^2=2\\sigma^2_{\\epsilon}\\) We can take expected values of the differences, and find \\[\\begin{align*} \\operatorname{E}\\left(d\\right) &amp; = \\pi+\\tau &amp; \\text{ in group 1}\\\\ \\operatorname{E}\\left(d\\right) &amp; = \\pi-\\tau &amp; \\text{ in group 2} \\end{align*}\\] If there is no treatment effect, ie. \\(\\tau=0\\), then we expect the means to be approximately equal. This means we can test the null hypothesis that \\(\\tau=0\\) with a two-sample \\(t\\)-test, to compare the two sets of within patient differences. Note, this is not a paired t-test: we are comparing the differences for the \\(n_1\\) participants in group 1 with the differences for the \\(n_2\\) participants in group 2. An estimate of the treatment effect can be found using \\[\\frac{1}{2}\\left(\\bar{d}_1 - \\bar{d}_2\\right),\\] where \\(\\bar{d}_k\\) is the mean difference in group \\(k\\). We can therefore find a confidence interval by halving the limits of the usual confidence interval for the difference in two means. Another important point to note is that since we have eliminated the individual participant random effect \\(\\xi_i\\) from the analysis, the precision of the result depends only on \\(\\sigma^2_{\\epsilon}\\), and not on the between-individual variance \\(\\sigma^2_B\\). This is especially good news in a scenario in which the measurement varies much more between individuals than within individuals. Intuitively this makes sense: it is best to use a patient as his or her own control. Example 10.1 For children with nocturnal enuresis (bed-wetting) a drug is available to potentially alleviate the problem, and in this trial the drug is to be compared with a placebo, and the treatment period is two weeks. Children in group A are given the drug first, followed by the placebo. Children in group B are given the placebo first, followed by the drug. The primary outcome variable is the number of dry nights out of the 14 night of each treatment period. The data are shown in Table 10.1. Table 10.1: Data for group A (left) and group B (right) Period1 Period2 Diff 8 5 3 14 10 4 8 0 8 9 7 2 11 6 5 3 5 -2 13 12 1 10 2 8 6 0 6 0 0 0 7 5 2 13 13 0 8 10 -2 7 7 0 9 0 9 10 6 4 2 2 0 Period1 Period2 Diff 12 11 1 6 8 -2 13 9 4 8 8 0 8 9 -1 4 8 -4 8 14 -6 2 4 -2 8 13 -5 9 7 2 7 10 -3 7 6 1 We can use this information to find the mean difference for each group mean(dfA$Diff) ## [1] 2.823529 mean(dfB$Diff) ## [1] -1.25 Immediately we can see that these back up the notion that the drug is effective (remember the difference for group A is drug minus placebo, the difference for group B is placebo minus drug), and that the number of dry nights tends to be more while taking the drug. To test the hypothesis more carefully we use a t-test. t.test(x=dfA$Diff, y=dfB$Diff, paired=F, alternative = &quot;two.sided&quot;, var.equal=T) ## ## Two Sample t-test ## ## data: dfA$Diff and dfB$Diff ## t = 3.2925, df = 27, p-value = 0.002773 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 1.535005 6.612054 ## sample estimates: ## mean of x mean of y ## 2.823529 -1.250000 We see that this backs up the hypothesis that the drug reduces nocturnal enuresis. We could analyse the same dataset without taking into account the crossover design of the trial, simply by performing a t-test of all measurements on the drug against all measurement on the placebo drug_meas = c(dfA$Period1,dfB$Period2) # Measurements taken while on drug placebo_meas = c(dfA$Period2, dfB$Period1) # Measurements taken while on placebo t.test( x=drug_meas, y=placebo_meas, paired=F, alternative=&quot;two.sided&quot;, var.equal=T) ## ## Two Sample t-test ## ## data: drug_meas and placebo_meas ## t = 2.253, df = 56, p-value = 0.0282 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.2408072 4.1040204 ## sample estimates: ## mean of x mean of y ## 8.448276 6.275862 We see that although the outcome is still significant at the \\(\\alpha=0.05\\) level, the p-value is much larger, because the individual effects have not been taken into account. 10.2 Longitudinal data / repeated measurements The second situation we will consider is where there are repeated measurements of the same participant: this is often called longitudinal data. So far we have considered only a baseline and final measurement of the outcome variable, but in practice it is common to measure the outcome at several points throughout the trial duration. Clinicians may want to understand not just how the outcome is behaving at the end of the trial, but how it evolves over time. They may also want to understand the variation in the measurement over time; for example, it is reasonable to expect that a particular patient’s blood presssure or respiratory function won’t be exactly the same every day, even under the same treatment and general conditions. In general we’ll say that we have two treatment groups, C (control) and T (treatment), containing \\(n_C\\) and \\(n_T\\) participants respectively. The \\(i^{\\text{th}}\\) participant in group \\(X\\) is observed \\(k_{Xi}\\) times through the course of the trial. Generally (and we’ll certainly assume here) the measurements are planned to be at the same times. Example 10.2 The first data set we’ll introduce is epilepsy, from the package HSAUR3. This trial (detailed in Thall and Vail (1990)) investigates the effect of an anti-epileptic drug called Progabide compared to a placebo (both groups were also receiving chemotherapy). For each patient, the number of seizures was recorded over four contiguous two-week periods. To visualise the data, we plot a spaghetti plot, as shown in Figure 10.1. Each line represents a participant, and shows their seizure rate over time. Figure 10.1: A spaghetti plot of the epilepsy data. We can also plot the mean seizure rate at each time within each group as in Figure 10.2 Figure 10.2: Mean seizure rate for each treatment group over time. To make visualisation somewhat clearer, we can do a standardised spaghetti plot. In this plot, the observations are standardised within each time point. So, if \\(x_{it}\\) is the outcome for participant \\(i\\) and time \\(t\\), the standardised spaghetti plot would show \\[\\frac{x_{it} - \\bar{x}_t}{s_t} \\] where \\(\\bar{x}_t,\\,s_t\\) are the sample mean and SD at time \\(t\\). The standardised spaghetti plot for the epilepsy data is shown in Figure 10.3 Figure 10.3: Standardised spaghetti plot for the epilepsy data. 10.2.1 Fitting a model In longitudinal data the measurements are ‘clustered’ within each participant, in a semi-similar way to the participants being clustered within settings in Chapter 9. Each participant will have certain characteristics that don’t change through the duration of the trial, for example their age (at start), sex, disease history etc. Some of these characteristics we will record as part of the dataset, but this will not fully explain the correlation within measurements of the same participant. We can also assume that, given the values of these covariates, the outcome of each participant has a slight (or mabye more than slight) propensity to be consistently either higher or lower than (or perhaps to be in line with) the mean. Therefore there will be a random effect associated with each participant. Because we are interested in how things evolve with time, we will also include time in the model. The questions we want to answer are: What is the effect of the covariates (including treatment group) on the outcome? What is the effect of time on the outcome? Does the relationship between outcome and time vary for different groups of individuals? 10.2.2 The empty model The very simplest mixed effects model we can fit is \\[\\begin{equation} Y_{ij} = \\beta_0 + u_{i} + \\epsilon_{ij}. \\tag{10.1} \\end{equation}\\] In this model, \\(Y_{ij}\\) is the outcome for the \\(i^{th}\\) participant at time \\(j\\), \\(\\beta_0\\) is a fixed effect representing the overall mean of the \\(Y_{ij}\\), \\(u_i\\) is the adjustment for the mean for participant \\(i\\) and \\(\\epsilon_{ij}\\) is the residual. In this model we assume: \\[\\begin{align*} u_{i} &amp; \\sim N\\left(0,\\,\\sigma^2_{u}\\right)\\\\ \\epsilon_{ij}&amp; \\sim N\\left(0,\\,\\sigma^2\\right)\\\\ u_i&amp;\\perp \\epsilon_{ij}. \\end{align*}\\] This model is actually a one-way analysis of variance (ANOVA) model. Example 10.3 We can fit the empty model for the epilepsy data via epi_0 &lt;- lmer(seizure.rate ~ 1 + (1 | subject), data = epilepsy) summary(epi_0) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: seizure.rate ~ 1 + (1 | subject) ## Data: epilepsy ## ## REML criterion at convergence: 1651.1 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.8126 -0.3264 -0.0797 0.1725 7.3634 ## ## Random effects: ## Groups Name Variance Std.Dev. ## subject (Intercept) 121.75 11.03 ## Residual 32.49 5.70 ## Number of obs: 236, groups: subject, 59 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 8.263 1.484 5.569 The estimated average seizure.rate is shown in the Fixed effects section, and is 8.263. The estimated variance of the individual random effect term (\\(u_i\\) above) is 121.75, and the estimated residual variance is 32.49. These are both given in the Random effects section of the summary. We can use these values to estimate the ICC, which is now the correlation between repeated measures of the same individual: \\[\\rho_{ICC} = \\frac{s^2_B}{s^2_B + s^2_W} = \\frac{121.75}{121.75+32.49} = 0.789.\\] This very high value of the ICC supports our choice of fitting a mixed effects model, rather than including only fixed effects. Building up the model from this empty model basically involves including more fixed effects and random effects. Since we are concerned with longitudinal data, the obvious first thing to include is time. We can start by including a linear effect of time, so that the model in Equation (10.1) becomes \\[\\begin{equation} Y_{ij} = \\beta_0 + \\gamma t_j + u_{i} + \\epsilon_{ij}. \\tag{10.2} \\end{equation}\\] So that the coefficient \\(\\gamma\\) describes the change in outcome with time. We might go on to extend the model in Equation (10.2) by including some baseline covariates \\(\\mathbf{b}_{i}\\) for participant \\(i\\) (these would be considered to be the same throughout the trial for participant \\(i\\)). Then our model becomes \\[\\begin{equation} Y_{ij} = \\beta_0 + \\gamma t_j + \\mathbf{b}_i^T\\boldsymbol\\beta + u_{i} + \\epsilon_{ij}. \\tag{10.3} \\end{equation}\\] In Equation (10.3), the \\(\\mathbf{b}_i\\) are being included as fixed effects, in much the same way we’re used to with linear models, and the corresponding vector of coefficients is \\(\\boldsymbol\\beta\\). We will explore these models by fitting them to our epilepsy data. ## Including just time epilepsy$period = as.numeric(epilepsy$period) epi_1 = lmer(seizure.rate ~ period + (1|subject), data=epilepsy) ## Including more baseline covariates epi_2 = lmer(seizure.rate ~ base + age + treatment + period + (1|subject), data=epilepsy) pm1 = plot_model(epi_1) pm2 = plot_model(epi_2) grid.arrange(pm1, pm2, nrow=1) Figure 10.4: Confidence intervals for fixed effects for the two fitted models. In this example it appears that neither time nor the treatment have a significant effect on the seizure rate. The final extension we’ll look at is if we think that different participants might change at different rates, all other things being equal. We can introduce a random effect \\(\\eta_i\\) to allow the coefficient of time to vary between participants \\[\\begin{equation} Y_{ij} = \\beta_0 + \\left(\\gamma + \\eta_i\\right) t_j + \\mathbf{b}_i^T\\boldsymbol\\beta + u_{i} + \\epsilon_{ij}. \\tag{10.4}. \\end{equation}\\] ## Including just time ## Including more baseline covariates epi_3 = lmer(seizure.rate ~ base + age + treatment + period + (period|subject), data=epilepsy) summary(epi_3) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: seizure.rate ~ base + age + treatment + period + (period | subject) ## Data: epilepsy ## ## REML criterion at convergence: 1581.8 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.8224 -0.3265 -0.0224 0.2306 8.0745 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## subject (Intercept) 49.0650 7.0046 ## period 0.2588 0.5088 -1.00 ## Residual 31.8399 5.6427 ## Number of obs: 236, groups: subject, 59 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) -5.25307 4.45463 -1.179 ## base 0.34338 0.03128 10.976 ## age 0.15667 0.13409 1.168 ## treatmentProgabide -0.82339 1.64777 -0.500 ## period -0.48475 0.33514 -1.446 ## ## Correlation of Fixed Effects: ## (Intr) base age trtmnP ## base -0.381 ## age -0.914 0.188 ## trtmntPrgbd -0.280 0.004 0.099 ## period -0.221 0.000 0.000 0.000 ## optimizer (nloptwrap) convergence code: 0 (OK) ## boundary (singular) fit: see help(&#39;isSingular&#39;) pm1 = plot_model(epi_2) pm2 = plot_model(epi_3) grid.arrange(pm1, pm2, nrow=1) Figure 10.5: Confidence intervals for model with same gradient for all participants (left) and for varying gradient (right). We see from the small variance for the random effect period that the gradients don’t vary very much at all, and indeed the coefficients of the fixed effects are very similar to those for a fixed gradient. Summary In these two lectures we’ve seen that random effects can be a useful way to take care of correlation between outputs for clusters of measurements. This clustering can be because participants are from some natural group (eg. a school, a family, a hospital) or it can be because repeated measurements are taken of the same participant. In either case, we’d expect the measurements within one cluster to be more highly correlated than those from different clusters. References Armitage, Peter, and Michael Hills. 1982. “The Two-Period Crossover Trial.” Journal of the Royal Statistical Society: Series D (The Statistician) 31 (2): 119–31. Thall, Peter F, and Stephen C Vail. 1990. “Some Covariance Models for Longitudinal Count Data with Overdispersion.” Biometrics, 657–71. "],["the-bayesian-approach.html", "11 The Bayesian Approach 11.1 The fundamental difference 11.2 Why is this important? 11.3 How do we choose a prior? 11.4 More complex trials", " 11 The Bayesian Approach In this course we have taken a consistently frequentist approach. The reason for this is mainly pragmatic rather than ideological: the frequentist approach is by far the most commonly used way to design and analyse clinical trials, and some organisations won’t even allow a Bayesian analysis. We aren’t going to cover the whole Bayesian approach in one lecture, but we’ll review some of the key differences, how the Bayesian approach to clinical trials works, and some advantages it has over frequentist methods (I’ll try to keep my least biased hat on). Two excellent sources for this are Spiegelhalter, Freedman, and Parmar (1994) (be warned this one is quite long, but the first sections alone are very useful) and Berry (2006). We’ll also illustrate the theory with some examples. You’ll have come across Bayesian methods at various points so far in other modules, but to varying extents, so we’ll approach things from a fairly elementary level. 11.1 The fundamental difference First, we’ll think a bit about exactly what’s happening when we use a frequentist approach to analysing a clinical trial. The frequentist approach In the frequentist approach, we view the underlying parameters \\(\\theta\\), including for example the treatment effect (whatever form it takes), group means, correlations, variances and so on, as fixed quantities. We view the data \\(X\\) as random, and as coming from some distribution governed by these fixed parameters. We might have different candidate values of these, most commonly a null hypothesis value of the treatment effect and (by implication) a range of alternative hypothesis values. This means that all the inferences we make are in terms of how likely or unlikely the data we observe (\\(\\mathbf{x}\\)) are in terms of their having been generated by this distribution (usually in terms of the null hypothesis distribution). We are led to outcomes like the \\(p\\)-value: the probability of seeing data at least this ‘unusual’ under \\(H_0\\) \\(100\\left(1-\\alpha\\right)\\%\\) confidence interval: the set of values of a parameter that cannot be rejected by a significance test performed at the \\(100\\left(1-\\alpha\\right)\\%\\) level. Maximum likelihood estimators, or likelihood ratio statistics: thinking in terms of which parameter values make the observed data \\(\\mathbf{x}\\) somehow the most likely Clearly none of these allow us to make direct statements about the treatment effect \\(\\tau\\), for example the probability of it being within a certain range, or about any other parameter. We can only say how consistent the observed data are with particular values of \\(\\tau\\), under certain assumptions. The Bayesian approach In the notation of parameter(s) \\(\\theta\\) and data \\(\\mathbf{x}\\), Bayes theorem is \\[\\begin{align*} p\\left(\\theta\\mid{\\mathbf{x}}\\right) &amp;= \\frac{p\\left(\\mathbf{x}\\mid\\theta\\right)p\\left(\\theta\\right)}{p\\left(\\mathbf{x}\\right)}\\\\ &amp;\\propto p\\left(\\mathbf{x}\\mid\\theta\\right)p\\left(\\theta\\right). \\end{align*}\\] Notice that \\(p\\left(\\mathbf{x}\\mid\\theta\\right)\\) is the likelihood, the same distribution we need in the frequentist approach We also need to specify a prior distribution \\(p\\left(\\theta\\right)\\) We don’t need to specify / find \\(p\\left(\\mathbf{x}\\right)\\) because it doesn’t depend on \\(\\theta\\), and we can find the one constant value that makes the numerator a legitimate probability distribution (ie. integrates to one) by other means. Above all, the most exciting thing is that we now have the posterior distribution, which is a probability distribution about the parameters \\(\\theta\\), and which therefore allows us to make direct inferences and probability statements about any parameter, including the treatment effect \\(\\tau\\). The Bayesian approach enables us to give a direct answer to the questions people actually ask, such as ‘what’s the probability the treatment effect is larger than 1?’ 11.2 Why is this important? Making probabilistic statements about the treatment effect Throughout this course we’ve thought in terms of a single null hypothesis where the treatment appears to have no better or worse effect on the participants than the control (generally we have \\(H_0:\\; \\tau=0\\), though not always). Really however, this is quite a simplistic breakdown of the scenario. In reality, it may be that in addition to the treatment effect in terms of the primary outcome, one drug has much worse or more common side effects implementing the new treatment would be very expensive in terms of equipment, manufacture, training etc. In this case, we might decide that the treatment effect has to be greater than some value \\(\\tau_S\\) to be considered clinically superior, and similarly that it has to be less than \\(\\tau_I\\) to be considered clinically inferior. This is linked to the idea of the minimum clinically important difference / minimum detectable effect size, except that in the Bayesian paradigm we have much more flexibility to consider other important values too. We call the region \\(\\left(\\tau_I,\\,\\tau_S\\right)\\) the range of equivalence. Figure 11.1, taken from Spiegelhalter, Freedman, and Parmar (1994), shows the clinically interesting possibilities for the treatment effect (our \\(\\tau\\), but \\(\\delta\\) in Spiegelhalter, Freedman, and Parmar (1994)). Given a posterior distribution for \\(\\tau\\), we can calculate the probability of each of these outcomes. This wouldn’t be possible in a frequentist framework. Figure 11.1: Possible clinically interesting ranges for the treatment effect. Sequential analysis In a frequentist setting, the result of the analysis (a p-value or confidence interval, say), depends on the trial design, because not only are we thinking about the data we observed, but we also include the notion of “other more extreme outcomes”. What these ‘more extreme outcomes’ are depends on how we’ve specified the trial design. For example, suppose we have a binary outcome \\(X\\), which can be 1 or 0, with unknown (but fixed) parameter \\(\\pi = p\\left(X=1\\right)\\). We observe 10 realisations (in sequence) and find that our observed data are \\(0, 0, 1, 0, 0, 1, 0, 0, 0, 1\\). We will write \\(n_1=3,\\,n_0=7\\). What would have been a ‘more extreme outcome’ than this? This notion depends on two things: The null value of \\(\\pi\\), which we’ll call \\(\\pi_0\\) (so \\(H_0: \\pi=\\pi_0\\)) The experimental design, and in particular the point at which we planned to stop making observations. If our plan was to take 10 observations and record \\(n_1\\), then for \\(\\pi_0&lt;0.3\\), \\(n_1 &gt; 3\\) would be ‘more extreme’. for \\(\\pi_0&gt;0.3\\), \\(n_1 = 0,1,2\\) would all be ‘more extreme’. Secondly, it could be that our plan was to keep taking observations until we had \\(n_1=3\\). If that was our design, then our outcome really is that it took \\(n_{stop}=10\\) observations. In this case, then for \\(\\pi_0&lt;0.3\\), then an outcome \\(n_{stop}&lt;10\\) would be ‘more extreme’ (since we expect it to take longer to find three ones) for \\(\\pi_0&gt;0.3\\) then an outcome of \\(n_{stop}&gt;10\\) would be ‘more extreme’. To do an analysis we’ll have to specify some values, so we’ll keep to \\(\\pi_0&gt;0.3\\) now. For the first type of design (observing 10 in total) we can find the probability of each possible outcome, shown in Table 11.1 for \\(\\pi_0=0.4\\) and \\(\\pi_0=0.8\\). To find the p-values associated with our observation \\(n_1=3\\), we need the cumulative probability for each outcome, and these are shown in Table 11.1. Table 11.1: Probabilities for all possible outcomes (left) and cumulative probabilities (right) for the design where we make 10 observations pi[0]=0.4 pi[0]=0.8 N1=0 0.006 0.000 N1=1 0.040 0.000 N1=2 0.121 0.000 N1=3 0.215 0.001 N1=4 0.251 0.006 N1=5 0.201 0.026 N1=6 0.111 0.088 N1=7 0.042 0.201 N1=8 0.011 0.302 N1=9 0.002 0.268 N1=10 0.000 0.107 pi[0]=0.4 pi[0]=0.8 N1=0 0.006 0.000 N1=1 0.046 0.000 N1=2 0.167 0.000 N1=3 0.382 0.001 N1=4 0.633 0.006 N1=5 0.834 0.033 N1=6 0.945 0.121 N1=7 0.988 0.322 N1=8 0.998 0.624 N1=9 1.000 0.893 N1=10 1.000 1.000 So we see that if \\(\\pi_0=0.4\\), our p-value is 0.382, whereas if \\(\\pi_0=0.8\\), our p-value is 0.000864 (rounded in the table). We can’t say anything (much) about other possible values of \\(\\pi_0\\), or make any probabilistic statement about the value of \\(\\pi\\). We can only say that our observation of \\(n_1=3\\) is quite unremarkable if \\(pi_0=0.4\\), but very unlikely if \\(\\pi_0=0.8\\). For our other possible design, where we stop after observing three 1’s, the possible values of \\(n_{stop}\\) are three and above. We can find the probability of each possible value by \\[ \\binom{n-1}{2} \\pi_0^3\\left(1-\\pi_0\\right)^{n-3}.\\] The first term comes from the fact that the first two observations of \\(X=1\\) can be distributed anywhere through the first \\(n-1\\) realisations, but to have \\(n_{stop}=n\\) the \\(n^{th}\\) observation must be 1. We can make the same tables for [some] possible outcomes of \\(n_{stop}\\), and these are shown in Table 11.2. Table 11.2: Probabilities for all possible outcomes (left) and cumulative probabilities (right) when we stop after the third observation of X=1. pi[0]=0.4 pi[0]=0.8 Nstop=3 0.064 0.512 Nstop=4 0.115 0.307 Nstop=5 0.138 0.123 Nstop=6 0.138 0.041 Nstop=7 0.124 0.012 Nstop=8 0.105 0.003 Nstop=9 0.084 0.001 Nstop=10 0.064 0.000 Nstop=11 0.048 0.000 Nstop=12 0.035 0.000 Nstop=13 0.026 0.000 Nstop=14 0.018 0.000 Nstop=15 0.013 0.000 pi[0]=0.4 pi[0]=0.8 N1=3 0.064 0.512 N1=4 0.179 0.819 N1=5 0.317 0.942 N1=6 0.456 0.983 N1=7 0.580 0.995 N1=8 0.685 0.999 N1=9 0.768 1.000 N1=10 0.833 1.000 N1=11 0.881 1.000 N1=12 0.917 1.000 N1=13 0.942 1.000 N1=14 0.960 1.000 N1=15 0.973 1.000 To find the p-value for our observation \\(n_{stop}=10\\) we need to take one minus the cumulative probability, since it’s all the larger values of \\(n\\) that are ‘more extreme’. So for \\(\\pi_0=0.4\\) we have \\(p=1-0.833 = 0.1873\\), and for \\(\\pi_0=0.8\\) we have \\(p=1-0.9999 = 7.79\\times{10^{-5}}\\). For exactly the same sequence of observations, we have two different p-values (for each value of \\(\\pi_0\\)) depending on what our plan was. We’ve done one-sided p-values (boo!) for simplicity, but if we included the more extreme values in the other direction we’d still have this issue. Notice also that to calculate the p-value (or any other subsequent things like confidence intervals) we had to complete the trial as planned, since only then can we specify all the ‘more extreme’ outcomes. 11.2.0.1 The Bayesian approach Instead of specifying a null value for \\(\\pi\\), we specify a prior distribution. Since we have no context, and therefore no idea what \\(\\pi\\) could be, we’ll set \\[\\pi \\sim \\operatorname{Unif}\\,\\left[0,\\,1\\right], \\] so \\[ p\\left(\\pi\\right) = \\begin{cases} 1 &amp; \\pi \\in \\left[0,1\\right]\\\\ 0 &amp; \\text{otherwise.} \\end{cases} \\] We also need to specify a likelihood, to say what process is generating the data. It seems pretty reasonable to say that \\[\\begin{align*} X\\mid\\pi &amp; \\sim \\operatorname{Binom}\\left(n,\\pi\\right)\\\\ p\\left(X\\mid{\\pi}\\right) &amp; = \\binom{n}{X} \\pi^X \\left(1-\\pi\\right)^{n-X} \\end{align*}\\] where \\(n\\) is the number of observations made, and \\(\\pi\\) is the unknown probability parameter. Since the prior is a constant, this means our posterior is \\[\\begin{align*} p\\left(\\pi\\mid x\\right) &amp; \\propto \\binom{n}{X} \\pi^X \\left(1-\\pi\\right)^{n-X}\\\\ &amp;\\propto \\pi^X \\left(1-\\pi\\right)^{n-X}, \\end{align*}\\] which is a Beta distribution with parameters \\(\\alpha=X+1,\\,\\beta=n-X+1\\). So, we can say that after our observations 0,0,1,0,0,1,0,0,0,1 we have \\(n=10,\\,X=3\\) and therefore \\(\\alpha=4,\\,\\beta=8\\), which has the shape in Figure 11.2. Figure 11.2: The posterior distribution after all 10 observations However we aren’t limited to this: we can update the posterior after any number of observations. For example, Figure 11.3 shows the posterior after each of the observations in the sequence we’ve listed them. Figure 11.3: Posterior distribution after each observation in sequence, starting with a uniform prior for pi If we were to carry on to 100, 500 and 1000 observations, still with a proportion of 0.3, the posterior variance would keep decreasing, as shown in Figure 11.4. Figure 11.4: Posterior distribution after 100, 500 and 1000 observations (mainting the sample estimate p=0.3) In a clinical trial setting, the parameter in question would usually be the treatment effect \\(\\tau\\), whatever form that takes. The important point is that after any number of observations, we can make probabilistic statements about \\(\\tau\\) (or any other parameter). This fact is often used to guide the trial process (this is described in a lot more detail in the articles mentioned earlier). 11.3 How do we choose a prior? Having established that there are some big advantages to following the Bayesian route, why doesn’t everyone do that?! One of the biggest barriers is that the choice of the prior distribution is inherently (and obviously) subjective: it represents the current understanding about the parameter(s) of interest (excluding any information from the trial), and different people could have different information. The information used to construct the prior could come from various sources: previous related trials, historical databases, animal data etc., but each of these will need some interpretation before it can be applied to the new trial. This subjectivity can be off-putting to people who are used to considering science and statistics to be objective. NOTE: There is a lot of hidden subjectivity in the frequentist approach too (choice of hypotheses, choice of model etc.). Berger and Berry (1988) is an excellent (and fairly short) article if you want to consider this more. All this means that any prior is inherent to one person (or group of people) and might not be acceptable to anyone else (eg. a regulating body). Whereas in the freqentist approach there are accepted ways of doing things (so that the subjectivity is generally hidden), there is no accepted ‘correct’ prior. The usual solution to this is to use a selection or ‘community’ of prior distributions. Four of the main types (outlined by Spiegelhalter, Freedman, and Parmar (1994)) are…. A reference prior The idea of the reference prior is to contribute as little information as possible - you can read about how they are derived in Berger, Bernardo, and Sun (2009). Sometimes the formulation leads to the prior being improper, meaning it doesn’t integrate to one. Although this all sounds quite good (apart from the improper bit), you could argue that these are the least realistic prior. Very extreme values of the treatment effect will be [nearly] equally weighted to ones that are realistic, and this almost certainly doesn’t reflect anyone’s actual beliefs. ‘Proper’ Bayesians look down on these priors. A clinical prior This type of trial is intended to formally represent the opinion of well-informed individuals, often the trial clinicians themselves. Deriving this type of trial therefore involves a thorough elicitation process, where many specific questions are asked to pin down people’s beliefs, and various sources of information can be considered. Figure 11.5 (taken from Spiegelhalter, Freedman, and Parmar (1994)) shows results of prior elicitation with nine clinicians for a trial of CHART, a cancer treatment. Figure 11.5: A table showing the results of prior elicitation with 9 clinicians. Spiegelhalter, Abrams, and Myles (2004) has a lot of detail about prior elicitation for clinical trials, so this is well worth a read. A sceptical prior A sceptical prior is designed to represent the beliefs of someone who doesn’t think the treatment will work, but is theoretically able to be brought round by sufficient data (a reasonable sceptic). A sceptical prior is often symmetric around a treatment effect of zero, with the variance chosen such that \\(p\\left(\\tau&gt;\\tau_S\\right) = \\gamma\\) (assuming \\(\\tau_S&gt;0\\)), for some small value \\(\\gamma\\). Sceptical priors are probably the most popular with regulatory bodies, since they have both consistency and caution. An optimistic / enthusiastic prior An enthusiastic prior is similar to the sceptical prior, but is this time centred on the alternative hypothesis value (usually \\(\\tau_S\\)), with \\(p\\left(\\tau&lt;0\\right) = \\gamma\\) (remember in this discussion we’re assuming \\(\\tau&gt;0\\) is good), for some small value \\(\\gamma\\). The philosophical argument for this prior is that it encourages conservatism in the face of early negative results. The more data are observed, the less effect the form of the prior will have on the posterior, but for small trials, or for the early stages of a large trial, the prior distribution is very influential. Example 11.1 To demonstrate the Bayesian approach to a simple RCT we’ll revisit the Captopril example we used in Chapter 4, starting with Example 4.1, and will apply the very simplest Bayesian model in Spiegelhalter, Freedman, and Parmar (1994). The dataset summarises a trial of 16 diabetes patients, and focusses on a drug (Captopril) that may reduce blood pressure. The primary outcome of the trial is blood pressure (measured in mmHg) and for the drug to be effective we need to see a decrease in blood pressure in the treatment group, compared to the control (so here, \\(\\tau&lt;0\\) is good). The aim of the trial is to learn about the treatment effect \\(\\tau\\), with \\(\\tau = \\mu_T - \\mu_C\\), the difference between outcome blood pressure for the two groups. After \\(m\\) observations we will have an observed value \\(\\bar{x}_{diff} = \\bar{x}_T - \\bar{x}_C\\). Our likelihood for this will be \\[ \\bar{x}_{diff}\\mid{\\tau} \\sim N\\left(\\tau,\\,\\frac{\\sigma^2}{m}\\right).\\] In this example we consider \\(\\sigma^2\\) to be known, and we’ll estimate it using the idea that individual responses are assumed to be normally distributed with variance \\(\\frac{\\sigma^2}{2}\\). From example 4.2 a reasonable value for \\(\\sigma\\) would be the pooled SD, 7.82 mmHg, so \\(\\sigma^2 = 61.1524\\). Our prior distribution will also be normal: \\[\\tau \\sim N\\left(\\tau_0,\\,\\frac{\\sigma^2}{n_0}\\right),\\] where \\(\\tau_0\\) is the prior mean, and this prior is equivalent to a normalised likelihood arising from a trial of \\(n_0\\) patients and an observed value of \\(\\tau_0\\). Finally, we combine these to find the posterior. Because of the simple way we have set things up, the posterior is also normal: \\[\\begin{equation} \\tau\\mid{\\bar{x}_{diff}} \\sim N\\left(\\frac{n_0\\tau_0 + m\\bar{x}_{diff}}{n_0 + m},\\,\\frac{\\sigma^2}{n_0+m}\\right). \\tag{11.1} \\end{equation}\\] As discussed above, we will use a community of priors. The reference prior in this case is the limit when \\(n_0 \\rightarrow 0\\). Clearly this makes the prior improper (essentially a uniform distribution over the range of \\(\\tau\\)), but the resulting posterior is proper. The original paper Hommel et al. (1986) doesn’t give a minimum detectable effect size (there is no sample size calculation since they use every available patient who fits the criteria), so after a bit of googling we will say that \\(\\tau_S=-5\\)mmHg, so if \\(\\tau&lt;-5\\)mmHG then that makes the treatment clinically superior. This allows us to define the sceptical and enthusiastic priors. For the sceptical prior \\(p^s_0\\left(\\tau\\right)\\), we will have \\(\\tau_0=0\\), representing the expectation that the treatment will be ineffective. One approach is to choose \\(n_0\\) such that \\(p^s_0\\left(\\tau&lt;-5\\right) = \\gamma\\), where \\(\\gamma\\) is some small number; we will set \\(\\gamma=0.1\\). Using \\[\\Phi\\left(\\frac{5}{\\sigma/\\sqrt{n_0}}\\right) = 0.9 = \\Phi\\left(\\underbrace{1.28}_{\\texttt{qnorm(0.9)}} \\right)\\] we find \\(n_0 = 4.017\\), so the sceptical prior is \\[\\tau \\sim N\\left(0, \\frac{\\sigma^2}{4.017}\\right).\\] An enthusiastic prior is centred around \\(\\tau_s\\) and should have probability \\(\\gamma\\) that \\(\\tau&gt;0\\), so we have the same variance (and hence same \\(n_0\\)), but the prior mean will be -5 mmHg. So, the enthusiastic prior is \\[\\tau \\sim N\\left(-5, \\frac{\\sigma^2}{4.017}\\right).\\] This means we have three posteriors, one for each prior, given by Equation (11.1) and shown in Figure 11.6. Figure 11.6: Priors (dashed lines) and posteriors (solid lines) for the Captopril data, for three different prior distributions (shown by legend). We see that Despite the different priors, even with this small dataset the posteriors are quite close. With any of the posteriors, we are able to make probabilistic statements about the value of \\(\\tau\\). 11.4 More complex trials Another big advantage of the Bayesian approach is that it can be used to model much more complicated trials, and we have great flexibility of trial design because we can make use of conditional probability to specify our model. Even if the posterior distribution is analytically intractable, we can make use of sampling schemes like MCMC to sample from it, so that we can understand its shape very well. If you’ve taken Bayesian Computational Modelling III this will probably fall into place more clearly than if you haven’t. Here we will mostly demonstrate this through an example. Example 11.2 This example is about a study I’m involved in at the moment, sponsored by the College of Policing. It’s not actually a randomized trial, because it wasn’t considered ethical or practical to randomize. Instead, all victims within the participating police regions are in the treatment group, and a large number of similar cases from other police areas are used as controls. The trial is about a forensic marking device called SmartWater, and about whether it improves outcomes in domestic violence situations where the perpetrator has been ordered to stay away from the victim. Specifically: Does it deter perpetrators from reoffending? Does it increase the proportion of reoffenders who are brought to justice? Victims are given forensic marking kits, containing a gel with which to mark parts of their property (door handles, gates etc.) and a spray canister that they can use to mark the perpetrator from a distance, should they appear nearby. Perpetrators have been informed that forensic equipment was now being used to protect their ex-partner. SmartWater has been used for forensic protection of shops and other premises for a number of years (see headline above), but it’s only very recently that it’s been deployed to protect victims of domestic violence (mostly in West and South Yorkshre) - you can read about it here. In our trial, several police areas (West Yorkshire, Hampshire, Durham and Staffordshire) will between them deploy several hundred forensic marking kits to victims of domestic violence for whom the perpetrator has a police order to stay away (eg. a restraining order). This group forms the treatment group. A larger group will be identified from other police regions, matched for various characteristics. For each participant, the output measures are all binary variables: \\(X^{CO}\\): Whether or not there were any police call outs (crimed or non-crimed incidents) in the follow-up period \\(X^{Arr}\\): Whether or not there were any arrests in the follow-up period \\(X^{Ch}\\): Whether or not there were any charges in the follow-up period \\(X^{Conv}\\): Whether or not there were any convictions These form a nested structure: if there were no police call outs, there definitely won’t have been any arrests. Similarly, if there were no arrests, there will have been no charges, and so on. We will model each of the outcomes as being Bernoulli(\\(\\pi^{Ev}\\)) where \\(\\pi^{Ev}\\) is the probability of the event in question (call-out, arrest etc.). We might reasonably expect that these probabilities depend on a number of characteristics, and in the real analysis we’ll definitely include some, but here the only one we’ll consider is \\(G_i\\), whether the participant was in the treatment group (and had been given the forensic marking equipment) or not. Because the \\(\\pi^{Ev}\\) are on the probability scale, we’ll use the logit transform, so that \\[ \\pi^{Ev} = \\begin{cases} \\operatorname{logit}^{-1}\\left( \\alpha^{Ev} + \\beta^{Ev}G_i\\right) &amp; \\text{ if }X^{Ev-1}=1\\\\ 0 &amp; \\text{ if } X^{Ev-1}=0 \\end{cases} \\] for some parameters \\(\\alpha^{Ev},\\,\\beta^{Ev}\\), where \\(X^{Ev-1}\\) is the event from the ‘level up’. So, for example, if \\(X^{CO}=0\\) then \\(\\pi^{Arr}=0\\). Therefore the structure of the model is as shown in Figure 11.7. Figure 11.7: The structure of the model for this trial. The study is ongoing, so here we’ll do a simulation study. This uses Stan (Stan Development Team (2024)), which you’ll have used too if you did Bayesian Computational modelling, but don’t worry if you haven’t seen it before. We’ll focus on the first and last levels of the model: call-outs and convictions. This is partly for simplicity, and partly because they’re the incidents for which I have been given ball-park estimates of probabilities in the general population. So, our model is \\[\\begin{align*} X_i^{CO}\\mid \\alpha^{CO},\\,\\beta^{CO} &amp; \\sim \\operatorname{Bernoulli}\\left(\\pi_i^{CO}\\right)\\\\ \\pi^{CO}_i&amp; = \\operatorname{logit}^{-1}\\left(\\alpha^{CO} + \\beta^{CO} G_i\\right)\\\\ X_i^{conv}\\mid X_i^{CO},\\,\\alpha^{conv}, \\, \\beta^{conv} &amp; \\sim \\operatorname{Bernoulli}\\left(\\pi_i^{conv}\\right)\\\\ \\pi^{conv}_i&amp; = \\begin{cases} \\operatorname{logit}^{-1}\\left(\\alpha^{conv} + \\beta^{conv} G_i\\right)&amp;\\text{if }x^{CO}_i=1\\\\ 0 &amp; \\text{otherwise} \\end{cases} \\end{align*}\\] We can also specify prior distributions for \\(\\alpha^{CO},\\,\\beta^{CO},\\,\\alpha^{conv},\\,\\beta^{conv}\\), although if we don’t then Stan will assume a non-informative prior. Natural choices for informative priors would be normal distributions, since theoretically these parameters can take any value in \\(\\mathbb{R}\\), but in this example we’ll stick with the default non-informative Stan priors. For the simulation study, we’ll fix \\(\\alpha^{CO},\\beta^{CO}, \\alpha^{conv}\\) and \\(\\beta^{conv}\\) and simulate from the model described above to produce a group of participants. First, with any Stan model it’s sensible to simulate a large dataset, to check that the model can learn the parameters correctly. Figure 11.8 shows the results with 8000 participants (4000 in each group), where in the simulation I specified \\(\\alpha^{CO}=-0.9,\\,\\beta^{CO}=-1,\\,\\alpha^{conv}=-2,\\,\\beta^{conv}=1\\). The \\(\\alpha\\) values lead to fairly realistic values of \\(\\pi^{CO} = 0.289\\) and \\(\\pi^{conv} = 0.119\\). The \\(\\beta\\) values are chosen to give some treatment effect - we’ll experiment with the values a bit later. Notice that we want \\(\\beta^{CO}&lt;0\\) (fewer callouts resulting from the deterrent effect) and \\(\\beta^{conv}&gt;0\\) (more convictions for those who did have a callout). Figure 11.8: Posterior credible intervals for each parameter with 8000 participants. Now that we’ve established the model is doing what it’s meant to, we can experiment with some more realistic scenarios. There are around 400 people in the intervention group, so let’s see what sorts of treatment differences we can detect with that number of participants. We’ll keep the values of the parameters in the simulation the same for now. Figure 11.9 shows the posterior densities for the four parameters. We see that the true values are well within the distributions for each parameter. We also see that the two \\(\\beta\\) distributions are well away from 0, indicating that with these numbers, we can be confident there is a deterrant effect and an increased conviction effect. Figure 11.9: Posterior densities with 800 participants These values aren’t especially meaningful to a clinician (or in this case, a criminologist) so a helpful things is to use the posterior samples of the \\(\\alpha^{Ev},\\,\\beta^{Ev}\\) to find the posterior samples of \\(\\pi^{Ev}\\) (for cases where \\(X^{Ev-1}=1\\)). Figure 11.10 shows the posterior densities of the probabilities of callouts and convictions for the two groups. Figure 11.10: Posterior densities for the probabilities of callout and arrest, with 800 participants. We could use the posterior samples to calculate the probabilities for various ranges, for example \\(\\beta^{CO}&lt;0\\) or \\(\\beta^{conv}&gt;0\\), to help inform decision making. One last thing we’ll try is to reduce the treatment effect, to see what the effect is on the posterior densities. We’ll still have 800 participants, and \\(\\alpha^{CO}=-0.9,\\,\\alpha^{conv}=-2\\) but we’ll change to \\(\\beta^{CO}=-0.5,\\,\\,\\beta^{conv}=0.5\\). The parameter posteriors samples are shown in Figure 11.11, and the distributions of the probabilities in Figure 11.12 Figure 11.11: Posterior densities with 800 participants, smaller treatment effect. Figure 11.12: Posterior densities for the probabilities of callout and arrest, with 800 participants, smaller treatment effects. Conclusion: Bayesian analysis for clinical trials is powerful and flexible: It gives us a posterior distribution that enables us to make probabilistic statements about the parameters of interest It allows us to model complex trials References Berger, James O, José M Bernardo, and Dongchu Sun. 2009. “The Formal Definition of Reference Priors.” Berger, James O, and Donald A Berry. 1988. “Statistical Analysis and the Illusion of Objectivity.” American Scientist 76 (2): 159–65. Berry, Donald A. 2006. “Bayesian Clinical Trials.” Nature Reviews Drug Discovery 5 (1): 27–36. Hommel, EHEBMJ, Hans-Henrik Parving, Elisabeth Mathiesen, Berit Edsberg, M Damkjaer Nielsen, and Jørn Giese. 1986. “Effect of Captopril on Kidney Function in Insulin-Dependent Diabetic Patients with Nephropathy.” Br Med J (Clin Res Ed) 293 (6545): 467–70. Spiegelhalter, David J, Keith R Abrams, and Jonathan P Myles. 2004. Bayesian Approaches to Clinical Trials and Health-Care Evaluation. Vol. 13. John Wiley &amp; Sons. Spiegelhalter, David J, Laurence S Freedman, and Mahesh KB Parmar. 1994. “Bayesian Approaches to Randomized Trials.” Journal of the Royal Statistical Society: Series A (Statistics in Society) 157 (3): 357–87. Stan Development Team. 2024. “RStan: The R Interface to Stan.” https://mc-stan.org/. "],["cp1allocation.html", "A Computer Practical 1 - Allocation A.1 R practicalities A.2 Allocation", " A Computer Practical 1 - Allocation WARNING This practical is designed to help you investigate the behaviour of the various allocation methods we’ve been looking at. There are lots of things we do here that you couldn’t do in a real trial. For example, you wouldn’t be able to plot the baseline covariates, since you wouldn’t have them all at the start of the allocation process. You also wouldn’t be able to re-do the allocation lots of times, because in most cases the participants join the trial (and are allocated) sequentially. You will need a lot of the techniques covered in this practical for your summative assignments, so consider it a sort of informal formative assignment to finish it if you don’t in class. Or, at the very least, you might need to return to it while working on your summative assignments. There will be a mixture of full solutions, examples of possible solutions and example code to adapt. If you’re not sure how to do something, please ask! In the example code, I have used the same names for most objects. In order to store your results and avoid confusion, it will be sensible to name things intelligently! For example, suffix each allocation data frame so that you know which allocation method you used. Create an R file with the code you use, so that you can easily replicate your work. A.1 R practicalities There are many, many packages in R that implement methods for designing and analysing clinical trials (see a list at CRAN task view). We will look at some of these, and will also write our own code for some tasks. Remember that to install a package, you can do install.packages(&quot;&lt;packagename&gt;&quot;) If you have problems running R on your laptop, or on the university machines, the most foolproof way might be to use Github codespaces (thanks to Louis Aslett, who developed this for Data Science and Statistical Computing II). You may be familiar with this approach if you did Bayesian Computational Modelling III. An advantage of this is that you can open the same codespace (the same instance of R) from any computer, so if you plan to work on things (for example your summative assignment, which will involve some R) from more than one computer, this might be ideal. This requires you to have a github account (you can sign up for free here) and there is a short guide to creating a github account here. Direct link to codespace Instructions for how to use codespace A.2 Allocation We’ll make use of several packages in this section. Installing them all now should hopefully prevent it from disrupting your flow! We’ll load them as we go along. Notice that where there are code snippets like this one that you want to copy directly, you can hover your cursor in the top right of the code box and a ‘copy’ icon will appear. install.packages(c(&quot;medicaldata&quot;, &quot;ggplot2&quot;, &quot;gtsummary&quot;, &quot;Minirand&quot;, &quot;blockrand&quot;, &quot;dplyr&quot;, &quot;randomizeR&quot;)) A.2.1 Licorice gargle dataset To work with allocation, we will use the licorice_gargle dataset from the package medicaldata, which you can find by library(medicaldata) data(&quot;licorice_gargle&quot;) You can find out about the dataset by looking at the help file ?licorice_gargle and at this website, and if you’re feeling really keen you can even read the original paper: Ruetzler et al. (2013) (but please don’t do any of that now!). Exercise A.1 Of the 19 columns, how many are baseline characteristics that we could use in the allocation? Click for solution Solution. In this dataset the baseline characteristics are helpfully prefixed by preOp, so there are seven baseline characteristics. ## [1] &quot;preOp_gender&quot; &quot;preOp_asa&quot; &quot;preOp_calcBMI&quot; &quot;preOp_age&quot; &quot;preOp_mallampati&quot; ## [6] &quot;preOp_smoking&quot; &quot;preOp_pain&quot; In order to be able to work with the dataset, we need to convert several of the columns to factor variables (you can see by looking at the structure str(licorice_gargle) that all columns are numeric to begin with), so run the following code to convert the necessary columns to factors, and to restrict the dataset to only the columns we’re interested in. lic_garg = licorice_gargle[ ,1:8] # vector of names of columns to be coerced to factor cols &lt;- c(&quot;preOp_gender&quot;, &quot;preOp_asa&quot;, &quot;preOp_mallampati&quot;, &quot;preOp_smoking&quot;, &quot;preOp_pain&quot;, &quot;treat&quot;) # convert each of those columns to factors lic_garg[cols] &lt;- lapply(lic_garg[cols], factor) # Check the result: str(lic_garg) ## &#39;data.frame&#39;: 235 obs. of 8 variables: ## $ preOp_gender : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ preOp_asa : Factor w/ 3 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;: 3 2 2 2 1 2 3 2 3 3 ... ## $ preOp_calcBMI : num 33 23.7 26.8 28.4 30.4 ... ## $ preOp_age : num 67 76 58 59 73 61 66 61 83 69 ... ## $ preOp_mallampati: Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 2 2 2 2 1 3 1 2 1 2 ... ## $ preOp_smoking : Factor w/ 3 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;: 1 2 1 1 2 1 1 1 1 3 ... ## $ preOp_pain : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ treat : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 2 2 2 2 2 2 2 2 2 ... A.2.1.1 Summary tables For every allocation, we’ll want to see a summary table of how the patients are distributed between the groups. In this practical we’ll use the function tbl_summary from gtsummary, but there are plenty of other ways to create nice tables around clinical trials (for example the packages table1 and atable). Notice that the licorice gargle data contains the allocation used in the trial, so we can create a table summarising the participants in each group. tbl_summary(lic_garg, by = &quot;treat&quot;) #yoxtghywvn table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #yoxtghywvn thead, #yoxtghywvn tbody, #yoxtghywvn tfoot, #yoxtghywvn tr, #yoxtghywvn td, #yoxtghywvn th { border-style: none; } #yoxtghywvn p { margin: 0; padding: 0; } #yoxtghywvn .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #yoxtghywvn .gt_caption { padding-top: 4px; padding-bottom: 4px; } #yoxtghywvn .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #yoxtghywvn .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #yoxtghywvn .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #yoxtghywvn .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #yoxtghywvn .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #yoxtghywvn .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #yoxtghywvn .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #yoxtghywvn .gt_column_spanner_outer:first-child { padding-left: 0; } #yoxtghywvn .gt_column_spanner_outer:last-child { padding-right: 0; } #yoxtghywvn .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #yoxtghywvn .gt_spanner_row { border-bottom-style: hidden; } #yoxtghywvn .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #yoxtghywvn .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #yoxtghywvn .gt_from_md > :first-child { margin-top: 0; } #yoxtghywvn .gt_from_md > :last-child { margin-bottom: 0; } #yoxtghywvn .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #yoxtghywvn .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #yoxtghywvn .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #yoxtghywvn .gt_row_group_first td { border-top-width: 2px; } #yoxtghywvn .gt_row_group_first th { border-top-width: 2px; } #yoxtghywvn .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #yoxtghywvn .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #yoxtghywvn .gt_first_summary_row.thick { border-top-width: 2px; } #yoxtghywvn .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #yoxtghywvn .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #yoxtghywvn .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #yoxtghywvn .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #yoxtghywvn .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #yoxtghywvn .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #yoxtghywvn .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #yoxtghywvn .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #yoxtghywvn .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #yoxtghywvn .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #yoxtghywvn .gt_left { text-align: left; } #yoxtghywvn .gt_center { text-align: center; } #yoxtghywvn .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #yoxtghywvn .gt_font_normal { font-weight: normal; } #yoxtghywvn .gt_font_bold { font-weight: bold; } #yoxtghywvn .gt_font_italic { font-style: italic; } #yoxtghywvn .gt_super { font-size: 65%; } #yoxtghywvn .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #yoxtghywvn .gt_asterisk { font-size: 100%; vertical-align: 0; } #yoxtghywvn .gt_indent_1 { text-indent: 5px; } #yoxtghywvn .gt_indent_2 { text-indent: 10px; } #yoxtghywvn .gt_indent_3 { text-indent: 15px; } #yoxtghywvn .gt_indent_4 { text-indent: 20px; } #yoxtghywvn .gt_indent_5 { text-indent: 25px; } #yoxtghywvn .katex-display { display: inline-flex !important; margin-bottom: 0.75em !important; } #yoxtghywvn div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after { height: 0px !important; } Characteristic 0 N = 1171 1 N = 1181 preOp_gender     0 73 (62%) 69 (58%)     1 44 (38%) 49 (42%) preOp_asa     1 19 (16%) 22 (19%)     2 67 (57%) 67 (57%)     3 31 (26%) 29 (25%) preOp_calcBMI 26.1 (22.4, 28.1) 25.7 (22.7, 28.6) preOp_age 63 (45, 68) 61 (48, 68) preOp_mallampati     1 31 (26%) 39 (33%)     2 69 (59%) 66 (56%)     3 16 (14%) 13 (11%)     4 1 (0.9%) 0 (0%) preOp_smoking     1 45 (38%) 45 (38%)     2 36 (31%) 36 (31%)     3 36 (31%) 37 (31%) preOp_pain     0 115 (98%) 118 (100%)     1 2 (1.7%) 0 (0%) 1 n (%); Median (Q1, Q3) Notice that we can also save this as an object and access the individual sub-tables, for example rb_tab = randbalance( trt = lic_garg$treat, covmat = lic_garg[,-8], ntrt=2, trtseq = c(&quot;0&quot;, &quot;1&quot;)) rb_tab$preOp_gender ## ## trt 0 1 ## 0 73 44 ## 1 69 49 There are also packages with functions to output these demographic tables formatted for use in latex documents, for example atable. A.2.1.2 Binning continuous variables Before we can use any of our allocation methods, we’re going to need to bin the two numeric variables preOp_age and preOp_calcBMI. In a real trial, we’d have to decide on the bins before seeing the data. The sort of information we’d be likely to get pre-trial (the point at which we need to decide on the bins) would be The study was open to all adults (aged 18+), but preOp_age was likely to centre around aged 60. preOp_BMI was expected to have mean 26 and standard deviation around 4.5. Exercise A.2 Using the informationn above, bin the two variables preOp_age and preOp_calcBMI to convert them into factor variables BMI and age. Investigate how the bins you have chosen split the actual data - are you pleased with your choices? In the solutions we’ll create a new data frame lg_df with the two new factor variables instead of the original two numeric variables. It might be helpful to you if you make sure your objects and columns are named the same (or else you may need to alter code further into the practical) Click for solution Solution. These solutions show you one way to create such factor variables, but if you choose different bins that’s fine! Based on the information given, one reasonable split for preOp_age would be \\(&lt;50,\\;50-70\\) and \\(&gt;70\\), in which case we could do lic_garg$age[lic_garg$preOp_age &lt; 50] &lt;- &quot;Under 50&quot; lic_garg$age[lic_garg$preOp_age &gt;= 50 &amp; lic_garg$preOp_age &lt; 70] &lt;- &quot;50 to 70&quot; lic_garg$age[lic_garg$preOp_age &gt;= 70] &lt;- &quot;70 plus&quot; lic_garg$age = factor(lic_garg$age, levels = c(&quot;Under 50&quot;, &quot;50 to 70&quot;, &quot;70 plus&quot;)) We can see how the data fall into these categories: ## ## Under 50 50 to 70 70 plus ## 67 122 46 These aren’t equal but we have a decent number in each group. With the BMI measurement, we know that there are predefined boundaries between categories, so it might be sensible to use those. The boundary between ‘medium’ and ‘high’ is usually given as 25, so since that is near our given mean we could use that as the boundary. lic_garg$BMI[lic_garg$preOp_calcBMI &lt; 25] &lt;- &quot;medium_or_low&quot; lic_garg$BMI[lic_garg$preOp_calcBMI &gt;= 25] &lt;- &quot;high&quot; lic_garg$BMI = factor(lic_garg$BMI, levels = c(&quot;medium_or_low&quot;, &quot;high&quot;)) Again, we can see how the participants fall into these bins ## ## medium_or_low high ## 99 136 and the split is fairly even. Finally, we can select the columns we want from lic_garg to create lg_df lg_df = lic_garg[,c(1,2,5,6,7,9,10,8)] If you’ve chosen different bins, that’s fine! But it will help you if the column names and data frame names are the same as on this page. A.2.2 Measures of imbalance It will be useful to have a simple numerical summary of how imbalanced an allocation is. First, we will use the imbalance measure defined in lectures, \\[D(n) = \\lvert N_T\\left(n\\right) - N_C\\left(n\\right) \\rvert \\] This is pretty simple to code in R: imbalance = function( df, # participant data frame with allocation column included alloc # name of allocation column ){ alloc_vec = as.factor(df[ ,names(df)==alloc]) alloc_lev = levels(alloc_vec) # how the treatment groups are coded n1 = nrow(df[df[alloc]==alloc_lev[1],]) n2 = nrow(df[df[alloc]==alloc_lev[2],]) abs(n1-n2) } Exercise A.3 Use the imbalance function above to find the imbalance in the allocation recorded in the lic_garg dataset. Do you find this helpful as a summary of the allocation? Click for solution Solution. imbalance(df = lic_garg, alloc = &quot;treat&quot;) ## [1] 1 All this tells us is the difference in overall numbers. This is a tiny bit useful, but it would be nice to have some information about how the allocation balances participants with respect to their characteristics - this would give a better understanding of how similar the two groups are to one another. To see how well balanced our allocations are in terms of each covariate, we will define a function summing the marginal imbalance marg_imbalance = function( df, # participant data frame, including allocation and all factor variables alloc, # name of allocation column factors # names of prognostic factors to be included ){ df = as.data.frame(df) # deals with tibbles n_fact = length(factors) # the numbers of factors imb_sum=0 # a running total of imbalance for (i in 1:n_fact){ # loop through the factors ind_i = (1:ncol(df))[names(df)==factors[i]] col_i = as.factor(df[ ,ind_i]) levels_i = levels(col_i) nlevels_i = length(levels_i) for (j in 1:nlevels_i){ # loop through the levels of factor i # df_ij contains just those entries with level j of factor i df_ij = df[df[ ,ind_i]==levels_i[j] , ] imb_ij = imbalance(df=df_ij, alloc=alloc) # find the imbalance for the sub-data-frame in which factor i has level j imb_sum = imb_sum + imb_ij } } imb_sum } For example, to find the marginal imbalance over the gender and age factors, use marg_imbalance(df=lg_df, alloc=&quot;treat&quot;, factors = c(&quot;preOp_gender&quot;, &quot;age&quot;)) ## [1] 20 Note that the larger the total number of factor levels, the larger the marginal imbalance will be, so if you’re comparing between methods, make sure you’re including all the same factors! A.2.3 Allocation methods To mimic the way that participants are recruited sequentially in a trial (which is generally the case), the code for each type of allocation will work its way through the participant data frame, even though this might not be the most efficient way to produce the end result. For the more complex methods we’ll use packages. Feel free to write your own code for the simpler methods if you want to! We’ll start by performing each method once with the whole dataset, and then go on to include baseline variables, and finally we’ll perform a simulation study. A.2.3.1 Simple random allocation In simple random allocation, each participant is allocated to one of the two trial arms with equal probability. srs = function( df, # DF should be the participant data frame. # A column &#39;treat&#39; will be added levels = c(&quot;0&quot;, &quot;1&quot;) # Levels of treat factor ){ n = nrow(df) # number of rows / participants # Create a new column &#39;treat&#39; df$treat = rep(NA, n) # work through the rows, randomly allocating patients with probably 1/2 for (i in 1:n){ df$treat[i] = sample(levels, size=1, prob = c(0.5, 0.5)) } df$treat = as.factor(df$treat) df } Exercise A.4 Use the function srs above to allocate the patients in the licorice_gargle dataset to groups T or C. Generate the balance table and imbalance and comment on them. Click for solution Solution. To allocate the patients, use lg_srs = srs( df = lg_df[,-8], levels = c(&quot;T&quot;, &quot;C&quot;) ) And to display the balance table tbl_summary(lg_srs, by = &quot;treat&quot;) #lpsqmhgtfc table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #lpsqmhgtfc thead, #lpsqmhgtfc tbody, #lpsqmhgtfc tfoot, #lpsqmhgtfc tr, #lpsqmhgtfc td, #lpsqmhgtfc th { border-style: none; } #lpsqmhgtfc p { margin: 0; padding: 0; } #lpsqmhgtfc .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #lpsqmhgtfc .gt_caption { padding-top: 4px; padding-bottom: 4px; } #lpsqmhgtfc .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #lpsqmhgtfc .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #lpsqmhgtfc .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #lpsqmhgtfc .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #lpsqmhgtfc .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #lpsqmhgtfc .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #lpsqmhgtfc .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #lpsqmhgtfc .gt_column_spanner_outer:first-child { padding-left: 0; } #lpsqmhgtfc .gt_column_spanner_outer:last-child { padding-right: 0; } #lpsqmhgtfc .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #lpsqmhgtfc .gt_spanner_row { border-bottom-style: hidden; } #lpsqmhgtfc .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #lpsqmhgtfc .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #lpsqmhgtfc .gt_from_md > :first-child { margin-top: 0; } #lpsqmhgtfc .gt_from_md > :last-child { margin-bottom: 0; } #lpsqmhgtfc .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #lpsqmhgtfc .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #lpsqmhgtfc .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #lpsqmhgtfc .gt_row_group_first td { border-top-width: 2px; } #lpsqmhgtfc .gt_row_group_first th { border-top-width: 2px; } #lpsqmhgtfc .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #lpsqmhgtfc .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #lpsqmhgtfc .gt_first_summary_row.thick { border-top-width: 2px; } #lpsqmhgtfc .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #lpsqmhgtfc .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #lpsqmhgtfc .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #lpsqmhgtfc .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #lpsqmhgtfc .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #lpsqmhgtfc .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #lpsqmhgtfc .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #lpsqmhgtfc .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #lpsqmhgtfc .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #lpsqmhgtfc .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #lpsqmhgtfc .gt_left { text-align: left; } #lpsqmhgtfc .gt_center { text-align: center; } #lpsqmhgtfc .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #lpsqmhgtfc .gt_font_normal { font-weight: normal; } #lpsqmhgtfc .gt_font_bold { font-weight: bold; } #lpsqmhgtfc .gt_font_italic { font-style: italic; } #lpsqmhgtfc .gt_super { font-size: 65%; } #lpsqmhgtfc .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #lpsqmhgtfc .gt_asterisk { font-size: 100%; vertical-align: 0; } #lpsqmhgtfc .gt_indent_1 { text-indent: 5px; } #lpsqmhgtfc .gt_indent_2 { text-indent: 10px; } #lpsqmhgtfc .gt_indent_3 { text-indent: 15px; } #lpsqmhgtfc .gt_indent_4 { text-indent: 20px; } #lpsqmhgtfc .gt_indent_5 { text-indent: 25px; } #lpsqmhgtfc .katex-display { display: inline-flex !important; margin-bottom: 0.75em !important; } #lpsqmhgtfc div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after { height: 0px !important; } Characteristic C N = 1111 T N = 1241 preOp_gender     0 65 (59%) 77 (62%)     1 46 (41%) 47 (38%) preOp_asa     1 19 (17%) 22 (18%)     2 68 (61%) 66 (53%)     3 24 (22%) 36 (29%) preOp_mallampati     1 32 (29%) 38 (31%)     2 67 (60%) 68 (55%)     3 11 (9.9%) 18 (15%)     4 1 (0.9%) 0 (0%) preOp_smoking     1 45 (41%) 45 (36%)     2 32 (29%) 40 (32%)     3 34 (31%) 39 (31%) preOp_pain     0 110 (99%) 123 (99%)     1 1 (0.9%) 1 (0.8%) age     Under 50 23 (21%) 44 (35%)     50 to 70 69 (62%) 53 (43%)     70 plus 19 (17%) 27 (22%) BMI     medium_or_low 46 (41%) 53 (43%)     high 65 (59%) 71 (57%) 1 n (%) To find the total imbalance, the command is imbalance(lg_srs, alloc = &quot;treat&quot;) ## [1] 13 and we can find the marginal imbalance using lg_factors_all = c(&quot;preOp_gender&quot;, &quot;preOp_asa&quot;, &quot;preOp_mallampati&quot;, &quot;preOp_smoking&quot;, &quot;preOp_pain&quot;, &quot;age&quot;, &quot;BMI&quot;) marg_imbalance(lg_srs, alloc = &quot;treat&quot;, factors = lg_factors_all) ## [1] 129 Notice that we’ve collected the names of the factors into the vector lg_factors_all. This saves time, and makes it easier to compare the same factors each time. Since srs uses the sample function, and we didn’t set a seed, you may well have got a different allocation (and therefore imbalance). A.2.3.2 Randomly permuted blocks We will do this with the package blockrand. library(blockrand) The function for generating RPB designs is also called blockrand. The default for the function blockrand is that it will randomly vary block length within \\(\\left\\lbrace 2,\\,4,\\,6,\\,8 \\right\\rbrace\\). Exercise A.5 Try playing around with the function blockrand, for example starting with blockrand(n=100) Can you generate an allocation for the licorice_gargle data (remember our data frame is now lg_df) and examine the balance? Click for solution Solution. There are 235 rows/participants in lg_df, and to keep consistency we would like the levels of our treatment to be \"T\" and \"C\". Therefore we can do rpb_lg = blockrand(n=235, levels = c(&quot;T&quot;, &quot;C&quot;)) Notice that this doesn’t have 235 rows: the blockrand function will always finish after a whole block. Let’s add this to our participant data to create a new data frame lg_rpb: # create the new data frame, a copy of lg_df lg_rpb = lg_df # Replace the original treat column with the RPB treatment column # Using only the first 235 allocations lg_rpb$treat = rpb_lg$treatment[1:235] Then we can generate the demographic table as before: tbl_summary(lg_rpb, by = &quot;treat&quot;) and calculate the imbalance ## [1] 1 and marginal imbalance ## [1] 113 The package blockrand contains a function plotblockrand, which outputs PDFs of randomization cards, ready to be printed and put into envelopes! A.2.3.3 Biased coin designs We can write code for a biased coin design by adapting the srs function above, setting \\(p=\\frac{2}{3}\\) by default as per Efron (1971). biased_coin = function( data, levels = c(&quot;T&quot;, &quot;C&quot;), p=2/3 ){ Dn = 0 # starting value of imbalance n = nrow(data) alloc = rep(NA, n) for (i in 1:n){ if (Dn==0){ # equally balanced alloc[i] = sample(levels, size=1, prob=c(0.5, 0.5) ) } else if(Dn&lt;0){ # More allocations to levels[2] up to this point alloc[i] = sample(levels, size=1, prob=c(p, 1-p) ) } else if(Dn&gt;0){ # More allocations to levels[1] up to this point alloc[i] = sample(levels, size=1, prob=c(1-p, p) ) } # Compute imbalance at this stage alloc_to_n = alloc[1:i] Dn = sum(alloc_to_n==levels[1]) - sum(alloc_to_n == levels[2]) } data$treat = as.factor(alloc) data } Exercise A.6 Use the function biased_coin above to allocate patients to the two groups. Produce the demographic table and calculate the imbalance. Try this for some different values of p. Click for solution Solution. To create a biased coin design with \\(p=0.9\\) (for example), we enter lg_bc1 = biased_coin( data=lg_df[,-8], p=0.9 ) and to produce a balance table: tbl_summary(lg_bc1, by = &quot;treat&quot;) Finally we can find the imbalance: imbalance(lg_bc1, alloc=&quot;treat&quot;) ## [1] 1 and marginal imbalance marg_imbalance(lg_bc1, alloc=&quot;treat&quot;, factors = lg_factors_all) ## [1] 93 A.2.3.4 Urn designs For the urn design, we will use the function udPar from the package randomizeR. We wrote this in lectures as \\(UD\\left(r,s\\right)\\), where \\(r\\) is the number of balls for each treatment group in the urn to begin with \\(s\\) is the number of balls added per treatment after each allocation Exercise A.7 Look at the help file for udPar. Which arguments correspond to \\(r\\) and \\(s\\) (as we called them in Section 3.2.3? The function udPar creates a model object, storing the parameters for the particular Urn design. To generate sequences of allocations, we use the function genSeq Exercise A.8 Create an urn design object using udPar with \\(r=3,\\;s=1\\), set to create enough allocations for the licorice gargle data. Use the function genSeq to generate a sequence of allocations. Click for solution Solution. library(randomizeR) ud_31 = udPar(235, 3, 1, c(&quot;0&quot;, &quot;1&quot;)) seq_ud31 = genSeq(ud_31) The allocation vector is stored as a row in the matrix M which is a slot in the genSeq object (part of the object). You can access it by seq_ud31$M[1,] ## [1] 0 0 0 1 1 1 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 0 1 1 1 0 1 1 1 0 0 0 1 0 0 0 1 0 0 0 1 ## [49] 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 1 0 1 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 0 0 1 1 1 1 0 1 0 1 0 0 0 ## [97] 1 1 0 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0 0 1 ## [145] 1 1 1 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 0 1 ## [193] 1 1 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 1 The argument r allows you to generate r sequences, in which case the matrix M has r rows. A.2.4 Stratifying the dataset The first way we thought about to account for baseline measurements was to use stratified sampling. For this, we split the dataset into a number of strata, within each of which the participants have the same levels of the stratifying factors. For example, in some hypothetical dataset one stratum might contain only women, aged 50-65, with a history of smoking. Once we have stratified the dataset, we can apply any of the methods above by simply treating the different strata as separate ‘mini-trials’. It’s not uncommon for some of the baseline data to be missing, in which case the following algorithms won’t work without some adjustment. Two possible approaches in that case would be to impute a ‘dummy’ value for missing data, chosen using some available information about likely values to not involve that covariate in the allocation method (especially if it’s thought not to be likely to be informative). An obvious problem with applying stratification to this dataset is the number of strata. We have (if you have the same numbers of levels for age and BMI as I do) \\[2\\times{3}\\times{4}\\times{3}\\times{2}\\times{3}\\times{2} = 864 \\] strata. Even by collecting levels together and so on, we are not going to get down to a sufficiently small number of strata. Therefore we will choose just a couple of covariates to stratify by. For example, gender and smoking status: library(dplyr) # Add an ID variable so that we can keep track of the order of participants lg_df$ID = 1:nrow(lg_df) # split the data frame according to levels of factors strat_gen_sm &lt;- lg_df %&gt;% group_split(preOp_gender, preOp_smoking) will create a list of data frames, one for each combination of preOp_gender and preOp_smoking. In this case there are six data frames, and for example the first (accessed by strat_gen_sm[[1]] contains all participants with preOp_gender=0 and preOp_smoking=1. You can choose different factors to stratify by if you want to! Exercise A.9 Split your group using the code above, choosing two factors to stratify by. How many participants are in each stratum? Click for solution Solution. We’ll stick with the group as above. To find the numbers of participants in each group we do group_sizes = sapply( 1:length(strat_gen_sm), function(i){ nrow(strat_gen_sm[[i]]) } ) group_sizes ## [1] 63 47 32 27 25 41 or if you prefer dplyr lg_df |&gt; group_by(preOp_gender, preOp_smoking) |&gt; summarise(count = n()) ## # A tibble: 6 × 3 ## # Groups: preOp_gender [2] ## preOp_gender preOp_smoking count ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ## 1 0 1 63 ## 2 0 2 47 ## 3 0 3 32 ## 4 1 1 27 ## 5 1 2 25 ## 6 1 3 41 Exercise A.10 Choose a couple of methods from Section A.2.3, and use them with your stratified dataset. Use the marg_imbalance function to find the marginal imbalance. Try this for just the factors you stratified by, and for other collections of factors. What do you expect to see? Click for solution Solution. The code for this will vary, but the basic idea is to work through the individual data sets individually, and apply the functions from Section A.2.3. One way to do this is by creating a for loop. The code below shows how this would work for simple random sampling, but you can change the function to use whichever method you prefer. # This command creates an empty list, which we will fill with allocation data frames as we go through alloc_list = list() # The loop works through the stratified data frames, applies SRS to allocate patients # and stores them in alloc_list for (i in 1:length(strat_gen_sm)){ alloc_list[[i]] = srs(strat_gen_sm[[i]]) } # bind all the data frames back together again alloc_full= dplyr::bind_rows(alloc_list) # re-order according to ID variable alloc_full[order(alloc_full$ID),] ## # A tibble: 235 × 9 ## preOp_gender preOp_asa preOp_mallampati preOp_smoking preOp_pain age BMI treat ID ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ## 1 0 3 2 1 0 50 to 70 high 0 1 ## 2 0 2 2 2 0 70 plus medium_or_low 0 2 ## 3 0 2 2 1 0 50 to 70 high 0 3 ## 4 0 2 2 1 0 50 to 70 high 1 4 ## 5 0 1 1 2 0 70 plus high 0 5 ## 6 0 2 3 1 0 50 to 70 high 1 6 ## 7 0 3 1 1 0 50 to 70 high 0 7 ## 8 0 2 2 1 0 50 to 70 high 1 8 ## 9 0 3 1 1 0 70 plus medium_or_low 0 9 ## 10 0 3 2 3 0 50 to 70 high 1 10 ## # ℹ 225 more rows It would be silly though to do this with SRS - why? Once you’ve performed the allocation, you can find the demographic tables, imbalance and marginal imbalance as before (with alloc_full, or whatever your is called, as the data frame) A.2.5 Minimisation If we want to try to achieve balance for all prognostic factors, minimisation is a more suitable method. The function Minirand in the package Minirand implements the minimisation algorithm. Much like we did in lectures (and like we would in a real trial), the function Minirand works from the point of view of having already allocated \\(j-1\\) particpants, and being presented with a \\(j^{th}\\). This example code is copied from the function’s help file, but with some extra comments to help you to follow it. It first creates a participant data frame, then allocates the particpants to treatment groups using Minimisation. ## Information about the treatment ntrt &lt;- 3 # There will three treatment groups trtseq &lt;- c(1, 2, 3) # the treatment groups are indexed 1, 2, 3 ratio &lt;- c(2, 2, 1) # the treatment groups will be allocated in a 2:2:1 ratio ## The next few rows generate the participant data frame nsample &lt;- 120 # we will have 120 participants c1 &lt;- sample(seq(1, 0), nsample, replace = TRUE, prob = c(0.4, 0.6)) c2 &lt;- sample(seq(1, 0), nsample, replace = TRUE, prob = c(0.3, 0.7)) c3 &lt;- sample(c(2, 1, 0), nsample, replace = TRUE, prob = c(0.33, 0.2, 0.5)) c4 &lt;- sample(seq(1, 0), nsample, replace = TRUE, prob = c(0.33, 0.67)) covmat &lt;- cbind(c1, c2, c3, c4) # generate the matrix of covariate factors for the subjects # label of the covariates colnames(covmat) = c(&quot;Gender&quot;, &quot;Age&quot;, &quot;Hypertension&quot;, &quot;Use of Antibiotics&quot;) covwt &lt;- c(1/4, 1/4, 1/4, 1/4) # equal weights/importance applied to each factor ## Applying the algorithm - start here if you already have participant data! res &lt;- rep(NA, nsample) # Generate a vector to store the results (the allocations) # generate treatment assignment for the 1st subject res[1] = sample(trtseq, 1, replace = TRUE, prob = ratio/sum(ratio)) # work through the remaining patients sequentially for (j in 2:nsample) { # get treatment assignment sequentially for all subjects # The vector res is updated and so all previous allocations are accounted for # covmat is the data frame of participant data res[j] &lt;- Minirand( covmat=covmat, j, covwt=covwt, ratio=ratio, ntrt=ntrt, trtseq=trtseq, method=&quot;Range&quot;, result=res, p = 0.9 ) } ## Store the allocation vector &#39;res&#39; as &#39;trt1&#39; trt1 &lt;- res # Display the number of randomized subjects at covariate factors balance1 &lt;- randbalance(trt1, covmat, ntrt, trtseq) balance1 ## $Gender ## ## trt 0 1 ## 1 31 17 ## 2 32 16 ## 3 16 8 ## ## $Age ## ## trt 0 1 ## 1 34 14 ## 2 34 14 ## 3 17 7 ## ## $Hypertension ## ## trt 0 1 2 ## 1 22 11 15 ## 2 23 11 14 ## 3 12 4 8 ## ## $`Use of Antibiotics` ## ## trt 0 1 ## 1 31 17 ## 2 31 17 ## 3 15 9 # Calculate the total imbalance of the allocation totimbal(trt = trt1, covmat = covmat, covwt = covwt, ratio = ratio, ntrt = ntrt, trtseq = trtseq, method = &quot;Range&quot;) ## [1] 1.375 Exercise A.11 Adapt the code above to apply the minimisation algorithm to the licorice gargle data. Investigate how balanced the data set is. Click for solution Solution. Notice in the code above that we only need to start about half way down if we already have a dataset (which we do). nsample = nrow(lg_df) res = rep(NA, nsample) res[1] = sample(c(0,1), 1, replace = TRUE, prob = c(0.5,0.5)) # work through the remaining patients sequentially for (j in 2:nsample){ # get treatment assignment sequentially for all subjects # The vector res is updated and so all previous allocations are accounted for # covmat is the data frame of participant data - including only the covariates res[j] &lt;- Minirand( covmat=lg_df[ ,1:7], j, covwt=rep(1,7)/7, ratio=c(1,1), ntrt=2, trtseq=c(0,1), method=&quot;Range&quot;, result=res, p = 0.9 ) } lg_df$treat = res ## You can now investigate the balance of the design as usual A.2.6 Extension: A simulation experiment! In the allocation section we used each method once and produced summaries like the marginal imbalance. Because they are inherently random, to understand the general behaviour of each method, we’ll conduct a simulation experiment. The general process for this is: Choose a numerical summary (probably either the imbalance or the marginal imbalance) Choose some large number \\(n_{sim}\\) and create a vector of zeroes or NAs that length Run through the following process \\(n_{sim}\\) times: Perform the allocation Find the numerical summary Store the numerical summary in the vector you created in step 2. Plot (or otherwise summarise) the vector of summaries. Exercise A.12 Perform the simulation experiment for some of the methods above. Make sure you store the summary vectors by different (and intelligible!) names. Based on your results, comment on how plausible it is that Ruetzler et al. (2013) used each method in their trial. Hint: think of the simulated summaries as approximations of probability distributions For the licorice dataset, Ruetzler et al. (2013) say that “Randomization (1:1) to licorice or placebo was assigned by a Web-based system that was accessed just before treatment by an independent researcher who was not involved in data collection; no stratification was used.” References Efron, Bradley. 1971. “Forcing a Sequential Experiment to Be Balanced.” Biometrika 58 (3): 403–17. Ruetzler, Kurt, Michael Fleck, Sabine Nabecker, Kristina Pinter, Gordian Landskron, Andrea Lassnigg, Jing You, and Daniel I Sessler. 2013. “A Randomized, Double-Blind Comparison of Licorice Versus Sugar-Water Gargle for Prevention of Postoperative Sore Throat and Postextubation Coughing.” Anesthesia &amp; Analgesia 117 (3): 614–21. "],["cp2analysis.html", "B Computer Practical 2 - Analysis Preliminaries B.1 Analysis", " B Computer Practical 2 - Analysis Preliminaries You will need a lot of the techniques covered in this practical for your summative assignments, so consider it a sort of informal formative assignment to finish it if you don’t in class. Or, at the very least, you might need to return to it while working on your summative assignments. There will be a mixture of full solutions, examples of possible solutions and example code to adapt. If you’re not sure how to do something, please ask! In the example code, I have used the same names for most objects. In order to store your results and avoid confusion, it will be sensible to name things intelligently! For example, suffix each allocation data frame so that you know which allocation method you used. Create an R file with the commands in that you use, so that you can easily replicate your work. R practicalities There are many, many packages in R that implement methods for designing and analysing clinical trials (see a list at CRAN task view). We will look at some of these, and will also write our own code for some tasks. Remember that to install a package, you can do install.packages(&quot;&lt;packagename&gt;&quot;) If you have problems running R on your laptop, or on the university machines, the most foolproof way might be to use Github codespaces (thanks to Louis Aslett, who developed this for Data Science and Statistical Computing II). You may be familiar with this approach if you did Bayesian Computational Modelling III. An advantage of this is that you can open the same codespace (the same instance of R) from any computer, so if you plan to work on things (for example your summative assignment, which will involve some R) from more than one computer, this might be ideal. This requires you to have a github account (you can sign up for free here) and there is a short guide to creating a github account here. Direct link to codespace Instructions for how to use codespace B.1 Analysis For our analysis section, we will work with real datasets, and use them to explore and compare some of the different analysis methods we’ve covered in lectures. B.1.1 Polyps data The dataset we’ll use concerns a small trial in which patients with familial adenomatous polyposis (FAP) were treated either with Sulindac (group T) or a placebo (group C). People with FAP tend to have polyps (small growths) grow in their colon. Although the polyps themselves are not dangerous or harmful, they can turn into colon cancer. You can read about the study in Giardiello et al. (1993). Exercise B.1 First of all let’s explore the dataset: Look at the help file for polyps Which columns should you use for baseline and outcome variables? Plot the data in these columns. Do you see any issues? Can you suggest a way to address them? What effect will this have on our estimator of the treatment effect? Click for solution Solution. First of all, to access the data, enter data(polyps, package = &quot;medicaldata&quot;) You can then view the help file by entering ?polyps. To investigate the columns in polyps we can use the structure function str(polyps) We have the baseline number of polyps for each patient in the column baseline, and a sensible column to use for the outcome would be number12m. Other baseline variables are sex and age. To plot the baseline and outcome variables we can do ggplot(data=polyps, aes(x=baseline)) + geom_histogram() ggplot(data=polyps, aes(x=number12m)) + geom_histogram() These are both very right skewed, and exclusively positive (which makes sense given they are counts). A sensible thing to do therefore would be to take the log of these two variables. Since they are counts we will use the log to base 10, so that our results are easier to interpret. If you’d prefer to use natural logs that’s fine, some of your numbers will be different. polyps$log_baseline = log10(polyps$baseline) polyps$log_number12m = log10(polyps$number12m) Since we used the logarithms of the numbers of polyps, our treatment effect is \\[\\log\\left(X_T\\right) - \\log\\left(X_C\\right) = \\log\\left(\\frac{X_T}{X_C}\\right).\\] We will work through the tests as we did in lectures, to see how the results differ. Exercise B.2 Using just the outcome variable (which should be log_number12m or something similar, see solution above if you aren’t sure why), test the hypothesis that the Sulindac has had some effect. Click for solution Solution. We can either do this the long way or the short way! For sanity’s sake, we’ll do it the long way once, then in subsequent questions I’ll only show the short way in the solutions. The long way The first thing to do is to calculate means for each group: polyps_df = na.omit(polyps) # two participants (001 and 018) don&#39;t have data for 12m, so remove these mean_T = mean(polyps_df$log_number12m[polyps_df$treatment==&quot;sulindac&quot;]) sd_T = sd(polyps_df$log_number12m[polyps_df$treatment==&quot;sulindac&quot;]) mean_C = mean(polyps_df$log_number12m[polyps_df$treatment==&quot;placebo&quot;]) sd_C = sd(polyps_df$log_number12m[polyps_df$treatment==&quot;placebo&quot;]) # There are 11 patients on Placebo (group C) and 9 on Sulindac (group T) # The pooled standard deviation pooled_sd_polypsX = sqrt((10*sd_C^2 + 8*sd_T^2)/(10+8)) # Finally we find the test statistic test_stat = (mean_T - mean_C)/(pooled_sd_polypsX*sqrt(1/11 + 1/9)) Under \\(H_0\\) (that there is no treatment effect) the test statistic follows a \\(t_{18}\\) distribution, and therefore we find our \\(p\\)-value by 2*pt(test_stat, df=18) ## [1] 0.001046571 Note that if the test statistic were positive, we’d have to do 2*(1-pt(test_stat, df=18)) Thus \\(p=0.001\\) and we conclude that the treatment effect is significant. Our confidence interval is given by estimate = mean_T - mean_C error = qt(0.975, df=18)*pooled_sd_polypsX*sqrt(1/11 + 1/9) c(estimate - error, estimate + error) ## [1] -1.2264748 -0.3678718 The short way R has a t-test function built in, so we can simply use that. Notice that it gives us everything our heart might desire (on a fairly mundane day), including a confidence interval! t.test( x=polyps_df$log_number12m[polyps_df$treatment == &quot;sulindac&quot;], y=polyps_df$log_number12m[polyps_df$treatment == &quot;placebo&quot;], alternative = &quot;two.sided&quot;, var.equal=T, # this makes the method use pooled variances, as we did in lectures conf.level = 0.95 # note that this is 1-alpha ) ## ## Two Sample t-test ## ## data: polyps_df$log_number12m[polyps_df$treatment == &quot;sulindac&quot;] and polyps_df$log_number12m[polyps_df$treatment == &quot;placebo&quot;] ## t = -3.9012, df = 18, p-value = 0.001047 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.2264748 -0.3678718 ## sample estimates: ## mean of x mean of y ## 0.6671373 1.4643106 Either way, our 95% confidence interval is \\(\\left(-1.226,\\;-0.368\\right)\\) (to 3 d.p). Therefore, the confidence interval for \\(\\frac{X_T}{X_C}\\) is \\[\\left(10^{-1.226},\\;10^{-0.368}\\right) = \\left(0.0594,\\;0.429\\right).\\] That is, the number of polyps at 12 months for someone in group \\(T\\) is likely to be somewhere between \\(0.0594 X_C\\) and \\(0.429 X_C\\). We’ll now move on to comparing the differences between baseline and outcome Exercise B.3 Perform another \\(t\\)-test, this time using the difference between outcome and baseline. Hint: because we’ve taken logs, you’ll have to think about what variable to use and perhaps experiment with some possibilities Click for solution Solution. The first step is to calculate a difference column. This is somewhat complicated by that fact that we have taken logs of the measurements. Taking logs of the difference would not work, since some are negative. Potentially it might work to just work with the (unlogged) differences polyps_df$diff = polyps_df$number12m - polyps_df$baseline ggplot(data=polyps_df, aes(x=diff, fill=treatment)) + geom_histogram(position=&quot;dodge&quot;, bins=10) But the outliers look potentially problematic, and the central bulk of each distribution doesn’t look very normal. We could also try the difference of the logged measurements: polyps_df$diff_log = polyps_df$log_number12m - polyps_df$log_baseline ggplot(data=polyps_df, aes(x=diff_log, fill=treatment)) + geom_histogram(position=&quot;dodge&quot;, bins=10) This looks a lot better - no more outliers and closer to normal-looking. Obviously with so few observations we won’t have a nice normal curve. To do a t-test, we can use R’s in-built function t.test( x=polyps_df$diff_log[polyps_df$treatment == &quot;sulindac&quot;], y=polyps_df$diff_log[polyps_df$treatment == &quot;placebo&quot;], alternative = &quot;two.sided&quot;, var.equal=T, # this makes the method use pooled variances, as we did in lectures conf.level = 0.95 # note that this is 1-alpha ) ## ## Two Sample t-test ## ## data: polyps_df$diff_log[polyps_df$treatment == &quot;sulindac&quot;] and polyps_df$diff_log[polyps_df$treatment == &quot;placebo&quot;] ## t = -3.5142, df = 18, p-value = 0.002477 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.0242756 -0.2578053 ## sample estimates: ## mean of x mean of y ## -0.58476481 0.05627565 Finally we can try fitting an ANCOVA model to the data, using the function lm. Exercise B.4 Fit an ANCOVA model to the licorice data, and interpret your results. If you aren’t familiar with the function lm, it might be a good idea to look at the solutions. Click for solution Solution. In the ANCOVA model we saw in lectures so far, we fit a linear model with the outcome as dependent / target variable, and the trial group and baseline as independent / explanatory variables. lm_polyp1 = lm(log_number12m ~ treatment + log_baseline, data=polyps_df) summary(lm_polyp1) ## ## Call: ## lm(formula = log_number12m ~ treatment + log_baseline, data = polyps_df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.69628 -0.15530 0.04155 0.17288 0.68553 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.6291 0.2798 2.249 0.038084 * ## treatmentsulindac -0.7046 0.1675 -4.205 0.000595 *** ## log_baseline 0.5932 0.1824 3.251 0.004698 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3673 on 17 degrees of freedom ## Multiple R-squared: 0.6659, Adjusted R-squared: 0.6266 ## F-statistic: 16.94 on 2 and 17 DF, p-value: 8.973e-05 Based on this, the effect of the drug sulindac is significant (\\(p=0.00595\\), lower than our previous models). The 95% CI for the treatment effect (which is still \\(\\log\\left(\\frac{X_T}{X_C}\\right)\\)) now is c(-0.7046 - qt(0.975, df=17)*0.1675, -0.7046 + qt(0.975, df=17)*0.1675 ) ## [1] -1.0579941 -0.3512059 We could create a nicer-looking table of the coefficients using tbl_regression (from gtsummary): #pgceiluesy table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #pgceiluesy thead, #pgceiluesy tbody, #pgceiluesy tfoot, #pgceiluesy tr, #pgceiluesy td, #pgceiluesy th { border-style: none; } #pgceiluesy p { margin: 0; padding: 0; } #pgceiluesy .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #pgceiluesy .gt_caption { padding-top: 4px; padding-bottom: 4px; } #pgceiluesy .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #pgceiluesy .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #pgceiluesy .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #pgceiluesy .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #pgceiluesy .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #pgceiluesy .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #pgceiluesy .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #pgceiluesy .gt_column_spanner_outer:first-child { padding-left: 0; } #pgceiluesy .gt_column_spanner_outer:last-child { padding-right: 0; } #pgceiluesy .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #pgceiluesy .gt_spanner_row { border-bottom-style: hidden; } #pgceiluesy .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #pgceiluesy .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #pgceiluesy .gt_from_md > :first-child { margin-top: 0; } #pgceiluesy .gt_from_md > :last-child { margin-bottom: 0; } #pgceiluesy .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #pgceiluesy .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #pgceiluesy .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #pgceiluesy .gt_row_group_first td { border-top-width: 2px; } #pgceiluesy .gt_row_group_first th { border-top-width: 2px; } #pgceiluesy .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #pgceiluesy .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #pgceiluesy .gt_first_summary_row.thick { border-top-width: 2px; } #pgceiluesy .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #pgceiluesy .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #pgceiluesy .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #pgceiluesy .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #pgceiluesy .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #pgceiluesy .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #pgceiluesy .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #pgceiluesy .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #pgceiluesy .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #pgceiluesy .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #pgceiluesy .gt_left { text-align: left; } #pgceiluesy .gt_center { text-align: center; } #pgceiluesy .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #pgceiluesy .gt_font_normal { font-weight: normal; } #pgceiluesy .gt_font_bold { font-weight: bold; } #pgceiluesy .gt_font_italic { font-style: italic; } #pgceiluesy .gt_super { font-size: 65%; } #pgceiluesy .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #pgceiluesy .gt_asterisk { font-size: 100%; vertical-align: 0; } #pgceiluesy .gt_indent_1 { text-indent: 5px; } #pgceiluesy .gt_indent_2 { text-indent: 10px; } #pgceiluesy .gt_indent_3 { text-indent: 15px; } #pgceiluesy .gt_indent_4 { text-indent: 20px; } #pgceiluesy .gt_indent_5 { text-indent: 25px; } #pgceiluesy .katex-display { display: inline-flex !important; margin-bottom: 0.75em !important; } #pgceiluesy div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after { height: 0px !important; } Characteristic Beta 95% CI1 p-value treatment     placebo — —     sulindac -0.70 -1.1, -0.35 log_baseline 0.59 0.21, 0.98 0.005 1 CI = Confidence Interval but note that this doesn’t show all the information that summary does. Unlike in the \\(t\\)-test where we can only compare measurements like-for-like, in ANCOVA we fit a coefficient to the baseline covariate. This means we are no longer limited to comparing the outcome variable to variables on the same scale, but can also include other baseline variables. Exercise B.5 Fit another linear model, this time including the other baseline variables. Click for solution Solution. lm_polyp2 = lm(log_number12m ~ treatment + log_baseline + sex + age, data = polyps_df) summary(lm_polyp2) ## ## Call: ## lm(formula = log_number12m ~ treatment + log_baseline + sex + ## age, data = polyps_df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.59539 -0.32008 0.09788 0.17374 0.62089 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.888170 0.503801 1.763 0.098266 . ## treatmentsulindac -0.730464 0.177936 -4.105 0.000936 *** ## log_baseline 0.495560 0.216900 2.285 0.037307 * ## sexmale 0.197442 0.201555 0.980 0.342821 ## age -0.009412 0.012205 -0.771 0.452620 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3779 on 15 degrees of freedom ## Multiple R-squared: 0.688, Adjusted R-squared: 0.6048 ## F-statistic: 8.27 on 4 and 15 DF, p-value: 0.0009895 In this case it appears that the other two baseline covariates, age and sex, don’t have a significant effect on the outcome. Exercise B.6 Inspect some plots of the residuals, to check whether these models have any systematic problems. Does there appear to be any multicollinearity in the data? Click for solution Solution. These solutions will demonstrate some methods with the first model. First of all, we can add columns with the residuals and fitted values from the model. polyps_df$resid1 = resid(lm_polyp1) polyps_df$fitted1 = fitted(lm_polyp1) ggplot(data=polyps_df, aes(x=fitted1, y=resid1, col=treatment)) + geom_point() ggplot(data=polyps_df, aes(x=log_baseline, y=resid1, col=treatment)) + geom_point() ggplot(data=polyps_df, aes(x=resid1, fill=treatment)) + geom_histogram(bins=20) None of these ring huge alarm bells, but because the dataset is so small it’s quite hard to tell! Arguably the residuals for the sulindac group are more spread out than those for the plaecbo, but there are no obvious systematic trends. We can find the variance inflation factor by vif(lm_polyp1) ## treatment log_baseline ## 1.029768 1.029768 and it indicates no problems. Another sign that the ANCOVA model is well-fitted is that the standard errors of the coefficients are all reasonably small. If there was a problem, for example multicollinearity, some of these would blow up. B.1.2 Extension: Treatment for maternal periodontal disease In this section we look at a larger and more complex (therefore more realistic) dataset. The opt data are in the package medicaldata, which you should already have loaded. The aim of the study is to find out whether treating women for periodontal disease during the first 21 weeks of pregnancy resulted in a low birth weight or pre-term birth. The groups are stored in the group variable group T: those who received periodontal treatment, and tooth polishing at their follow-ups group C: Brief oral exams There are 823 participants and 171 variables - many of these are baseline covariates, but there are also quite a few interim measurements. The study’s primary outcome variable is gestational age at end of pregnancy, but birth weight was also measured, as well as some clinical measures of periodontal disease and some microbiological and immunological outcomes. To make this more managable, we will concentrate on the outcome Birthweight. Exercise B.7 Plot the variable Birthweight. Do we need to use any transformations before we model it? Click for solution Solution. ggplot(data=opt, aes(x=Birthweight, fill=Group)) + geom_histogram(position=&quot;dodge&quot;) This data looks very nice and normal (although there is a bit of a fat tail to the left), so we won’t transform it. There are lots of issues with the opt data, many of which are common in real datasets. We’ll reduce the dataset to a more managable size, by removing all outcomes except Birthweight and reducing the number of covariates. One issue with the opt dataset is missingness. While some of the missing data are genuinely missing, quite a lot of it can be logically filled in. This is explained in the code below. opt_red = opt[ ,c(1:22,72)] # Change NAs to &quot;None&quot; for diabetic # since in opt the non-diabetic people are coded as NA (and therefore excluded from the model) diab = as.character(opt_red$BL.Diab.Type) diab[is.na(diab)] = &quot;None&quot; opt_red$BL.Diab.Type = as.factor(diab) # similar problem with smokers and how many cigarettes per day # If people are non-smokers and have missing for number of cigarettes per day # change their number of cigarettes to zero sm = opt_red$Use.Tob cigs = opt_red$BL.Cig.Day cigs[(is.na(cigs)&amp;(sm==&quot;No &quot;))] = 0 opt_red$BL.Cig.Day = cigs # Same for alcohol and drinks per day alc = opt_red$Use.Alc dr = opt_red$BL.Drks.Day dr[(is.na(dr)&amp;(alc==&quot;No &quot;))] = 0 opt_red$BL.Drks.Day = dr # If a participant hasn&#39;t had a previous pregnancy, her N.prev.preg should be zero (not NA) pp = opt_red$Prev.preg npp = opt_red$N.prev.preg npp[pp==&quot;No &quot;] = 0 opt_red$N.prev.preg = npp When we use lm to fit an ANCOVA model, all rows with an NA in will be ignored. It’s therefore important to try to eradicate any NAs where we actually do know the value! In reality we’d also investigate the patterns of missingness to check whether this might have introduced bias into the trial. There’s a whole extra computer practical on this that I’ve included on the website but that we aren’t going to do (and so it won’t be assessed!) - C. Feel free to have a look. Exercise B.8 Perform a \\(t\\)-test on the outcome Birthweight. What do you find? Can you also perform a \\(t\\)-test using difference from baseline? Click for solution Solution. To perform the \\(t\\)-test we can use the inbuilt R function # Check SDs are fairly close before proceeding sd(opt_red$Birthweight[opt_red$Group == &quot;T&quot;], na.rm=T) ## [1] 636.82 sd(opt_red$Birthweight[opt_red$Group == &quot;C&quot;], na.rm=T) ## [1] 727.4854 t.test( x=opt_red$Birthweight[opt_red$Group == &quot;T&quot;], y=opt_red$Birthweight[opt_red$Group == &quot;C&quot;], alternative = &quot;two.sided&quot;, var.equal=T, # this makes the method use pooled variances, as we did in lectures conf.level = 0.95 # note that this is 1-alpha ) ## ## Two Sample t-test ## ## data: opt_red$Birthweight[opt_red$Group == &quot;T&quot;] and opt_red$Birthweight[opt_red$Group == &quot;C&quot;] ## t = 0.74585, df = 807, p-value = 0.456 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -58.49266 130.18492 ## sample estimates: ## mean of x mean of y ## 3216.670 3180.824 We find that the difference in Birthweights between the groups is not even close to significant (\\(p=0.456\\)). We can’t do a \\(t\\)-test with differences because there is no comparable baseline measurement. Now we will move on to ANCOVA. Exercise B.9 Before fitting the model, think about what you expect to find. Bear in mind: the result above the description of risk factors of preterm birth / low birth weight in the help file for opt which variables you will include in your model Click for solution Solution. The \\(p\\)-value from the \\(t\\)-test was very high, so it seems unlikely that we’ll find a significant treatment effect using ANCOVA unless there are some significant interactions going on between covariates. The help file says (under ‘Background’): Many risk factors for preterm birth have already been identified, including maternal age, drug use, and diabetes. However, such factors are exhibited in only about half of preterm birth mothers, highlighting a need to expand our understanding of what contributes to preterm birth risk. Therefore, if we include related factors in our model (many of which we have in our dataset) we should expect to see significant coefficients. But, it sounds like there is a lot that isn’t well understood, so our model is likely not to explain a huge proportion of the variance. Exercise B.10 Fit an ANCOVA model. What do you find? Click for solution Solution. To fit a linear model including every variable as a covariate (apart from the target variable), we can do lm_full = lm(Birthweight ~ ., data = opt_red[ ,-1]) # don&#39;t include the ID column! summary(lm_full) ## ## Call: ## lm(formula = Birthweight ~ ., data = opt_red[, -1]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3019.46 -233.35 46.89 382.82 1935.06 ## ## Coefficients: (1 not defined because of singularities) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3906.439 702.835 5.558 3.91e-08 *** ## ClinicMN -57.521 73.391 -0.784 0.43345 ## ClinicMS -173.068 88.189 -1.962 0.05011 . ## ClinicNY -146.548 100.180 -1.463 0.14397 ## GroupT 28.780 49.914 0.577 0.56440 ## Age -1.686 5.417 -0.311 0.75572 ## BlackYes -158.055 127.807 -1.237 0.21664 ## WhiteYes -47.195 100.791 -0.468 0.63976 ## Nat.AmYes -224.757 112.946 -1.990 0.04699 * ## AsianYes -142.693 301.298 -0.474 0.63594 ## HispNo 133.343 75.097 1.776 0.07624 . ## HispYes 126.498 115.009 1.100 0.27177 ## EducationLT 8 yrs -43.324 70.513 -0.614 0.53915 ## EducationMT 12 yrs 23.011 66.301 0.347 0.72864 ## Public.AsstceYes -144.623 62.886 -2.300 0.02176 * ## HypertensionY -372.548 148.033 -2.517 0.01207 * ## DiabetesYes 443.811 170.343 2.605 0.00938 ** ## BL.Diab.TypeType I -425.019 318.490 -1.334 0.18249 ## BL.Diab.TypeType II NA NA NA NA ## BMI 2.124 3.857 0.551 0.58206 ## Use.TobYes -17.794 134.088 -0.133 0.89446 ## BL.Cig.Day -23.101 11.560 -1.998 0.04607 * ## Use.AlcYes -159.827 223.653 -0.715 0.47509 ## BL.Drks.Day 28.460 24.851 1.145 0.25252 ## Drug.AddNo -498.971 659.330 -0.757 0.44944 ## Drug.AddYes -475.778 806.745 -0.590 0.55555 ## Prev.pregYes 85.355 70.825 1.205 0.22856 ## N.prev.preg -17.281 18.228 -0.948 0.34345 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 653.6 on 686 degrees of freedom ## (110 observations deleted due to missingness) ## Multiple R-squared: 0.06774, Adjusted R-squared: 0.03241 ## F-statistic: 1.917 on 26 and 686 DF, p-value: 0.004183 We see from the model summary that our model is terrible: \\(R^2=0.03241\\), which means it is explaining about 3% of the variance in Birthweight. If you want to use only certain terms, you can include them in the formula, for example lm_eg = lm(Birthweight ~ Group + Age + Hypertension, data=opt_red) We see that, as we expected, the Group variable is not significant (\\(p=0.5644\\)). However, some terms are significant, for example whether or not the participant has diabetes, and how many cigarettes a mother smokes per day - this isn’t surprising given the contextual information we had. Exercise B.11 Perform some diagnostic checks on your model. Do you have any reason to suspect it isn’t adequate? Click for solution Solution. One thing to notice from the linear model summary is that many coefficients have proportionally quite large standard errors. This could be because of a lack of data (eg. if there are very few participants in some category) or because of multicollinearity (in which case the model cannot know which of the linked variables to attribute the effect to). Combining some categories or carefully removing some covariates could help improve the model. We can also study the residuals of our model, to check that they appear to be homoskedastic and approximately normal with mean 0. The first step is to extract the residuals and the fitted values (which are also useful). We will create a new data frame called opt_diag with these in, so that we can plot things easily but don’t pollute our original dataset (in case we want to fit any more models) opt_diag = na.omit(opt_red) # lm only fits where all variables are present opt_diag$resid = resid(lm_full) opt_diag$fitted = fitted(lm_full) Some examples of plots are ggplot(data=opt_diag, aes(x=resid, fill=Group)) + geom_histogram(position=&quot;dodge&quot;) ggplot(data=opt_diag, aes(x=fitted, y=resid, col=Group)) + geom_point() Exercise B.12 Given the results of your ANCOVA model, what do you think the risks would be if the study had been much smaller, or if the allocation had not been well-balanced? Click for solution Solution. In this case, it would be possible for the baseline factors that did turn out to be significant to make it appear that the treatment had a significant effect. This wouldn’t be possible with ANCOVA, but it would with a \\(t\\)-test in which the other covariates aren’t considered. References Giardiello, Francis M, Stanley R Hamilton, Anne J Krush, Steven Piantadosi, Linda M Hylind, Paul Celano, Susan V Booker, C Rahj Robinson, and G Johan A Offerhaus. 1993. “Treatment of Colonic and Rectal Adenomas with Sulindac in Familial Adenomatous Polyposis.” New England Journal of Medicine 328 (18): 1313–16. "],["cp-missing.html", "C Extra Computer Practical - Missing data C.1 Missing data C.2 Understanding the patterns of missingness C.3 Exploring the relationship between missingness and other variables C.4 What to do about missing data?!", " C Extra Computer Practical - Missing data This computer practical is just here because it exists and might as well be used - it isn’t going to be covered in any of the sessions, and it isn’t going to be assessed. So, if you’re not especially interested in missing data, feel free to stop reading now! In this computer practical we focus on an important issue in clinical trials (and most real world projects!): missing data. There will be a bit more reading than in future practicals, to give you the necessary theory and background. We will only have time to skim the surface of working with missing data, but if you want to find out more, some good references are Little and Rubin (2019) and chapter 17 of Gelman, Hill, and Vehtari (2021). Consider it a sort of informal formative assignment to finish this practical in your own time if you don’t in class. Or, at the very least, you might need to return to it while working on future assignments. For this reason, it would be sensible to keep track of everything you do in an .R file that you can return to in the future. There will be a mixture of full solutions, examples of possible solutions and example code to adapt. If you’re not sure how to do something, please ask! R practicalities There are many, many packages in R that implement methods for designing and analysing clinical trials (see a list at CRAN task view). We will look at some of these, and will also write our own code for some tasks. Remember that to install a package, you can do install.packages(&quot;&lt;packagename&gt;&quot;) and to then load the package (it doesn’t load automatically on install) enter library(&lt;packagename&gt;) If you have problems running R on your laptop, or on the university machines, the most foolproof way might be to use Github codespaces (thanks to Louis Aslett, who developed this for Data Science and Statistical Computing II). You may be familiar with this approach if you did Bayesian Computational Modelling III. An advantage of this is that you can open the same codespace (the same instance of R) from any computer, so if you plan to work on things (for example your summative assignment, which will involve some R) from more than one computer, this might be ideal. Codespace requires you to have a github account (you can sign up for free here) and there is a short guide to creating a github account here. Direct link to codespace Instructions for how to use codespace C.1 Missing data In [almost] any real-world study or trial, there will be some missing data. This can happen for a whole host of reasons: perhaps some participants dropped out completely, or couldn’t make some appointments. Perhaps some data were lost or corrupted. Perhaps some participants decided not to provide certain details, or didn’t comply properly with the study. Why does this matter? Or, why do we need to think properly about this? Well, a huge amount of work in the planning and design of an RCT goes into the random allocation: we want to eliminate all sources of bias (including those we can’t observe, or aren’t even aware of) by randomly balancing the participants between the trial arms. If some of these participants’ data are missing, we can no longer be confident that we still have this balance. Dealing with missing data boils down to two main tasks: Understanding the pattern(s) of missingness Processing the data to mitigate against the effect of the missingness so this is what we’ll be working on now. Before that though, we need some data to work with! C.1.1 Datasets The datasets we’ll work with come from the packages medicaldata and smdi, so you’ll need to install those if you don’t have them. We’ll also be using the packages naniar and visdat, which contain lots of methods for handling missing data, and rstanarm, which will enable us to easily sample from regression models. Other packages are available, as you can see from the CRAN task view on Missing Data. We’ll also use tidyverse for various plotting and data handling tasks. require(medicaldata) library(naniar) library(smdi) library(tidyverse) library(visdat) library(rstanarm) Throughout this practical we’ll work with three datasets. C.1.1.1 Supraclavicular The first dataset is the supraclavicular data. This is a fairly simple data set (at least in terms of missingness) which I’ll use to to demonstrate techniques before asking you to work on the more complex data! Exercise C.1 Load the supraclavicular data. - What is the study about? - What is the primary outcome? - What are the baseline covariates (remember these must be measured before allocation) Create a data frame containing just the baseline covariates, the allocation and the primary outcome variable. Make sure these are of the correct type. Click for solution Solution. The details of the study can be found using ?supraclavicular. The primary outcome variable is time to 4-nerve sensory block, which has the column name onset_sensory. The baseline covariates are gender, BMI and age. To create our data frame, we need sup_df = supraclavicular[ ,c(2:5, 9)] sup_df = sup_df%&gt;% mutate(across(c(group, gender), factor)) C.1.1.2 Lung cancer dataset This is the smdi_data dataset from the package smdi. The intervention group variable is exposure. The outcome data are survival (or ‘time-to-event’) data: in this study, the ‘event’ is death and the follow-up period is five (probably years, but it doesn’t say!). Some participants are still alive at the end of the follow-up period (so they have status=0) and the data are therefore right censored. We will learn more about this type of data later in the course. For now, the only preparation to do is to convert status to a factor variable: smdi_data$status = factor(smdi_data$status) This is a synthetic (or possibly synthetically altered) dataset designed for use with missing data exploration, so we have the advantage that the help file tells about the missingness mechanisms (more on that soon). C.1.1.3 Obstetrics and Periodontal Therapy The final dataset we’ll work with is opt, which is about the treatment of maternal periodontal disease and its effect on birthweight and term. Exercise C.2 Use ?opt to find out about this trial. There are a lot of columns here, but we won’t keep all of them. Our dataset (for now) will be opt_df = opt[ ,c(1:4, 10:22, 72)] which retains most of the baseline covariates, intervention group and outcome variable (birthweight). There is some ‘messiness’ in the data, for example for some variables missing values are recorded as \"\", rather than NA. We’ll sort that now by running the following command opt_df = opt_df %&gt;% mutate(across(c(Use.Tob, Use.Alc, Drug.Add), gsub, pattern = &quot;^ +$&quot;, replacement = NA))%&gt;% mutate(across(c(Use.Tob, Use.Alc, Drug.Add), factor)) C.2 Understanding the patterns of missingness The first thing we’ll do is explore the data to find out what the pattern of missingness is. First of all, we can visualise which data are missing. C.2.1 Visualising missingness The function vis_dat (from visdat) gives an excellent first look at a dataset, colouring data by type (and missing data in grey). The default is to sort the columns by type, but you can see below that I’ve made it keep to the origianl order. vis_dat(sup_df, sort_type=F) The functions gg_miss_var, gg_miss_case and gg_miss_upset (all from naniar) allow us to quickly see how much missing data there is, either by variable, by case or by intersection. Look at the help files to find out what the arguments do. We see that in supraclavicular there are three missing entries for BMI, and all other variables are complete. gg_miss_var(sup_df) Plotting by cases shows that each of these missing data are associated with a different participant, so there are three participants with one missing item each. gg_miss_case(sup_df) md.pattern(sup_df) ## group gender age onset_sensory bmi ## 100 1 1 1 1 1 0 ## 3 1 1 1 1 0 1 ## 0 0 0 0 3 3 Exercise C.3 Make these plots for the remaining datasets, smdi_data and opt_df. How does the missingness compare? Which plots do you find more useful and why? Click for solution Solution. Lung cancer We see there is missingness for three variables: vis_dat(smdi_data) The naniar functions can show us how this is spread across the variables and participants. gg_miss_var(smdi_data, show_pct=T) gg_miss_case(smdi_data, order_cases = T) The function md.pattern shows us how many participants have each combination of missing data: md.pattern(smdi_data, rotate.names = T) ## exposure age_num female_cat smoking_cat physical_cat alk_cat histology_cat ses_cat copd_cat ## 795 1 1 1 1 1 1 1 1 1 ## 479 1 1 1 1 1 1 1 1 1 ## 446 1 1 1 1 1 1 1 1 1 ## 263 1 1 1 1 1 1 1 1 1 ## 151 1 1 1 1 1 1 1 1 1 ## 176 1 1 1 1 1 1 1 1 1 ## 93 1 1 1 1 1 1 1 1 1 ## 97 1 1 1 1 1 1 1 1 1 ## 0 0 0 0 0 0 0 0 0 ## eventtime status pdl1_num ecog_cat egfr_cat ## 795 1 1 1 1 1 0 ## 479 1 1 1 1 0 1 ## 446 1 1 1 0 1 1 ## 263 1 1 1 0 0 2 ## 151 1 1 0 1 1 1 ## 176 1 1 0 1 0 2 ## 93 1 1 0 0 1 2 ## 97 1 1 0 0 0 3 ## 0 0 517 899 1015 2431 Obstetrics and Periodontal Therapy Using vis_dat we see that there is a lot of missing data in the opt dataset, and that some variables are particularly badly affected. vis_dat(opt_df) Now the naniar functions really do help us to understand what we’re dealing with! gg_miss_var(opt_df, show_pct=T) gg_miss_case(opt_df, order_cases = T, show_pct=T) Not many individual participants have more than about 25% of their data missing, but there are some variables that are missing for nearly all participants (more on this later). C.2.2 Summary tables We may also want to visualise or summarise missingness in a table, and there are various ways to do this, for example miss_case_summary, miss_var_summary and miss_var_summary. Exercise C.4 Use the three table generating functions listed above on our datasets. Do they give you any new information compared to the plots? Click for solution Solution. Working through the list of functions (just for opt_df), we have: miss_case_summary(opt_df) ## # A tibble: 823 × 3 ## case n_miss pct_miss ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 423 9 50 ## 2 80 8 44.4 ## 3 93 8 44.4 ## 4 477 8 44.4 ## 5 539 8 44.4 ## 6 11 7 38.9 ## 7 21 7 38.9 ## 8 43 7 38.9 ## 9 54 7 38.9 ## 10 150 7 38.9 ## # ℹ 813 more rows This shows how many variables (both in number and as a percentage of the total number of variables) are missing, for each case (in decreasing order of missingness). miss_case_table(opt_df) ## # A tibble: 9 × 3 ## n_miss_in_case n_cases pct_cases ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 7 0.851 ## 2 2 87 10.6 ## 3 3 462 56.1 ## 4 4 224 27.2 ## 5 5 17 2.07 ## 6 6 12 1.46 ## 7 7 9 1.09 ## 8 8 4 0.486 ## 9 9 1 0.122 miss_case_table tabulates the number of cases with \\(n\\) missing variables, for \\(n=1,\\ldots\\). The remaining functions look at things from the point of view of variables: miss_var_summary(opt_df) ## # A tibble: 18 × 3 ## variable n_miss pct_miss ## &lt;chr&gt; &lt;int&gt; &lt;num&gt; ## 1 BL.Drks.Day 810 98.4 ## 2 BL.Diab.Type 799 97.1 ## 3 BL.Cig.Day 731 88.8 ## 4 N.prev.preg 217 26.4 ## 5 BMI 73 8.87 ## 6 Use.Alc 27 3.28 ## 7 Drug.Add 27 3.28 ## 8 Use.Tob 26 3.16 ## 9 Birthweight 14 1.70 ## 10 PID 0 0 ## 11 Clinic 0 0 ## 12 Group 0 0 ## 13 Age 0 0 ## 14 Education 0 0 ## 15 Public.Asstce 0 0 ## 16 Hypertension 0 0 ## 17 Diabetes 0 0 ## 18 Prev.preg 0 0 Now we see how many (and what percentage) of cases are missing for each variable. There are many, many possible ways to visualise or tabulate missing data, and we have used but a few. If you want to find more, you can look in the help file for naniar, or indeed at another package designed for working with missing data. C.2.3 Mechanisms of missingness So far, things have been a bit ad-hoc. We may have noticed some patterns or trends, but we haven’t subjected these to any scrutiny, or used any statistical methodology to understand the missingness. When working with missing data, it’s important to think about the mechanism that is causing some data to be missing. Is there some systematic reason why some variables are missing? Broadly speaking, there are three types of missing data. These were first coined by Rubin (1976). C.2.3.1 Missing completely at random (MCAR) If data are MCAR, then the probability of missingness is the same for all units: missingness is statistically independent of all covariates (observed and unobserved), of the treatment and of the outcome. If each participant decided whether or not to provide a certain piece of information by rolling a dice (eg. 1 = don’t provide), then the resulting missingness would be MCAR. This type of missingness is the simplest to deal with, because removing the missing units is unlikely to cause any bias. However, it’s extremely rare in real life, and it’s difficult to even think of an example where it would definitely hold! C.2.3.2 Missing at random (MAR) For data to be MAR, the probability of missingness must depend only on available information. In this case, we can use the available information to model the missingness. MAR data are somewhat more common (or at least the mechanism is more commonly assumed than MCAR), and is thankfully still moderately simple to deal with. If missingness is ‘at random’, then adjusting for the relevant covariates in the analysis will avoid bias. C.2.3.3 Missing not at random (MNAR) This is the trickiest (and probably most common) mechanism, in which the missingness depends on unobserved data. It is likely then that the unobserved (and therefore unavailable) information also predicts the value that is missing, and possibly also the outcome. For example, suppose that the treatment in a clinical trial causes some unwanted side effect, such as pain. In this case, a participant in the intervention group is more likely to drop out of the study than a participant in the control group. Unless we measure this side effect (for example, a pain score), the resulting missingness is not at random. The most difficult case of MNAR is when the missingness depends on the (potentially missing) variable itself. For example, in a trial of a weight loss intervention, participants with greater weights might be more reticent to reveal them, especially if they feel the trial is going badly. MNAR mechanisms can be [imperfectly] modelled, or else mitigated by including more covariates in the data, which brings the mechanism closer to MAR. It is vital to work with the subject experts, who will have a much better idea of the sorts of mechanisms that might be causing missingness in their study. We can never prove which of these is the case, since by definition we don’t have the information we would need to establish that the mechanism is (or isn’t) MNAR. All we can do is study patterns in the data and find evidence one way or the other. C.3 Exploring the relationship between missingness and other variables In a clinical trial, ultimately what we care about is whether the missingness has changed our understanding of the outcome variable(s). This is most likely to happen if the missingness is related to the outcome variable. This could happen either directly, or because the missingness is correlated to a variable that affects the outcome. Again, we will explore things visually before using some more rigorous methods. Now, instead of simply understanding how much missing data there is, and which variables and/or cases are involved, we want to look at the relationships between missingness and the values of the other variables. In the package naniar you can create a copy of the data frame containing just NA and !NA, indexing exactly where the missing values are. as_shadow(sup_df) group_NA gender_NA bmi_NA age_NA onset_sensory_NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA !NA To avoid duplication of the original names, the column names are suffixed by ’_NA’. This means it can be appended to the original data frame to create what the authors of naniar call a nabular object. nabular(sup_df) group gender bmi age onset_sensory group_NA gender_NA bmi_NA age_NA onset_sensory_NA 1 0 41.15 52 0 !NA !NA !NA !NA !NA 2 0 25.22 54 7 !NA !NA !NA !NA !NA 2 0 34.14 46 24 !NA !NA !NA !NA !NA 1 0 41.57 54 4 !NA !NA !NA !NA !NA 1 1 27.17 41 30 !NA !NA !NA !NA !NA 2 1 22.05 21 4 !NA !NA !NA !NA !NA 1 1 26.32 68 12 !NA !NA !NA !NA !NA 2 1 24.69 61 13 !NA !NA !NA !NA !NA 1 0 35.63 44 27 !NA !NA !NA !NA !NA 1 0 35.12 28 4 !NA !NA !NA !NA !NA 1 0 36.11 36 3 !NA !NA !NA !NA !NA 1 0 28.34 60 21 !NA !NA !NA !NA !NA 2 0 22.60 34 9 !NA !NA !NA !NA !NA 2 1 30.14 64 9 !NA !NA !NA !NA !NA 2 1 26.36 37 5 !NA !NA !NA !NA !NA 2 0 NA 51 50 !NA !NA NA !NA !NA 2 1 22.32 58 7 !NA !NA !NA !NA !NA 1 0 37.84 24 2 !NA !NA !NA !NA !NA 2 1 39.85 28 6 !NA !NA !NA !NA !NA 1 0 31.21 50 3 !NA !NA !NA !NA !NA 2 1 25.64 74 5 !NA !NA !NA !NA !NA 2 1 21.26 52 41 !NA !NA !NA !NA !NA 1 0 24.22 53 4 !NA !NA !NA !NA !NA 2 1 25.10 19 14 !NA !NA !NA !NA !NA 1 0 25.42 41 7 !NA !NA !NA !NA !NA 1 0 23.60 49 2 !NA !NA !NA !NA !NA 2 0 28.20 46 4 !NA !NA !NA !NA !NA 1 1 28.33 57 3 !NA !NA !NA !NA !NA 2 0 30.40 50 17 !NA !NA !NA !NA !NA 1 0 42.38 60 1 !NA !NA !NA !NA !NA 1 0 36.98 52 2 !NA !NA !NA !NA !NA 2 1 30.67 39 1 !NA !NA !NA !NA !NA 1 1 23.43 53 1 !NA !NA !NA !NA !NA 2 1 30.61 64 3 !NA !NA !NA !NA !NA 2 1 23.62 51 20 !NA !NA !NA !NA !NA 1 1 30.17 47 6 !NA !NA !NA !NA !NA 1 0 18.80 66 10 !NA !NA !NA !NA !NA 1 1 24.52 59 33 !NA !NA !NA !NA !NA 2 1 29.60 31 39 !NA !NA !NA !NA !NA 1 1 29.16 64 8 !NA !NA !NA !NA !NA 1 1 25.85 61 48 !NA !NA !NA !NA !NA 2 1 25.84 50 9 !NA !NA !NA !NA !NA 2 1 35.36 32 3 !NA !NA !NA !NA !NA 1 1 28.43 54 22 !NA !NA !NA !NA !NA 2 1 35.25 53 27 !NA !NA !NA !NA !NA 2 0 19.70 57 6 !NA !NA !NA !NA !NA 2 1 27.50 56 37 !NA !NA !NA !NA !NA 2 1 24.40 32 38 !NA !NA !NA !NA !NA 1 1 24.30 23 6 !NA !NA !NA !NA !NA 1 0 37.92 61 5 !NA !NA !NA !NA !NA 1 0 23.76 52 4 !NA !NA !NA !NA !NA 2 1 22.12 18 19 !NA !NA !NA !NA !NA 2 0 32.40 42 15 !NA !NA !NA !NA !NA 2 0 20.49 44 23 !NA !NA !NA !NA !NA 1 1 NA 22 16 !NA !NA NA !NA !NA 2 0 32.50 48 8 !NA !NA !NA !NA !NA 2 0 36.72 52 10 !NA !NA !NA !NA !NA 2 0 39.87 48 19 !NA !NA !NA !NA !NA 1 0 33.00 51 13 !NA !NA !NA !NA !NA 1 1 30.70 54 6 !NA !NA !NA !NA !NA 1 0 27.20 36 9 !NA !NA !NA !NA !NA 1 0 30.60 53 9 !NA !NA !NA !NA !NA 1 0 28.33 66 11 !NA !NA !NA !NA !NA 1 0 34.84 60 50 !NA !NA !NA !NA !NA 1 1 NA 50 6 !NA !NA NA !NA !NA 2 0 29.00 53 7 !NA !NA !NA !NA !NA 2 1 28.59 52 26 !NA !NA !NA !NA !NA 2 0 38.19 36 18 !NA !NA !NA !NA !NA 2 0 36.96 66 9 !NA !NA !NA !NA !NA 1 1 22.55 32 16 !NA !NA !NA !NA !NA 2 1 35.15 57 6 !NA !NA !NA !NA !NA 2 0 37.62 54 19 !NA !NA !NA !NA !NA 1 0 42.93 52 13 !NA !NA !NA !NA !NA 2 1 20.98 31 9 !NA !NA !NA !NA !NA 2 1 27.37 36 8 !NA !NA !NA !NA !NA 1 0 27.02 35 5 !NA !NA !NA !NA !NA 1 0 22.83 51 2 !NA !NA !NA !NA !NA 1 0 28.73 56 5 !NA !NA !NA !NA !NA 2 0 29.49 62 10 !NA !NA !NA !NA !NA 1 0 34.08 32 18 !NA !NA !NA !NA !NA 1 0 25.59 57 15 !NA !NA !NA !NA !NA 1 1 34.17 59 12 !NA !NA !NA !NA !NA 2 1 31.39 62 24 !NA !NA !NA !NA !NA 2 0 21.26 25 50 !NA !NA !NA !NA !NA 1 0 24.20 29 8 !NA !NA !NA !NA !NA 1 1 32.28 65 4 !NA !NA !NA !NA !NA 2 1 30.45 28 8 !NA !NA !NA !NA !NA 1 1 23.34 65 6 !NA !NA !NA !NA !NA 2 0 24.54 46 5 !NA !NA !NA !NA !NA 2 0 23.61 67 10 !NA !NA !NA !NA !NA 2 0 43.46 55 10 !NA !NA !NA !NA !NA 1 0 19.91 59 3 !NA !NA !NA !NA !NA 1 1 24.45 38 20 !NA !NA !NA !NA !NA 1 0 27.78 60 7 !NA !NA !NA !NA !NA 2 0 23.06 41 11 !NA !NA !NA !NA !NA 2 0 32.79 68 14 !NA !NA !NA !NA !NA 1 1 24.96 66 12 !NA !NA !NA !NA !NA 2 0 23.06 41 10 !NA !NA !NA !NA !NA 2 1 24.40 19 10 !NA !NA !NA !NA !NA 1 0 22.24 31 9 !NA !NA !NA !NA !NA 1 0 33.91 40 11 !NA !NA !NA !NA !NA 2 1 31.65 53 22 !NA !NA !NA !NA !NA 1 0 41.54 41 40 !NA !NA !NA !NA !NA This is useful because we can investigate the values of the actual data while conditioning on the missingness. We can summarize the outcome distribution according to whether a variable is missing or observed: sup_nab %&gt;% group_by(bmi_NA) %&gt;% summarise_at(.vars = &quot;onset_sensory&quot;, .funs = c(&quot;mean&quot;, &quot;sd&quot;, &quot;var&quot;, &quot;min&quot;, &quot;max&quot;), na.rm = TRUE) ## # A tibble: 2 × 6 ## bmi_NA mean sd var min max ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 !NA 13 11.4 131. 0 50 ## 2 NA 24 23.1 532 6 50 and visualise the outcome distribution for missing and non-missing values of a covariate: ggplot(sup_nab, aes(x = onset_sensory, fill = bmi_NA)) + geom_histogram() We could also explore whether the missingness is related to the values of the other covariates, for example gender (in this data 0=female, 1=male) ggplot(sup_nab, aes(x = gender, fill = bmi_NA)) + geom_bar() This isn’t a great example because there are only three missing BMI values, but you can probably guess what you’ll be doing next… Exercise C.5 For the lung cancer and obstetric periodontal treatment datasets, investigate whether / how the outcome distribution appears to be affected by the missing values. For the smdi_data dataset, remember that the outcome variable is the combination of eventtime and status. Can you think of a way to visualise the data that combines these pieces of information? For opt, ignore the variables BL.Diab.Type, BL.Cig.Day, BL.Cig.Day and N.prev.preg. You can create a temporary opt data frame to work with opt_tmp = opt_df[ ,-c(9,12,14,17)] There are many, many plots you could make here, but try not to spend too long on this - why not divide up tasks between you and your neighbours? Click for solution Solution. We’ll look here in some detail about the lung cancer dataset (smdi_data) - these aren’t really solutions as such, just some more detail of things you could do. smdi_nab = nabular(smdi_data) First by ecog_cat. Since there is a lot more missing data here, I’ve chosen to compare the distributions side-by-side using facet_wrap. This also allows me to colour by status (although it is sort of obvious from the large peak at eventtime=5 that those people are still alive). Setting scales = \"free_y\" makes it easier to compare the overall shape of the distribution. You may have found a different way to display the data that may be equally (or more!) informative - if so that’s fine! smdi_nab %&gt;% group_by(ecog_cat) %&gt;% summarise_at(.vars = &quot;eventtime&quot;, .funs = c(&quot;mean&quot;, &quot;sd&quot;, &quot;var&quot;, &quot;min&quot;, &quot;max&quot;), na.rm = TRUE) ## # A tibble: 3 × 6 ## ecog_cat mean sd var min max ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 2.21 1.80 3.25 0.0000215 5 ## 2 1 2.07 1.79 3.22 0.000754 5 ## 3 &lt;NA&gt; 2.22 1.86 3.46 0.00248 5 ggplot(smdi_nab, aes(x = eventtime, fill = status)) + geom_histogram() + facet_wrap(~ecog_cat_NA, scales = &quot;free_y&quot;) Next by egfr_cat: smdi_nab %&gt;% group_by(egfr_cat) %&gt;% summarise_at(.vars = &quot;eventtime&quot;, .funs = c(&quot;mean&quot;, &quot;sd&quot;, &quot;var&quot;, &quot;min&quot;, &quot;max&quot;), na.rm = TRUE) ## # A tibble: 3 × 6 ## egfr_cat mean sd var min max ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 1.96 1.79 3.22 0.000754 5 ## 2 1 2.94 1.91 3.66 0.0115 5 ## 3 &lt;NA&gt; 2.15 1.76 3.10 0.0000215 5 ggplot(smdi_nab, aes(x = eventtime, fill = status)) + geom_histogram() + facet_wrap(~egfr_cat_NA, scales = &quot;free_y&quot;) and finally by pdl1_num: smdi_nab %&gt;% group_by(pdl1_num) %&gt;% summarise_at(.vars = &quot;eventtime&quot;, .funs = c(&quot;mean&quot;, &quot;sd&quot;, &quot;var&quot;, &quot;min&quot;, &quot;max&quot;), na.rm = TRUE) ## # A tibble: 1,540 × 6 ## pdl1_num mean sd var min max ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 6.14 0.655 NA NA 0.655 0.655 ## 2 12.4 0.463 NA NA 0.463 0.463 ## 3 12.9 0.679 NA NA 0.679 0.679 ## 4 13.6 0.358 NA NA 0.358 0.358 ## 5 15.8 0.290 NA NA 0.290 0.290 ## 6 16.7 1.71 NA NA 1.71 1.71 ## 7 17.4 0.245 NA NA 0.245 0.245 ## 8 17.8 1.52 NA NA 1.52 1.52 ## 9 18.7 4.91 NA NA 4.91 4.91 ## 10 19.0 2.03 NA NA 2.03 2.03 ## # ℹ 1,530 more rows ggplot(smdi_nab, aes(x = eventtime, fill = status))+ geom_histogram() + facet_wrap(~pdl1_num_NA, scales = &quot;free_y&quot;) It appears that pdl1_num values are likely to be MNAR, since the outcome distribution looks different (proportionally more early deaths) for the missing values. We could make many more plots of this type: for example, by plotting the distributions of other variables rather than the outcome. For example, we could look at how the missingness of each variable relates to smoking category: library(gridExtra) ecog_sm = ggplot(smdi_nab, aes(x = smoking_cat, fill = ecog_cat_NA))+ geom_bar() egfr_sm = ggplot(smdi_nab, aes(x = smoking_cat, fill = egfr_cat_NA))+ geom_bar() pdl1_sm = ggplot(smdi_nab, aes(x = smoking_cat, fill = pdl1_num_NA))+ geom_bar() grid.arrange(ecog_sm, egfr_sm, pdl1_sm, nrow=1) So, it looks likely that the missingness of egfr_cat depends on smoking status, but less likely that the missingness of the other two variables does. Looking into all the variables one by one like this would take a very long time, and wouldn’t even allow us to make any (useful) conclusions. Really what we want to know is whether there is a significant difference in the observed data compared to the missing data. C.3.1 Statistical summaries of the effect of missingness According to the framework coined by Rubin (1976), if the data are MAR (missing at random) then the missingness can be explained by (some of) the observed covariates. We would therefore expect the participant characteristics to be different between those with missing values and those without. If the missingness is MCAR (missing completely at random) we would expect no significant difference. Similarly, if the data are MNAR (missing not-at-random) due to some unobserved variable that is independent of all observed variables, we would expect no pattern. In reality this is almost never the case (the confounding variables are usually linked to some observed variables) and so really we are testing against the data being MCAR. C.3.1.1 Hotelling’s multivariate t-test This test examines the differences between those observations with an observation (of some partially observed variable) and those without. The test statistic is derived by assuming both groups are drawn from the same multivariate normal distribution, and so a high value of the test statistic (conversely a low p-value) suggests that there are significant differences between the groups. This test is from a generalisation of Student’s \\(t\\)-test made by Hotelling et al. (1931). To perform Hotelling’s multivariate \\(t\\)-test on the sup_df data we enter smdi_hotelling(sup_df) ## covariate hotteling_p ## 1 bmi 0.340 For this data, we find that there is insufficient evidence to reject the hypothesis that the BMI data are MCAR. Exercise C.6 Perform Hotelling’s multivariate \\(t\\)-test on the other two datasets, opt_tmp and smdi_data. What do you find? Click for solution First we’ll look at smdi_data. smdi_hotelling(smdi_data) ## covariate hotteling_p ## 1 ecog_cat 0.783 ## 2 egfr_cat &lt;.001 ## 3 pdl1_num &lt;.001 It appears that ecog_cat may be MCAR, but for egfr_cat and pdl1_num there is sufficient evidence to suggest MAR or MNAR. Now for opt_tmp smdi_hotelling(opt_tmp) ## covariate hotteling_p ## 1 BMI &lt;.001 ## 2 Use.Tob &lt;.001 ## 3 Use.Alc &lt;.001 ## 4 Drug.Add &lt;.001 ## 5 Birthweight &lt;.001 These all appear to have a significant departure from MCAR. Caution: The power of this test (and others like it) can be strongly influenced by sample size, so it is sensible to combine it with a more detailed approach. C.3.1.2 Absolute standardised mean difference The absolute standardised mean difference (ASMD) gives a measure of how different the values of the observed covariates are for missing versus observed values of each partially observed covariate. For every partially observed covariate, there is an ASMD for each other covariate. Label the partially observed covariate \\(X_M\\), and suppose \\(X_1,\\ldots,X_K\\) are the other covariates. The dataset is split into those cases with \\(X_M\\) observed and those with \\(X_M\\) missing. For each covariate \\(X_1,\\ldots,X_K\\) we find the absolute value of the difference in means, and divide this by the standard deviation of that covariate. The ASMD is therefore always non-negative, and should not be affected by sample size. A general rule of thumb is that ASMD values over 0.1 are cause for concern, though again this can be vulnerable to small sample sizes. The function smdi_asmd from smdi creates an asmd object, which has several parts to it. Note that we set includeNA=T so that we can see the effect of missingness, as well as observed values, of other variables. This won’t make a difference for sup_df, but it will in the exercise. asmd_sup = smdi_asmd(sup_df, includeNA=T) Firstly there is a summary table, which shows the median, min and max of ASMD for each partially observed covariate. asmd_sup ## # A tibble: 1 × 4 ## covariate asmd_median asmd_min asmd_max ## * &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 bmi 0.471 0.343 0.604 There is also a Table 1, so called because a summary table of this nature should always be included when summarising a dataset in terms of the difference between two groups. This is formatted a little strangely, as it is designed for use in printed works. This table includes the result of a statistical test (by default a chi-squared test) showing whether the differences are statistically significant. (The function kable here is to do with formatting the table for HTML, not to do with missing data). kable(asmd_sup$bmi$asmd_table1) 0 1 p test SMD n 100 3 group = 2 (%) 50 (50.0) 1 (33.3) 1.000 0.343 gender = 1 (%) 44 (44.0) 2 (66.7) 0.850 0.468 age (mean (SD)) 48.10 (13.30) 41.00 (16.46) 0.367 0.474 onset_sensory (mean (SD)) 13.00 (11.44) 24.00 (23.07) 0.114 0.604 Finally there is a plot showing each ASMD asmd_sup$bmi$asmd_plot We see that although the ASMD values are quite large (much bigger than the advised 0.1), because there are a very small number of them they are not statistically significant. Exercise C.7 Investigate the ASMD for our datasets smdi_data and opt_tmp. Of the partially observed covariates, which seem to be most strongly related to other covariates? Do any seem to be MCAR? Make sure you remove any participant ID variables, since we don’t want to include those in our analysis! Again, it might be a good idea to pair up! Click for solution Solution. We will do this just for the opt_tmp data in the solutions. asmd_opt = smdi_asmd(opt_tmp[ ,-1], includeNA=T) asmd_opt ## # A tibble: 5 × 4 ## covariate asmd_median asmd_min asmd_max ## * &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 BMI 0.195 0.041 1.717 ## 2 Use.Tob 0.093 0.004 39.900 ## 3 Use.Alc 0.099 0.004 8.204 ## 4 Drug.Add 0.073 0.004 7.229 ## 5 Birthweight 0.340 0.004 2.556 These values are all well above 0.1, so it seems unlikely that the data are MCAR. We can look in a little more detail, first at BMI: asmd_opt$BMI$asmd_plot kable(asmd_opt$BMI$asmd_table1) 0 1 p test SMD n 750 73 Clinic (%) &lt;0.001 1.717 KY 205 (27.3) 6 ( 8.2) MN 237 (31.6) 10 (13.7) MS 192 (25.6) 0 ( 0.0) NY 116 (15.5) 57 (78.1) Group = T (%) 375 (50.0) 38 (52.1) 0.832 0.041 Age (mean (SD)) 25.91 (5.54) 26.68 (5.87) 0.256 0.136 Education (%) 0.137 0.229 8-12 yrs 441 (58.8) 38 (52.1) LT 8 yrs 134 (17.9) 20 (27.4) MT 12 yrs 175 (23.3) 15 (20.5) Public.Asstce = Yes (%) 563 (75.1) 38 (52.1) &lt;0.001 0.492 Hypertension = Y (%) 25 ( 3.3) 0 ( 0.0) 0.220 0.263 Diabetes = Yes (%) 24 ( 3.2) 0 ( 0.0) 0.235 0.257 Use.Tob (%) 0.153 0.269 No 636 (84.8) 68 (93.2) Yes 89 (11.9) 4 ( 5.5) NA 25 ( 3.3) 1 ( 1.4) Use.Alc (%) 0.583 0.146 No 709 (94.5) 71 (97.3) Yes 15 ( 2.0) 1 ( 1.4) NA 26 ( 3.5) 1 ( 1.4) Drug.Add (%) 0.434 0.161 No 720 (96.0) 71 (97.3) Yes 4 ( 0.5) 1 ( 1.4) NA 26 ( 3.5) 1 ( 1.4) Prev.preg = Yes (%) 555 (74.0) 56 (76.7) 0.715 0.063 Birthweight (mean (SD)) 3194.08 (684.92) 3247.29 (669.27) 0.529 0.079 We see that there appears to be a strong relationship between clinic and missingness of BMI. In particular, a disproportionately high number of missing BMI values seem to be from the New York (NY) clinic. There also appears to be less missingness for those with public.asstce=1 (those for whom the government paid for the delivery). Next we look at Use.Alc kable(asmd_opt$Use.Alc$asmd_table1) 0 1 p test SMD n 796 27 Clinic (%) &lt;0.001 0.874 KY 207 (26.0) 4 (14.8) MN 239 (30.0) 8 (29.6) MS 191 (24.0) 1 ( 3.7) NY 159 (20.0) 14 (51.9) Group = T (%) 399 (50.1) 14 (51.9) 1.000 0.035 Age (mean (SD)) 25.96 (5.59) 26.37 (4.99) 0.710 0.077 Education (%) 0.936 0.070 8-12 yrs 464 (58.3) 15 (55.6) LT 8 yrs 149 (18.7) 5 (18.5) MT 12 yrs 183 (23.0) 7 (25.9) Public.Asstce = Yes (%) 582 (73.1) 19 (70.4) 0.924 0.061 Hypertension = Y (%) 23 ( 2.9) 2 ( 7.4) 0.438 0.206 Diabetes = Yes (%) 24 ( 3.0) 0 ( 0.0) 0.738 0.249 BMI (mean (SD)) 27.70 (7.16) 26.88 (6.18) 0.568 0.122 Use.Tob (%) &lt;0.001 8.204 No 704 (88.4) 0 ( 0.0) Yes 92 (11.6) 1 ( 3.7) NA 0 ( 0.0) 26 (96.3) Drug.Add (%) &lt;0.001 7.079 No 790 (99.2) 1 ( 3.7) Yes 5 ( 0.6) 0 ( 0.0) NA 1 ( 0.1) 26 (96.3) Prev.preg = Yes (%) 591 (74.2) 20 (74.1) 1.000 0.004 Birthweight (mean (SD)) 3198.27 (677.67) 3225.88 (949.61) 0.873 0.033 asmd_opt$Use.Alc$asmd_plot This time the two most strongly related covariates are Use.Tob and Drug.Add. This is perhaps not surprising because they are quite similar variables. Notice that clinic still has a strong effect (the \\(x\\)-axis goes a lot higher on this plot than the one for BMI). From the table, we see that again a disproportionately high number of missing values come from the NY clinic, and that missingness in Use.Alc is much more likely if Use.Tob and/or Drug.Add are missing. C.3.2 Modelling missingness One of the most common ways to model missingness is using logistic regression. You might already have come across this in your degree, and we’ll do more about it later, but for now the box below tells you all you need to know. Logistic regression is a type of generalised linear model that is used to model a binary categorical variable \\(Y\\) (often labelled 0 and 1). In our cases the values are NA and !NA or ‘missing’ and ‘not missing’. The logistic regression model has the form \\[ \\operatorname{logit}\\left(p\\right) = \\log \\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_p x_p,\\] where \\(p = p\\left(Y=1\\right)\\) the \\(x_i\\) are explanatory variables (in our case this will be the baseline covariates and the trial arm), the \\(\\beta_i\\) are coefficients and \\(p\\) is the probability of an outcome of 1. The logit function rescales the probability (which can only be in \\(\\left[0,1\\right]\\)) to the real line, so that it works with linear combination on the right hand side. Conversely, applying the inverse logit function to the RHS gives a value in \\(\\left[0,1\\right]\\). Fitting a logistic regression model in R is very similar to fitting a linear regression. We use the function glm, which is a general function for generalised linear models. To specify that we want logistic regression, we must include the argument family = binoial(link = \"logit\"). We’ll use our nabular objects for this, since we already have variables denoting missingness. For example, we can see whether missingness of BMI in the supraclavicular dataset relates to any of the other variables. sup_nab = nabular(sup_df) sup_glm = glm(bmi_NA ~ group + gender + age + onset_sensory, data = sup_nab, family = binomial(link = &quot;logit&quot;)) summary(sup_glm) ## ## Call: ## glm(formula = bmi_NA ~ group + gender + age + onset_sensory, ## family = binomial(link = &quot;logit&quot;), data = sup_nab) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.45041 2.26927 -1.080 0.280 ## group2 -1.46042 1.38616 -1.054 0.292 ## gender1 0.99119 1.28605 0.771 0.441 ## age -0.04954 0.04609 -1.075 0.282 ## onset_sensory 0.06444 0.04116 1.566 0.117 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 27.128 on 102 degrees of freedom ## Residual deviance: 22.854 on 98 degrees of freedom ## AIC: 32.854 ## ## Number of Fisher Scoring iterations: 7 In this case none of the other variables are significant, so it would be reasonable to proceed as though the missingness of BMI is MCAR (unless there is expert knowledge to suggest that missingness might be linked to some other observed factor). This agrees with what we found in Section C.3.1 A word of caution: the default in most R functions, particularly for plotting or fitting models, is to remove all rows with any missingness. In a situation where there are missing values for multiple variables, particularly if the missingness is related, this could in itself introduce bias. Exercise C.8 Model the patterns of missingness for smdi and opt (use the opt_tmp data for this, without the severely missing variables). What do you find? To avoid typing out all the column names for the formula, you can copy and paste the output from paste(names(smdi_data), collapse = &quot; + &quot;) and delete the terms you don’t want. Click for solution Solution. Lung cancer data We can fit a few models, to see what we find. First we’ll tackle ecog_cat_NA. Missingness in ecog_cat doesn’t appear to be associated with any other baseline covariates: ecog_NA_glm1 = glm(ecog_cat_NA ~ exposure + age_num + female_cat + smoking_cat + physical_cat + alk_cat + histology_cat + ses_cat + copd_cat, data=smdi_nab, family = binomial(link=&quot;logit&quot;)) summary(ecog_NA_glm1) ## ## Call: ## glm(formula = ecog_cat_NA ~ exposure + age_num + female_cat + ## smoking_cat + physical_cat + alk_cat + histology_cat + ses_cat + ## copd_cat, family = binomial(link = &quot;logit&quot;), data = smdi_nab) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.546409 0.226805 -2.409 0.016 * ## exposure -0.089400 0.088616 -1.009 0.313 ## age_num -0.002654 0.003034 -0.875 0.382 ## female_cat1 -0.034202 0.086890 -0.394 0.694 ## smoking_cat1 0.101202 0.097823 1.035 0.301 ## physical_cat1 0.137491 0.087074 1.579 0.114 ## alk_cat1 -0.049371 0.257094 -0.192 0.848 ## histology_cat1 -0.002256 0.104115 -0.022 0.983 ## ses_cat2_middle 0.128238 0.113992 1.125 0.261 ## ses_cat3_high 0.098773 0.113447 0.871 0.384 ## copd_cat1 -0.017240 0.098015 -0.176 0.860 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 3265.9 on 2499 degrees of freedom ## Residual deviance: 3259.5 on 2489 degrees of freedom ## AIC: 3281.5 ## ## Number of Fisher Scoring iterations: 4 Now including the other variables with missing values: ecog_NA_glm2 = glm(ecog_cat_NA ~ exposure + age_num + female_cat + smoking_cat + physical_cat + alk_cat + histology_cat + ses_cat + copd_cat + egfr_cat + pdl1_num, data=smdi_nab, family = binomial(link=&quot;logit&quot;)) summary(ecog_NA_glm2) ## ## Call: ## glm(formula = ecog_cat_NA ~ exposure + age_num + female_cat + ## smoking_cat + physical_cat + alk_cat + histology_cat + ses_cat + ## copd_cat + egfr_cat + pdl1_num, family = binomial(link = &quot;logit&quot;), ## data = smdi_nab) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.063844 0.418284 -0.153 0.879 ## exposure -0.094567 0.140956 -0.671 0.502 ## age_num -0.005729 0.004484 -1.278 0.201 ## female_cat1 -0.215749 0.133629 -1.615 0.106 ## smoking_cat1 0.036868 0.139759 0.264 0.792 ## physical_cat1 0.149057 0.130846 1.139 0.255 ## alk_cat1 1.050366 0.741154 1.417 0.156 ## histology_cat1 -0.092049 0.173989 -0.529 0.597 ## ses_cat2_middle 0.215523 0.166471 1.295 0.195 ## ses_cat3_high 0.179477 0.166802 1.076 0.282 ## copd_cat1 0.106252 0.137255 0.774 0.439 ## egfr_cat1 0.082752 0.148229 0.558 0.577 ## pdl1_num -0.008005 0.006214 -1.288 0.198 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1620.9 on 1240 degrees of freedom ## Residual deviance: 1606.6 on 1228 degrees of freedom ## (1259 observations deleted due to missingness) ## AIC: 1632.6 ## ## Number of Fisher Scoring iterations: 4 None of the variables are significant, so it is probably reasonable to suppose that ecog_cat is MCAR (indeed the help file tells us it is). Now for egfr_cat: egfr_NA_glm1 = glm(egfr_cat_NA ~ exposure + age_num + female_cat + smoking_cat + physical_cat + alk_cat + histology_cat + ses_cat + copd_cat, data=smdi_nab, family = binomial(link=&quot;logit&quot;)) summary(egfr_NA_glm1) ## ## Call: ## glm(formula = egfr_cat_NA ~ exposure + age_num + female_cat + ## smoking_cat + physical_cat + alk_cat + histology_cat + ses_cat + ## copd_cat, family = binomial(link = &quot;logit&quot;), data = smdi_nab) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.776612 0.253687 -10.945 &lt; 2e-16 *** ## exposure 0.855880 0.093967 9.108 &lt; 2e-16 *** ## age_num 0.014198 0.003271 4.340 1.42e-05 *** ## female_cat1 0.781194 0.092458 8.449 &lt; 2e-16 *** ## smoking_cat1 0.626782 0.102280 6.128 8.89e-10 *** ## physical_cat1 0.513986 0.092421 5.561 2.68e-08 *** ## alk_cat1 1.116563 0.271932 4.106 4.03e-05 *** ## histology_cat1 0.807125 0.108896 7.412 1.25e-13 *** ## ses_cat2_middle -0.253823 0.119805 -2.119 0.0341 * ## ses_cat3_high -0.165891 0.118853 -1.396 0.1628 ## copd_cat1 0.579084 0.102993 5.623 1.88e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 3376.8 on 2499 degrees of freedom ## Residual deviance: 2949.9 on 2489 degrees of freedom ## AIC: 2971.9 ## ## Number of Fisher Scoring iterations: 3 egfr_NA_glm2 = glm(egfr_cat_NA ~ exposure + age_num + female_cat + smoking_cat + physical_cat + alk_cat + histology_cat + ses_cat + copd_cat + ecog_cat + pdl1_num, data=smdi_nab, family = binomial(link=&quot;logit&quot;)) summary(egfr_NA_glm2) ## ## Call: ## glm(formula = egfr_cat_NA ~ exposure + age_num + female_cat + ## smoking_cat + physical_cat + alk_cat + histology_cat + ses_cat + ## copd_cat + ecog_cat + pdl1_num, family = binomial(link = &quot;logit&quot;), ## data = smdi_nab) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -7.111213 0.592182 -12.008 &lt; 2e-16 *** ## exposure 1.003371 0.164023 6.117 9.52e-10 *** ## age_num 0.023939 0.005328 4.493 7.02e-06 *** ## female_cat1 0.886902 0.149387 5.937 2.90e-09 *** ## smoking_cat1 1.048884 0.165025 6.356 2.07e-10 *** ## physical_cat1 1.089550 0.150277 7.250 4.16e-13 *** ## alk_cat1 3.727345 0.719142 5.183 2.18e-07 *** ## histology_cat1 1.294128 0.173948 7.440 1.01e-13 *** ## ses_cat2_middle -0.072720 0.189281 -0.384 0.701 ## ses_cat3_high -0.123102 0.189587 -0.649 0.516 ## copd_cat1 0.994580 0.164698 6.039 1.55e-09 *** ## ecog_cat1 0.939474 0.152432 6.163 7.13e-10 *** ## pdl1_num 0.040946 0.007493 5.465 4.63e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1686.9 on 1273 degrees of freedom ## Residual deviance: 1209.5 on 1261 degrees of freedom ## (1226 observations deleted due to missingness) ## AIC: 1235.5 ## ## Number of Fisher Scoring iterations: 5 This time there is a definite relationship between missingness and the values of the other covariates, suggesting an MAR or MNAR mechanism. Finally, let’s model missingness in pdl1_num: pdl1_NA_glm1 = glm( pdl1_num_NA ~ exposure + age_num + female_cat + smoking_cat + physical_cat + alk_cat + histology_cat + ses_cat + copd_cat, data=smdi_nab, family = binomial(link=&quot;logit&quot;)) summary(pdl1_NA_glm1) ## ## Call: ## glm(formula = pdl1_num_NA ~ exposure + age_num + female_cat + ## smoking_cat + physical_cat + alk_cat + histology_cat + ses_cat + ## copd_cat, family = binomial(link = &quot;logit&quot;), data = smdi_nab) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.337969 0.271141 -4.935 8.03e-07 *** ## exposure -0.707361 0.112571 -6.284 3.31e-10 *** ## age_num 0.002670 0.003621 0.737 0.460901 ## female_cat1 0.115354 0.102830 1.122 0.261947 ## smoking_cat1 0.070899 0.117623 0.603 0.546663 ## physical_cat1 -0.057181 0.105602 -0.541 0.588177 ## alk_cat1 0.880597 0.260178 3.385 0.000713 *** ## histology_cat1 -0.106808 0.127451 -0.838 0.402014 ## ses_cat2_middle -0.072022 0.135175 -0.533 0.594168 ## ses_cat3_high -0.034611 0.133995 -0.258 0.796175 ## copd_cat1 0.094061 0.118031 0.797 0.425500 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 2548.4 on 2499 degrees of freedom ## Residual deviance: 2488.3 on 2489 degrees of freedom ## AIC: 2510.3 ## ## Number of Fisher Scoring iterations: 4 pdl1_NA_glm2 = glm(pdl1_num_NA ~ exposure + age_num + female_cat + smoking_cat + physical_cat + alk_cat + histology_cat + ses_cat + copd_cat + ecog_cat + egfr_cat, data=smdi_nab, family = binomial(link=&quot;logit&quot;)) summary(pdl1_NA_glm2) ## ## Call: ## glm(formula = pdl1_num_NA ~ exposure + age_num + female_cat + ## smoking_cat + physical_cat + alk_cat + histology_cat + ses_cat + ## copd_cat + ecog_cat + egfr_cat, family = binomial(link = &quot;logit&quot;), ## data = smdi_nab) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.834316 0.566570 -6.768 1.31e-11 *** ## exposure -0.431716 0.221450 -1.950 0.0512 . ## age_num 0.015682 0.007177 2.185 0.0289 * ## female_cat1 0.358442 0.197460 1.815 0.0695 . ## smoking_cat1 0.300876 0.220606 1.364 0.1726 ## physical_cat1 0.500724 0.201918 2.480 0.0131 * ## alk_cat1 3.890278 0.673079 5.780 7.48e-09 *** ## histology_cat1 0.479733 0.245695 1.953 0.0509 . ## ses_cat2_middle -0.102158 0.261232 -0.391 0.6958 ## ses_cat3_high 0.125765 0.253598 0.496 0.6199 ## copd_cat1 0.882138 0.225914 3.905 9.43e-05 *** ## ecog_cat1 0.307018 0.195834 1.568 0.1169 ## egfr_cat1 0.301910 0.227109 1.329 0.1837 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 830.66 on 945 degrees of freedom ## Residual deviance: 737.55 on 933 degrees of freedom ## (1554 observations deleted due to missingness) ## AIC: 763.55 ## ## Number of Fisher Scoring iterations: 5 This time there are far fewer significantly related variables, but we can again be confident that the mechanism isn’t MCAR. Frustratingly, the only way we could determine whether the mechanisms for egfr_cat and pdl1_num was MAR or MNAR would be to measure (or otherwise procure) some of the missing data. We simply do not have the necessary information to work out which is the case. If this were a real trial, we would now talk at length with the experts/clinicians, who will have a much better understanding of the probable causes of missingness. opt Now we’ll do the same with opt_tmp. We have missingness in several variables: BMI, Use.Tob, Use.Alc, Drug.Add, Birthweight. A model fit to the data in R will remove cases with any of these missing (which for this dataset might mean a lot are removed!). So, as well as building models with all variables, we can build models using only the fully observed data, and use the _NA variables from nabular as covariates: glm_opt_BMI_allvar = glm( BMI_NA ~ Clinic + Group + Age + Education + Public.Asstce + Hypertension + Diabetes + Use.Tob + Use.Alc + Drug.Add + Prev.preg, data = nabular(opt_tmp), family = binomial(link = &quot;logit&quot;)) summary(glm_opt_BMI_allvar) ## ## Call: ## glm(formula = BMI_NA ~ Clinic + Group + Age + Education + Public.Asstce + ## Hypertension + Diabetes + Use.Tob + Use.Alc + Drug.Add + ## Prev.preg, family = binomial(link = &quot;logit&quot;), data = nabular(opt_tmp)) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.69872 0.86773 -4.263 2.02e-05 *** ## ClinicMN 0.38878 0.58170 0.668 0.5039 ## ClinicMS -15.63977 755.63175 -0.021 0.9835 ## ClinicNY 3.02941 0.51105 5.928 3.07e-09 *** ## GroupT 0.15793 0.28816 0.548 0.5837 ## Age -0.00288 0.02650 -0.109 0.9135 ## EducationLT 8 yrs 0.67635 0.35763 1.891 0.0586 . ## EducationMT 12 yrs 0.23558 0.37844 0.622 0.5336 ## Public.AsstceYes -0.18766 0.30399 -0.617 0.5370 ## HypertensionY -15.53379 1839.84596 -0.008 0.9933 ## DiabetesYes -15.18021 1942.81416 -0.008 0.9938 ## Use.TobYes -0.24898 0.59964 -0.415 0.6780 ## Use.AlcYes 0.60910 1.49054 0.409 0.6828 ## Drug.AddYes 0.09438 1.58954 0.059 0.9527 ## Prev.pregYes 0.02862 0.36074 0.079 0.9368 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 483.11 on 794 degrees of freedom ## Residual deviance: 330.17 on 780 degrees of freedom ## (28 observations deleted due to missingness) ## AIC: 360.17 ## ## Number of Fisher Scoring iterations: 18 Complete cases only: glm_opt_BMI_comp = glm( BMI_NA ~ Clinic + Group + Age + Education + Public.Asstce + Hypertension + Diabetes + Prev.preg, data = nabular(opt_tmp), family = binomial(link = &quot;logit&quot;)) summary(glm_opt_BMI_comp) ## ## Call: ## glm(formula = BMI_NA ~ Clinic + Group + Age + Education + Public.Asstce + ## Hypertension + Diabetes + Prev.preg, family = binomial(link = &quot;logit&quot;), ## data = nabular(opt_tmp)) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.522e+00 8.233e-01 -4.279 1.88e-05 *** ## ClinicMN 1.960e-01 5.469e-01 0.358 0.7200 ## ClinicMS -1.582e+01 7.539e+02 -0.021 0.9833 ## ClinicNY 2.752e+00 4.724e-01 5.825 5.72e-09 *** ## GroupT 1.292e-01 2.777e-01 0.465 0.6416 ## Age -4.618e-03 2.615e-02 -0.177 0.8599 ## EducationLT 8 yrs 6.861e-01 3.442e-01 1.993 0.0463 * ## EducationMT 12 yrs 2.811e-01 3.674e-01 0.765 0.4442 ## Public.AsstceYes -2.250e-01 2.980e-01 -0.755 0.4502 ## HypertensionY -1.633e+01 1.764e+03 -0.009 0.9926 ## DiabetesYes -1.522e+01 1.938e+03 -0.008 0.9937 ## Prev.pregYes 9.714e-02 3.527e-01 0.275 0.7830 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 493.01 on 822 degrees of freedom ## Residual deviance: 348.69 on 811 degrees of freedom ## AIC: 372.69 ## ## Number of Fisher Scoring iterations: 18 Using missingness of incomplete variables in model: glm_opt_BMI_NA = glm( BMI_NA ~ Clinic + Group + Age + Education + Public.Asstce + Hypertension + Diabetes + Use.Tob_NA + Use.Alc_NA + Drug.Add_NA + Prev.preg, data = nabular(opt_tmp), family = binomial(link = &quot;logit&quot;)) summary(glm_opt_BMI_NA) ## ## Call: ## glm(formula = BMI_NA ~ Clinic + Group + Age + Education + Public.Asstce + ## Hypertension + Diabetes + Use.Tob_NA + Use.Alc_NA + Drug.Add_NA + ## Prev.preg, family = binomial(link = &quot;logit&quot;), data = nabular(opt_tmp)) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.493e+00 8.286e-01 -4.215 2.49e-05 *** ## ClinicMN 2.307e-01 5.472e-01 0.422 0.6733 ## ClinicMS -1.582e+01 7.547e+02 -0.021 0.9833 ## ClinicNY 2.825e+00 4.740e-01 5.960 2.52e-09 *** ## GroupT 1.310e-01 2.800e-01 0.468 0.6398 ## Age -4.480e-03 2.613e-02 -0.171 0.8639 ## EducationLT 8 yrs 6.498e-01 3.462e-01 1.877 0.0605 . ## EducationMT 12 yrs 2.335e-01 3.705e-01 0.630 0.5286 ## Public.AsstceYes -2.047e-01 3.006e-01 -0.681 0.4959 ## HypertensionY -1.563e+01 1.824e+03 -0.009 0.9932 ## DiabetesYes -1.527e+01 1.945e+03 -0.008 0.9937 ## Use.Tob_NANA 1.737e+01 1.532e+04 0.001 0.9991 ## Use.Alc_NANA -3.129e+00 1.091e+04 0.000 0.9998 ## Drug.Add_NANA -1.604e+01 1.075e+04 -0.001 0.9988 ## Prev.pregYes 7.120e-02 3.553e-01 0.200 0.8412 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 493.01 on 822 degrees of freedom ## Residual deviance: 343.93 on 808 degrees of freedom ## AIC: 373.93 ## ## Number of Fisher Scoring iterations: 18 From these three models it appears that BMI is much more likely to be missing for those from the NY clinic (we already knew this from our previous investigations!): ggplot(data=nabular(opt_tmp), aes(x=Clinic, fill = BMI_NA)) + geom_bar() Perhaps the most concerning missing data in the opt dataset is in the outcome Birthweight. glm_opt_BW_allvar = glm( Birthweight_NA ~ Clinic + Group + Age + Education + Public.Asstce + Hypertension + Diabetes + BMI + Use.Tob + Use.Alc + Drug.Add + Prev.preg, data = nabular(opt_tmp), family = binomial(link = &quot;logit&quot;)) summary(glm_opt_BW_allvar) ## ## Call: ## glm(formula = Birthweight_NA ~ Clinic + Group + Age + Education + ## Public.Asstce + Hypertension + Diabetes + BMI + Use.Tob + ## Use.Alc + Drug.Add + Prev.preg, family = binomial(link = &quot;logit&quot;), ## data = nabular(opt_tmp)) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 33.729 16752.867 0.002 0.998 ## ClinicMN -133.713 20792.481 -0.006 0.995 ## ClinicMS -103.928 39728.644 -0.003 0.998 ## ClinicNY 137.894 40148.666 0.003 0.997 ## GroupT -85.621 6320.331 -0.014 0.989 ## Age -14.213 877.730 -0.016 0.987 ## EducationLT 8 yrs 38.459 39853.578 0.001 0.999 ## EducationMT 12 yrs 234.150 38187.193 0.006 0.995 ## Public.AsstceYes -196.390 12389.447 -0.016 0.987 ## HypertensionY 37.256 48814.207 0.001 0.999 ## DiabetesYes 424.730 25942.553 0.016 0.987 ## BMI 2.829 615.616 0.005 0.996 ## Use.TobYes -41.710 44071.964 -0.001 0.999 ## Use.AlcYes 26.362 75102.687 0.000 1.000 ## Drug.AddYes 369.274 101563.415 0.004 0.997 ## Prev.pregYes -152.840 9926.365 -0.015 0.988 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 3.8896e+01 on 722 degrees of freedom ## Residual deviance: 9.5715e-07 on 707 degrees of freedom ## (100 observations deleted due to missingness) ## AIC: 32 ## ## Number of Fisher Scoring iterations: 25 glm_opt_BW_comp = glm( Birthweight_NA ~ Clinic + Group + Age + Education + Public.Asstce + Hypertension + Diabetes + Prev.preg, data = nabular(opt_tmp), family = binomial(link = &quot;logit&quot;)) summary(glm_opt_BW_comp) ## ## Call: ## glm(formula = Birthweight_NA ~ Clinic + Group + Age + Education + ## Public.Asstce + Hypertension + Diabetes + Prev.preg, family = binomial(link = &quot;logit&quot;), ## data = nabular(opt_tmp)) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.39389 1.55326 -1.541 0.1233 ## ClinicMN -17.39271 1787.56149 -0.010 0.9922 ## ClinicMS -1.66204 1.20073 -1.384 0.1663 ## ClinicNY 1.24364 0.72519 1.715 0.0864 . ## GroupT -0.11738 0.56324 -0.208 0.8349 ## Age -0.04719 0.05873 -0.804 0.4217 ## EducationLT 8 yrs -0.87213 1.07459 -0.812 0.4170 ## EducationMT 12 yrs 0.35302 0.66105 0.534 0.5933 ## Public.AsstceYes -0.18665 0.66256 -0.282 0.7782 ## HypertensionY -16.92650 5484.29344 -0.003 0.9975 ## DiabetesYes 2.48332 1.25710 1.975 0.0482 * ## Prev.pregYes -0.49570 0.62136 -0.798 0.4250 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 141.83 on 822 degrees of freedom ## Residual deviance: 116.01 on 811 degrees of freedom ## AIC: 140.01 ## ## Number of Fisher Scoring iterations: 20 glm_opt_BW_NA = glm( Birthweight_NA ~ Clinic + Group + Age + Education + Public.Asstce + Hypertension + Diabetes + BMI_NA + Use.Tob_NA + Use.Alc_NA + Drug.Add_NA + Prev.preg, data = nabular(opt_tmp), family = binomial(link = &quot;logit&quot;)) summary(glm_opt_BW_NA) ## ## Call: ## glm(formula = Birthweight_NA ~ Clinic + Group + Age + Education + ## Public.Asstce + Hypertension + Diabetes + BMI_NA + Use.Tob_NA + ## Use.Alc_NA + Drug.Add_NA + Prev.preg, family = binomial(link = &quot;logit&quot;), ## data = nabular(opt_tmp)) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.018e+00 2.652e+00 -0.761 0.4465 ## ClinicMN -2.355e+01 2.073e+03 -0.011 0.9909 ## ClinicMS -3.526e+00 2.002e+00 -1.761 0.0782 . ## ClinicNY -2.028e-01 1.090e+00 -0.186 0.8524 ## GroupT 3.653e-01 9.552e-01 0.382 0.7021 ## Age -2.149e-01 1.176e-01 -1.827 0.0677 . ## EducationLT 8 yrs 1.192e+00 1.840e+00 0.648 0.5170 ## EducationMT 12 yrs 4.508e+00 2.026e+00 2.225 0.0261 * ## Public.AsstceYes -1.239e+00 1.108e+00 -1.118 0.2638 ## HypertensionY -2.131e+01 7.405e+03 -0.003 0.9977 ## DiabetesYes 4.688e+00 1.901e+00 2.466 0.0137 * ## BMI_NANA -2.385e-01 1.384e+00 -0.172 0.8632 ## Use.Tob_NANA -1.047e+01 6.859e+04 0.000 0.9999 ## Use.Alc_NANA 8.377e+00 4.876e+04 0.000 0.9999 ## Drug.Add_NANA 1.078e+01 4.824e+04 0.000 0.9998 ## Prev.pregYes -9.335e-02 9.792e-01 -0.095 0.9240 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 141.830 on 822 degrees of freedom ## Residual deviance: 42.686 on 807 degrees of freedom ## AIC: 74.686 ## ## Number of Fisher Scoring iterations: 21 It appears from this that missing valuse of Birthweight aren’t associated with missingness in the other variables. Birthweight is more likely to be missing for patients with diabetes. ggplot(data = nabular(opt_tmp), aes(x=Diabetes, fill = Birthweight_NA)) + geom_bar() C.4 What to do about missing data?! Having established that missing data can be a problem, we now need to do something about it. Methods for handling missing data fall into one of two categories: Discard some (non-missing) data Add in (‘impute’) some synthetic data We’ll look at a few versions of these methods now, and use them on our practice datasets. C.4.1 Discarding some (non-missing) data C.4.1.1 Complete case analysis The very simplest thing we can do is to discard the data for any participant who has some missing data. This is called a complete-case analysis, because we only analyse data for participants whose data are complete. There are two main problems with this: If the missing data are not MCAR, then this can induce bias. This approach can drastically reduce the amount of data Exercise C.9 For each of our datasets, how many complete cases are there? Would you recommend a complete case analysis for any of these datasets? Hint: you can use na.omit to remove all rows with at least one NA from a data frame. Click for solution Solution. All we need to do is find the number of complete rows in each dataset, and compare it to the number of participants. nrow(na.omit(sup_df)) ## [1] 100 nrow(sup_df) ## [1] 103 If we perform a complete case analysis on the sup_df data, we lose data from three participants. nrow(na.omit(smdi_data)) ## [1] 795 nrow(smdi_data) ## [1] 2500 A complete case analysis of the smdi_data would leave us with only 795 (about 32%!) of the participants. nrow(na.omit(opt_tmp)) ## [1] 720 nrow(opt_tmp) ## [1] 823 If we use a complete case analysis on opt_tmp (remember we’re ignoring some of the most missing variables for now!) we’d lose 113 cases (out of 823). There are some other methods that involve discarding some data, but for the remainder of this practical we’ll focus on methods that involve imputing synthetic data. C.4.2 Imputing data In order to keep all the data we have, even for those cases with some missing variables, we will need to impute (add in) some new data. Let’s assume we have no way of actually measuring the true value now, and our only option is to choose some value that seems appropriate. One of our main reasons for imputing data is to avoid the bias that would result from discarding incomplete cases, but we could inadvertently introduce bias if we aren’t careful (or if we are careful and unlucky) while imputing synthetic data. We’ll look at a few methods, ranging from the very simple to the moderately complex. C.4.2.1 Mean imputation In this method we simply replace each missing value by the mean of the observed values for that variable. This method is not uncommon in practice, but it can have a number of undesirable effects: If the data are not MCAR, bias is introduced The sample standard deviation is reduced Relationships between this and other variables are distorted C.4.2.2 Imputing using logic Sometimes there is missingness in a dataset that we can fill in using information about how that variable relates to other variables. For example, in the opt data, consider the two columns Use.Tob and BL.Cig.Day: Use.Tob: Self-reported participant history of tobacco use, factor; Yes, No; Blank = Missing BL.Cig.Day: Self-reported number of cigarettes per day for those with tobacco use history, numeric, range: 1-30; Blank = Missing (variable 16= Yes or blank) or non-smoker (variable 16 = No)` The first, Use.Tob, is a binary variable indicating whether or not the participant uses tobacco. The second, BL.Cig.Day is a numerical variable indicating how many cigarettes per day the participant smokes. As the data are now, there are a lot of missing values for BL.Cig.Day. However, if for a particular participant we have Use.Tob = No then we know that BL.Cig.Day is zero, and we can impute that value. Exercise C.10 Using the information in the help file, impute values for the columns BL.Cig.Day, BL.Drks.Day and N.prev.preg in the opt_df data. Make a new version of opt_df so that we can compare before and after. Note the annoying space in some of the responses - often we have \"No \"! Click for solution Solution. One important thing to remember is that if the associated categorical variable is missing, then the value for the variable we’re imputing will also be missing. We therefore need to condition on the ‘parent’ variable, rather than replace all missing values of the ‘child’ variable. For example: opt_df_imp = opt_df opt_df_imp$BL.Cig.Day[opt_df_imp$Use.Tob==&quot;No &quot;] = 0 By doing this we have ‘fixed’ 704 missing values. Similarly we cand fix those for BL.Drks.Day and N.prev.preg: opt_df_imp$BL.Drks.Day[opt_df_imp$Use.Alc==&quot;No &quot;] = 0 opt_df_imp$N.prev.preg[opt_df_imp$Prev.preg==&quot;No &quot;] = 0 We can see that this has very much improved our situation! We could do something similar for BL.Diab.Type too since this is linked to Diabetes. vis_dat(opt_df, sort_type = F) vis_dat(opt_df_imp, sort_type = F) C.4.2.3 Imputation using a regression model In this section we’re going to use the STAN regression functions from the package rstanarm. The syntax is very similar to the base R regression and glm functions, but random sampling is much simpler. The rstanarm functions use MCMC to generate samples from the posterior distribution of the regression model. There is a lot of output about chains (which you can ignore, in this practical). There are no p-values associated with coefficients, We’ve already looked at using logistic regression to understand patterns of missingness, and so it may not come as a surprise that we can use regression models to choose appropriate values for imputation. This won’t be the same model, since we’re now interested in the value, rather than the missingness. Which type of regression model we use depends on the type of the variable we’re imputing values for. If the variable is continuous, linear regression is likely to work well. If the variable is binary, we should try logistic regression. There are plenty of other types of model we could use (as well as a whole host of machine learning type models!) but in this practical we’ll stick to those two. Let’s suppose we want to use regression to impute values for pdl1_num pdl1_lm = stan_glm( pdl1_num ~ exposure + age_num + female_cat + smoking_cat + physical_cat + alk_cat + histology_cat + ses_cat + copd_cat + eventtime + status + ecog_cat + egfr_cat, data = smdi_data ) ## ## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 1.4e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.049 seconds (Warm-up) ## Chain 1: 0.086 seconds (Sampling) ## Chain 1: 0.135 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 6e-06 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 2: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 2: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 2: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 2: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 2: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 2: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 2: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 2: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.041 seconds (Warm-up) ## Chain 2: 0.083 seconds (Sampling) ## Chain 2: 0.124 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 5e-06 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 3: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 3: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 3: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 3: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 3: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 3: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 3: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 3: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 3: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 3: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 3: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.041 seconds (Warm-up) ## Chain 3: 0.084 seconds (Sampling) ## Chain 3: 0.125 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 3e-06 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 4: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 4: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 4: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 4: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 4: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 4: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 4: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 4: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 4: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 4: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 4: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.047 seconds (Warm-up) ## Chain 4: 0.084 seconds (Sampling) ## Chain 4: 0.131 seconds (Total) ## Chain 4: summary(pdl1_lm) ## ## Model Info: ## function: stan_glm ## family: gaussian [identity] ## formula: pdl1_num ~ exposure + age_num + female_cat + smoking_cat + physical_cat + ## alk_cat + histology_cat + ses_cat + copd_cat + eventtime + ## status + ecog_cat + egfr_cat ## algorithm: sampling ## sample: 4000 (posterior sample size) ## priors: see help(&#39;prior_summary&#39;) ## observations: 795 ## predictors: 15 ## ## Estimates: ## mean sd 10% 50% 90% ## (Intercept) 36.9 2.8 33.4 37.0 40.5 ## exposure 8.8 0.7 7.9 8.8 9.7 ## age_num 0.1 0.0 0.0 0.1 0.1 ## female_cat1 -0.9 0.7 -1.9 -0.9 0.0 ## smoking_cat1 -0.6 0.8 -1.6 -0.6 0.4 ## physical_cat1 -0.8 0.8 -1.8 -0.8 0.2 ## alk_cat1 -4.6 5.7 -11.9 -4.6 2.9 ## histology_cat1 0.4 1.0 -0.8 0.4 1.7 ## ses_cat2_middle 0.7 0.9 -0.5 0.7 1.8 ## ses_cat3_high 1.0 0.9 -0.2 1.0 2.1 ## copd_cat1 0.6 0.8 -0.4 0.6 1.6 ## eventtime 0.5 0.3 0.1 0.5 0.9 ## status1 -1.7 1.4 -3.5 -1.7 0.1 ## ecog_cat1 0.4 0.7 -0.4 0.4 1.3 ## egfr_cat1 -1.5 0.9 -2.6 -1.5 -0.4 ## sigma 9.4 0.2 9.1 9.4 9.7 ## ## Fit Diagnostics: ## mean sd 10% 50% 90% ## mean_PPD 44.4 0.5 43.8 44.4 45.0 ## ## The mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help(&#39;summary.stanreg&#39;)). ## ## MCMC diagnostics ## mcse Rhat n_eff ## (Intercept) 0.0 1.0 4772 ## exposure 0.0 1.0 6283 ## age_num 0.0 1.0 6203 ## female_cat1 0.0 1.0 7510 ## smoking_cat1 0.0 1.0 5162 ## physical_cat1 0.0 1.0 7954 ## alk_cat1 0.1 1.0 8826 ## histology_cat1 0.0 1.0 7201 ## ses_cat2_middle 0.0 1.0 5258 ## ses_cat3_high 0.0 1.0 4789 ## copd_cat1 0.0 1.0 5464 ## eventtime 0.0 1.0 5199 ## status1 0.0 1.0 5465 ## ecog_cat1 0.0 1.0 8373 ## egfr_cat1 0.0 1.0 6543 ## sigma 0.0 1.0 6587 ## mean_PPD 0.0 1.0 4399 ## log-posterior 0.1 1.0 1710 ## ## For each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1). Exercise C.11 This approach is going to fail! Can you tell why? Can you suggest what to do about it? Click for solution Solution. The model involves variables that also have missingness (ecog_cat1 and egfr_cat), and so for any case with either of those missing, we won’t be able to impute a value. There are a couple of possibilities: Use the missingness of those values as an input (by creating a nabular object) Remove those variables from the model Work iteratively, generating temporary imputed values and cycling round the variables with missingness Because all our investigations suggest that ecog_cat1 is more-or-less unrelated to anything, we will remove it from the model. However, because egfr_cat is quite close to being significant in the model, we will keep it in but use missingness in the model. smdi_nab = nabular(smdi_data) pdl1_lm2 = stan_glm( pdl1_num ~ exposure + age_num + female_cat + smoking_cat + physical_cat + alk_cat + histology_cat + ses_cat + copd_cat + eventtime + status + egfr_cat_NA, data = smdi_nab ) ## ## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 1.2e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.038 seconds (Warm-up) ## Chain 1: 0.15 seconds (Sampling) ## Chain 1: 0.188 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 5e-06 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 2: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 2: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 2: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 2: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 2: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 2: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 2: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 2: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.037 seconds (Warm-up) ## Chain 2: 0.149 seconds (Sampling) ## Chain 2: 0.186 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 5e-06 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 3: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 3: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 3: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 3: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 3: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 3: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 3: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 3: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 3: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 3: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 3: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.039 seconds (Warm-up) ## Chain 3: 0.153 seconds (Sampling) ## Chain 3: 0.192 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 6e-06 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 4: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 4: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 4: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 4: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 4: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 4: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 4: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 4: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 4: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 4: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 4: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.037 seconds (Warm-up) ## Chain 4: 0.149 seconds (Sampling) ## Chain 4: 0.186 seconds (Total) ## Chain 4: summary(pdl1_lm2) ## ## Model Info: ## function: stan_glm ## family: gaussian [identity] ## formula: pdl1_num ~ exposure + age_num + female_cat + smoking_cat + physical_cat + ## alk_cat + histology_cat + ses_cat + copd_cat + eventtime + ## status + egfr_cat_NA ## algorithm: sampling ## sample: 4000 (posterior sample size) ## priors: see help(&#39;prior_summary&#39;) ## observations: 1983 ## predictors: 14 ## ## Estimates: ## mean sd 10% 50% 90% ## (Intercept) 39.6 1.7 37.4 39.6 41.8 ## exposure 8.0 0.5 7.4 8.0 8.7 ## age_num 0.0 0.0 0.0 0.0 0.1 ## female_cat1 -0.4 0.5 -1.0 -0.4 0.2 ## smoking_cat1 -0.5 0.5 -1.2 -0.5 0.2 ## physical_cat1 -1.5 0.5 -2.2 -1.5 -0.9 ## alk_cat1 -1.8 1.5 -3.7 -1.8 0.2 ## histology_cat1 -0.4 0.6 -1.1 -0.4 0.3 ## ses_cat2_middle 0.1 0.6 -0.7 0.1 0.9 ## ses_cat3_high 1.2 0.6 0.4 1.2 1.9 ## copd_cat1 -0.9 0.5 -1.6 -0.9 -0.2 ## eventtime 0.6 0.2 0.3 0.6 0.8 ## status1 -1.6 0.9 -2.7 -1.6 -0.5 ## egfr_cat_NANA 4.1 0.5 3.4 4.1 4.8 ## sigma 9.8 0.2 9.6 9.8 10.0 ## ## Fit Diagnostics: ## mean sd 10% 50% 90% ## mean_PPD 46.0 0.3 45.6 46.0 46.4 ## ## The mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help(&#39;summary.stanreg&#39;)). ## ## MCMC diagnostics ## mcse Rhat n_eff ## (Intercept) 0.0 1.0 3862 ## exposure 0.0 1.0 6502 ## age_num 0.0 1.0 5811 ## female_cat1 0.0 1.0 7575 ## smoking_cat1 0.0 1.0 5572 ## physical_cat1 0.0 1.0 7212 ## alk_cat1 0.0 1.0 6784 ## histology_cat1 0.0 1.0 6841 ## ses_cat2_middle 0.0 1.0 4357 ## ses_cat3_high 0.0 1.0 4397 ## copd_cat1 0.0 1.0 5163 ## eventtime 0.0 1.0 3859 ## status1 0.0 1.0 3848 ## egfr_cat_NANA 0.0 1.0 5139 ## sigma 0.0 1.0 7523 ## mean_PPD 0.0 1.0 5472 ## log-posterior 0.1 1.0 1619 ## ## For each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1). We can see from the means and standard deviations that most of the coefficients are not even close to ‘significant’, but if we want to investigate some more closely we can visualise them using the plot function (separately in this case because they are quite far apart numerically) plot(pdl1_lm2, plotfun=&quot;areas&quot;, prob = 0.95, pars = c(&quot;exposure&quot;)) plot(pdl1_lm2, plotfun=&quot;areas&quot;, prob = 0.95, pars = c(&quot;age_num&quot;)) We see that for both variables, zero is not in the central 95% of the distribution. This isn’t directly relevant here but it’s nice to know about! We will now use this second model to impute values for pdl1_num. ## Split the data according to whether egfr_cat is missing smdi_pdl1_comp = smdi_nab[!is.na(smdi_nab$pdl1_num),] smdi_pdl1_miss = smdi_nab[is.na(smdi_nab$pdl1_num),] ## Use the GLM to fit values to egfr_cat pdl1_imp_lm = predict(object = pdl1_lm2, newdata = smdi_pdl1_miss) smdi_pdl1_miss$pdl1_num = pdl1_imp_lm ## Join the data back together again (in a different order, but it doesn&#39;t matter) smdi_imp = rbind(smdi_pdl1_miss, smdi_pdl1_comp) This might seem sensible, but actually it isn’t a very realistic use of a regression model. Compared to the observed values, the imputed values have a strong central tendency. This is because we have imputed the mean fitted value for each point, ignoring the measure of spread / uncertainty in the model. We can see this by comparing the observed values of pdl1_num to their fitted values (which we would have imputed had they been missing). smdi_pdl1_comp$pdl1_imp = predict(pdl1_lm2, newdata = smdi_pdl1_comp) # The next line just recreates the imputataion so that we can bind the datasets together smdi_pdl1_miss$pdl1_imp = predict(pdl1_lm2, newdata = smdi_pdl1_miss) smdi_pdl1_imp = rbind(smdi_pdl1_comp, smdi_pdl1_miss) ggplot(data = smdi_pdl1_imp, aes(x=pdl1_imp, y=pdl1_num, col = pdl1_num_NA)) + geom_point() + xlab(&quot;Regression fit&quot;) + ylab(&quot;Observed value / imputed value&quot;) + theme_bw() A more realistic approach would be to sample one value from the posterior distribution for each point (this is why we are using rstanarm!) smdi_pdl1_comp$pdl1_imp_rand = smdi_pdl1_comp$pdl1_num ## This time draw one point at random from the posterior distribution for each participant ## The output is a 1xdraws matrix, which we&#39;ll convert to a vector pdl1_rand_draw = posterior_predict(pdl1_lm2, newdata = smdi_pdl1_miss, draws=1) smdi_pdl1_miss$pdl1_imp_rand = as.numeric(pdl1_rand_draw) ## Now we can bind the two dataframes together and plot the randomly imputed / observed ## data against the deterministically fitted data smdi_pdl1_imp = rbind(smdi_pdl1_comp, smdi_pdl1_miss) ggplot(data = smdi_pdl1_imp, aes(x=pdl1_imp, y=pdl1_imp_rand, col = pdl1_num_NA)) + geom_point() + xlab(&quot;Regression fit&quot;) + ylab(&quot;Observed value / randomly imputed value&quot;) + theme_bw() This imputed data looks much more representative of the actual dataset. Exercise C.12 Use regression imputation to impute values for BMI in opt_tmp. C.4.3 Multiple imputation Having reached the point of acknowledging the randomness needed in imputation, a natural next step would be to draw multiple values from the posterior distribution, rather than just one. This leads to a method called multiple imputation, which is (arguably) the most widely used approach to imputing missing data, though sadly we don’t have time to go into it in this practical. References Gelman, Andrew, Jennifer Hill, and Aki Vehtari. 2021. Regression and Other Stories. Cambridge University Press. Hotelling, Harold et al. 1931. “The Generalization of Student’s Ratio.” Little, Roderick JA, and Donald B Rubin. 2019. Statistical Analysis with Missing Data. Vol. 793. John Wiley &amp; Sons. Rubin, Donald B. 1976. “Inference and Missing Data.” Biometrika 63 (3): 581–92. "],["references.html", "References", " References This sections lists the references used in the course - it will be updated as the notes are updated. Some of the more accessible (dare I say ‘interesting’) resources are linked from the notes. If you want to read any of these articles, the easiest way is to copy the title into Google scholar. Altman, Douglas G. 1990. Practical Statistics for Medical Research. CRC press. ———. 1998. “Confidence Intervals for the Number Needed to Treat.” Bmj 317 (7168): 1309–12. Altman, Douglas G, and J Martin Bland. 1999. “Treatment Allocation in Controlled Trials: Why Randomise?” Bmj 318 (7192): 1209–9. Armitage, Peter, and Michael Hills. 1982. “The Two-Period Crossover Trial.” Journal of the Royal Statistical Society: Series D (The Statistician) 31 (2): 119–31. Berger, James O, José M Bernardo, and Dongchu Sun. 2009. “The Formal Definition of Reference Priors.” Berger, James O, and Donald A Berry. 1988. “Statistical Analysis and the Illusion of Objectivity.” American Scientist 76 (2): 159–65. Berry, Donald A. 2006. “Bayesian Clinical Trials.” Nature Reviews Drug Discovery 5 (1): 27–36. Borm, George F, Jaap Fransen, and Wim AJG Lemmens. 2007. “A Simple Sample Size Formula for Analysis of Covariance in Randomized Clinical Trials.” Journal of Clinical Epidemiology 60 (12): 1234–38. Collett, David. 2003a. Modelling Binary Data. 2nd ed. Texts in Statistical Science. Chapman &amp; Hall. ———. 2003b. Modelling Survival Data in Medical Research. 2nd ed. Texts in Statistical Science. Chapman &amp; Hall. Cottingham, Marci D, and Jill A Fisher. 2022. “Gendered Logics of Biomedical Research: Women in US Phase i Clinical Trials.” Social Problems 69 (2): 492–509. Cox, David R. 1972. “Regression Models and Life-Tables.” Journal of the Royal Statistical Society: Series B (Methodological) 34 (2): 187–202. Dickinson, L Miriam, Brenda Beaty, Chet Fox, Wilson Pace, W Perry Dickinson, Caroline Emsermann, and Allison Kempe. 2015. “Pragmatic Cluster Randomized Trials Using Covariate Constrained Randomization: A Method for Practice-Based Research Networks (PBRNs).” The Journal of the American Board of Family Medicine 28 (5): 663–72. Edmonson, John H, Thomas R Fleming, David G Decker, George D Malkasian, Edward O Jorgensen, John A Jefferies, Maurice J Webb, and Larry K Kvols. 1979. “Prognosis in Advanced Ovarian Carcinoma Versus Minimal Residual.” Cancer Treatment Reports 63 (2): 241–47. Efron, Bradley. 1971. “Forcing a Sequential Experiment to Be Balanced.” Biometrika 58 (3): 403–17. Elmunzer, B Joseph, James M Scheiman, Glen A Lehman, Amitabh Chak, Patrick Mosler, Peter DR Higgins, Rodney A Hayward, et al. 2012. “A Randomized Trial of Rectal Indomethacin to Prevent Post-ERCP Pancreatitis.” New England Journal of Medicine 366 (15): 1414–22. Fentiman, Ian S, Robert D Rubens, and John L Hayward. 1983. “Control of Pleural Effusions in Patients with Breast Cancer a Randomized Trial.” Cancer 52 (4): 737–39. Freedman, LS, and Susan J White. 1976. “On the Use of Pocock and Simon’s Method for Balancing Treatment Numbers over Prognostic Factors in the Controlled Clinical Trial.” Biometrics, 691–94. Gelman, Andrew, Jennifer Hill, and Aki Vehtari. 2021. Regression and Other Stories. Cambridge University Press. Giardiello, Francis M, Stanley R Hamilton, Anne J Krush, Steven Piantadosi, Linda M Hylind, Paul Celano, Susan V Booker, C Rahj Robinson, and G Johan A Offerhaus. 1993. “Treatment of Colonic and Rectal Adenomas with Sulindac in Familial Adenomatous Polyposis.” New England Journal of Medicine 328 (18): 1313–16. Goldacre, B. 2012. Bad Pharma: How Medicine Is Broken, and How We Can Fix It. HarperCollins Publishers. https://books.google.co.uk/books?id=4amY1Q6Id4QC. Grambsch, Patricia M, and Terry M Therneau. 1994. “Proportional Hazards Tests and Diagnostics Based on Weighted Residuals.” Biometrika 81 (3): 515–26. Greenwood, Major. 1926. “The Natural Duration of Cancer.” Reports on Public Health and Medical Subjects 33: 1–26. Hayes, Richard J, and Lawrence H Moulton. 2017. Cluster Randomised Trials. CRC press. Health, National Institute of. 2023. “History of Women’s Participation in Clinical Research.” Office of Research on Women’s Health. https://orwh. od. nih. gov/toolkit …. https://orwh.od.nih.gov/toolkit/recruitment/history. Hjalmas, Hanson, and Kruse Hellstrom. 1998. “Long‐term Treatment with Desmopressin in Children with Primary Monosymptomatic Nocturnal Enuresis: An Open Multicentre Study.” British Journal of Urology. Hommel, EHEBMJ, Hans-Henrik Parving, Elisabeth Mathiesen, Berit Edsberg, M Damkjaer Nielsen, and Jørn Giese. 1986. “Effect of Captopril on Kidney Function in Insulin-Dependent Diabetic Patients with Nephropathy.” Br Med J (Clin Res Ed) 293 (6545): 467–70. Hotelling, Harold et al. 1931. “The Generalization of Student’s Ratio.” Hulley, Stephen B, Steven R. Cummings, Warren S. Browner, Deborah G. Grady, and Thomas B. Newman. 2013. Designing Clinical Research, Fourth Edition. Lippincott Williams &amp; Wilkins. Kallis, P, JA Tooze, S Talbot, D Cowans, DH Bevan, and T Treasure. 1994. “Pre-Operative Aspirin Decreases Platelet Aggregation and Increases Post-Operative Blood Loss–a Prospective, Randomised, Placebo Controlled, Double-Blind Clinical Trial in 100 Patients with Chronic Stable Angina.” European Journal of Cardio-Thoracic Surgery: Official Journal of the European Association for Cardio-Thoracic Surgery 8 (8): 404–9. Kaplan, Edward L, and Paul Meier. 1958. “Nonparametric Estimation from Incomplete Observations.” Journal of the American Statistical Association 53 (282): 457–81. Kar, Sumit, Ajay Krishnan, Preetha K, and Atul Mohankar. 2012. “A Review of Antihistamines Used During Pregnancy.” Journal of Pharmacology and Pharmacotherapeutics 3 (2): 105–8. Kassambara, Alboukadel. 2019. Datarium: Data Bank for Statistical Analysis and Visualization. https://CRAN.R-project.org/package=datarium. Kendall, John. 2003. “Designing a Research Project: Randomised Controlled Trials and Their Principles.” Emergency Medicine Journal: EMJ 20 (2): 164. Le-Rademacher, Jennifer G, Ryan A Peterson, Terry M Therneau, Ben L Sanford, Richard M Stone, and Sumithra J Mandrekar. 2018. “Application of Multi-State Models in Cancer Clinical Trials.” Clinical Trials 15 (5): 489–98. Little, Roderick JA, and Donald B Rubin. 2019. Statistical Analysis with Missing Data. Vol. 793. John Wiley &amp; Sons. Marshall, Geoffrey. 1948. “STREPTOMYCIN TREATMENT OF PULMONARY TUBERCULOSIS a MEDICAL RESEARCH COUNCIL INVESTIGATION.” British Medical Journal. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2091872/pdf/brmedj03701-0007.pdf. Matthews, John NS. 2006. Introduction to Randomized Controlled Clinical Trials. CRC Press. Newcombe, Robert G. 1998. “Interval Estimation for the Difference Between Independent Proportions: Comparison of Eleven Methods.” Statistics in Medicine 17 (8): 873–90. Pocock, Stuart J, and Richard Simon. 1975. “Sequential Treatment Assignment with Balancing for Prognostic Factors in the Controlled Clinical Trial.” Biometrics, 103–15. Rubin, Donald B. 1976. “Inference and Missing Data.” Biometrika 63 (3): 581–92. Ruetzler, Kurt, Michael Fleck, Sabine Nabecker, Kristina Pinter, Gordian Landskron, Andrea Lassnigg, Jing You, and Daniel I Sessler. 2013. “A Randomized, Double-Blind Comparison of Licorice Versus Sugar-Water Gargle for Prevention of Postoperative Sore Throat and Postextubation Coughing.” Anesthesia &amp; Analgesia 117 (3): 614–21. Schoenfeld, David. 1982. “Partial Residuals for the Proportional Hazards Regression Model.” Biometrika 69 (1): 239–41. Smith, AC, JF Dowsett, RCG Russell, ARW Hatfield, and PB Cotton. 1994. “Randomised Trial of Endoscopic Steriting Versus Surgical Bypass in Malignant Low Bileduct Obstruction.” The Lancet 344 (8938): 1655–60. Spiegelhalter, David J, Keith R Abrams, and Jonathan P Myles. 2004. Bayesian Approaches to Clinical Trials and Health-Care Evaluation. Vol. 13. John Wiley &amp; Sons. Spiegelhalter, David J, Laurence S Freedman, and Mahesh KB Parmar. 1994. “Bayesian Approaches to Randomized Trials.” Journal of the Royal Statistical Society: Series A (Statistics in Society) 157 (3): 357–87. Stan Development Team. 2024. “RStan: The R Interface to Stan.” https://mc-stan.org/. Syriopoulou, Elisavet, Tove Wästerlid, Paul C Lambert, and Therese M-L Andersson. 2022. “Standardised Survival Probabilities: A Useful and Informative Tool for Reporting Regression Models for Survival Data.” British Journal of Cancer 127 (10): 1808–15. Taves, Donald R. 1974. “Minimization: A New Method of Assigning Patients to Treatment and Control Groups.” Clinical Pharmacology &amp; Therapeutics 15 (5): 443–53. Teerenstra, Steven, Sandra Eldridge, Maud Graff, Esther de Hoop, and George F Borm. 2012. “A Simple Sample Size Formula for Analysis of Covariance in Cluster Randomized Trials.” Statistics in Medicine 31 (20): 2169–78. Thall, Peter F, and Stephen C Vail. 1990. “Some Covariance Models for Longitudinal Count Data with Overdispersion.” Biometrics, 657–71. Therneau, Terry M. 2024. A Package for Survival Analysis in r. https://CRAN.R-project.org/package=survival. Treasure, Tom, and Kenneth D MacRae. 1998. “Minimisation: The Platinum Standard for Trials?: Randomisation Doesn’t Guarantee Similarity of Groups; Minimisation Does.” Bmj. British Medical Journal Publishing Group. Villar, Jesús, Carlos Ferrando, Domingo Martı́nez, Alfonso Ambrós, Tomás Muñoz, Juan A Soler, Gerardo Aguilar, et al. 2020. “Dexamethasone Treatment for the Acute Respiratory Distress Syndrome: A Multicentre, Randomised Controlled Trial.” The Lancet Respiratory Medicine 8 (3): 267–76. Vitale, Cristiana, Massimo Fini, Ilaria Spoletini, Mitja Lainscak, Petar Seferovic, and Giuseppe MC Rosano. 2017. “Under-Representation of Elderly and Women in Clinical Trials.” International Journal of Cardiology 232: 216–21. Wei, LJ. 1978. “An Application of an Urn Model to the Design of Sequential Controlled Clinical Trials.” Journal of the American Statistical Association 73 (363): 559–63. Wilson, Edwin B. 1927. “Probable Inference, the Law of Succession, and Statistical Inference.” Journal of the American Statistical Association 22 (158): 209–12. Zhong, Baoliang. 2009. “How to Calculate Sample Size in Randomized Controlled Trial?” Journal of Thoracic Disease 1 (1): 51. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
